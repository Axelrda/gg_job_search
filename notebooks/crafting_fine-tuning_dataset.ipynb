{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from openai import OpenAI\n",
    "\n",
    "from jobsearch.database import fetch_data_from_postgresql\n",
    "from jobsearch.params import OPENAI_API_KEY\n",
    "\n",
    "from jsonschema import validate\n",
    "import json\n",
    "\n",
    "\n",
    "pd.options.display.max_colwidth = 10000\n",
    "pd.options.display.max_rows = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CLOUD postgres database ...\n"
     ]
    }
   ],
   "source": [
    "df = fetch_data_from_postgresql(columns='id, description', import_from_cloud=True)\n",
    "ft_df = pd.read_csv('../db/llm_fine-tuning_dataset.csv')\n",
    "\n",
    "#df_all = pd.read_csv('../db/jobs_02-02-2024.csv')\n",
    "#df = pd.read_csv('../db/all_french_descriptions_08-02-2024.csv')\n",
    "# with open('../db/json_finetuning_data.json') as f:\n",
    "#     json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### quick prepro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "taken_ids = ft_df.id.tolist()\n",
    "df_ = df[~df['description'].isin(taken_ids)]\n",
    "df_ = df_[~df_.description.isna()]\n",
    "df2 = df_[df_.description.str.contains('de') & (df_.description.str.contains('à partir de')) & (df_.description.str.contains('€'))]#[5:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobsearch.cleaning import convert_to_tfidf, compute_cosine_similarity, convert_cosine_similarity_to_coo, filter_over_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_list = df2.description.tolist()\n",
    "df2_tfidf = convert_to_tfidf(df2_list)\n",
    "df2_cosine_matrix = compute_cosine_similarity(df2_tfidf)\n",
    "cosine_coo = convert_cosine_similarity_to_coo(df2_cosine_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_over_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m overth \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_over_threshold\u001b[49m(df2_cosine_matrix)\n\u001b[1;32m      3\u001b[0m idx_infer_2 \u001b[38;5;241m=\u001b[39m [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, over_th_array \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(overth\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(overth[idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#df[df.index.isin(idx_infer_2)]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filter_over_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "overth = filter_over_threshold(df2_cosine_matrix)\n",
    "\n",
    "idx_infer_2 = [idx for idx, over_th_array in enumerate(overth.values()) if len(overth[idx]) == 0]\n",
    "#df[df.index.isin(idx_infer_2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Craft fine-tuning dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get job description length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = df.description\n",
    "descriptions_len = descriptions.str.len()\n",
    "\n",
    "descriptions_len.hist(bins=200)\n",
    "plt.xlim([0,10000])\n",
    "\n",
    "descriptions_len.describe().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rough discrimination of descriptions with infos related to salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get descriptions ids w/ high probability of wage info in it\n",
    "ids1 = df[df.description.str.lower().str.contains(r'remuneration', regex=True)].id.to_list()\n",
    "ids2 = df[df.description.str.lower().str.contains(r'salaire', regex=True)].id.to_list()\n",
    "ids3 = df[df.description.str.lower().str.contains(r'euros', regex=True)].id.to_list()\n",
    "ids4 = df[df.description.str.lower().str.contains(r'k€|,00€|€ par', regex=True)].id.to_list()\n",
    "wage_ids = set(ids1 + ids2 + ids3 + ids4)\n",
    "len(wage_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sampling descriptions for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries_init = df[df.id.isin(wage_ids)]\n",
    "others = df[~df.id.isin(wage_ids)]\n",
    "\n",
    "sample1 = salaries_init.sample(90, random_state=42)\n",
    "sample2 = others.sample(20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.description.str.contains('tjm')].shape)\n",
    "\n",
    "#df[df.description.str.contains(r'(\\d+\\/\\d+)\\s{0,3}€', regex=True)]\n",
    "df[df.description.str.contains(r'\\d+-\\d+\\s{0,3}€', regex=True)]\n",
    "df[df.description.str.contains(r'\\d{0,4}k€', regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get handipcked ids with salaries info in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../db/handpicked_ids_high_quality_job_descrptions_for_fine-tuning.txt', 'r') as fichier:\n",
    "    handpicked_ids = fichier.read().split(', ')\n",
    "    handpicked_ids = set([int(ids_str) for ids_str in handpicked_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df.id.isin(handpicked_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handpicked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat all records chosed for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.concat([sample1, sample2, handpicked_df], axis=0)\n",
    "\n",
    "# remove ids used as examples in prompt\n",
    "sample = sample[~sample.id.isin([34, 170, 1914])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### language detection second pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jobsearch.ml_logic.lang_classifier import detect_language\n",
    "\n",
    "lang = detect_language(model_path='../models/xlm-roberta-base-language-detection/', df=sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sample = sample.drop('lang', axis=1)\n",
    "sample['new_lang'] = lang.tolist()\n",
    "\n",
    "# keep french records only\n",
    "sample = sample[(sample.new_lang == 'fr') & (sample.lang == 'fr')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep descriptions of specific length only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound=descriptions_len.describe().iloc[4]\n",
    "upper_bound=descriptions_len.describe().iloc[6]\n",
    "\n",
    "df = df[(descriptions.str.len() >= lower_bound) & (descriptions.str.len() >= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.description.str.contains('k€')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['id', 'description']]\n",
    "n = 100\n",
    "df[n:n+100].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt design "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### prompt salaire, complements, stack, langues, etude, remote ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_instructions = \"\"\"Tu dois extraire les informations clés d'une offre d'emploi au format JSON :\n",
    "\n",
    "# 1. Salaire:\n",
    "# - Pour un salaire indiqué comme minimum (ex. 'à partir de'), utilise-le pour le champ 'montant_min'.\n",
    "# - Pour un salaire indiqué comme maximum (ex. 'jusqu'à'), saisis-le dans 'montant_max'.\n",
    "# - En cas de montant unique sans autres indications, remplis à la fois 'montant_min' et 'montant_max' avec cette valeur.\n",
    "\n",
    "# format : dict\n",
    "\n",
    "# 2. Compléments de salaire:\n",
    "# - Dans le cas où l'offre mentionne explicitement des éléments additionnels au salaire de base, comme des primes ou des avantages, liste ces éléments dans le champ 'avantages_complementaires'.\n",
    "# - Inclure uniquement les éléments qui décrivent le type d'avantage offert, sans détails supplémentaires sur les conditions ou les pourcentages. Par exemple, 'Carte ticket restaurant' suffit sans mentionner le pourcentage de prise en charge.\n",
    "\n",
    "# format : list\n",
    "\n",
    "# 2. Stack Technologique:\n",
    "# - Si l'offre d'emploi spécifie certaines technologies, outils, librairies ou langages de programmation (ex. 'Recherche développeur Java, expérience en SQL souhaitée'), liste ces technologies dans le champ 'stack_technologique'.\n",
    "\n",
    "# format : list\n",
    "\n",
    "# 4. Niveau d'Étude:\n",
    "# - Note le niveau d'étude demandé dans le champ 'niveau_etude'. Par exemple, si l'offre mentionne 'Bac+5 requis', ajoute 'Bac+5' dans le champ correspondant.\n",
    "\n",
    "# format : str\n",
    "\n",
    "# 5. Langues Parlées:\n",
    "# - Liste les langues parlées demandées sous 'langues_parlees'. Par exemple, si l'offre indique 'Anglais courant nécessaire', ajoute 'Anglais' dans le champ correspondant.\n",
    "\n",
    "# format : list\n",
    "\n",
    "# 6. Travail en distanciel / remote:\n",
    "# - Indique dans le champs 'remote' si l'offre propose du télétravail par true ou false. \n",
    "\n",
    "# format : bool\n",
    "\n",
    "# 7. Informations Manquantes:\n",
    "# - En cas d'absence d'information, utilise null pour les champs numériques comme 'montant_min' et 'montant_max'.\n",
    "#         - une liste vide pour les champs de type liste comme 'avantages_complementaires', 'niveau_etude' et 'langues_parlees'.\n",
    "#         - reste le plus proche possible du verbatim et limite les interprétations.\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = \"\"\"\n",
    "\n",
    "# Chez Foxintelligence, nous sommes des amoureux de la data.\\n\\nNotre mission est de rendre les consommateurs et les marques plus intelligents, grâce à une donnée de marché révolutionnaire issue de l'intelligence collective. Pour les consommateurs, Foxintelligence rend accessibles et utiles leurs données transactionnelles issues de leurs boîtes mail. En les anonymisant, nous créons des statistiques uniques qui permettent aux marques de rester en phase avec les produits qui se vendent le mieux et les attentes des consommateurs.\\n\\nNous publions une application grand public aimée par des millions de personnes (Cleanfox, 4m+ de téléchargements, 4.8/5 sur l'app store et le play store) et préparons le lancement d'une app de bilan carbone à l'expérience radicalement nouvelle. Grâce à notre plateforme data, elle va rendre possible une prise de conscience massive des enjeux du changement climatique et des actions individuelles possibles pour le combattre. Nous pensons que l'information est une... force de changement puissante, lorsqu'elle s'appuie sur la data.\\n\\nUtilisant ces données personnelles totalement anonymisées, notre plateforme SaaS est devenue la référence de la market intelligence digitale en Europe avec des dizaines de grands groupes clients. Le point commun entre Deliveroo, Just Eat, BackMarket ou The Boston Consulting Group ? Ils vont très vite (on ne s'ennuie pas avec eux ) et ils nous font tous confiance\\n\\nSoutenus par des investisseurs de premier plan et récemment intégrés au groupe NielsenIQ (17m levés auprès de Daphni, Partech, GFC et eFounders), nous sommes maintenant à la conquête de l'Europe et en phase d'hyper-croissance (+100% attendue en 2022).\\n\\nSi notre FoxHQ reste à Paris, notre équipe de plus de 100 talents pluridisciplinaires (tech, data et business) travaille depuis partout en France. Nous pensons que notre politique de remote, notre culture forte (bienveillance, exigence, résilience et data-first) et notre innovation permanente en termes de modes de travail (ex. grille de salaire transparente, formation au changement climatique, transparence sur la stratégie) sont les clés de notre réussite collective.... et de ta réussite avec nous\\nJob Description\\n\\nFoxintelligence recherche un(e) stagiaire Data Analyst.\\n\\nTu seras rattaché(e) à l'équipe Data Management et tu interviendras au niveau de l'enrichissement de la donnée. Cet enrichissement rend possible l'analyse de la donnée par nos équipes analytics et nos clients. Ton rôle sera de garantir que les champs enrichis aient le niveau de completion et de qualité attendues sur l'ensemble de nos industries (e-commerce, food delivery etc.) et territoires couverts (7 pays européens, ainsi que l'Inde).\\n\\nTes responsabilités au quotidien :\\n• Qualité et implémentation de règles métier : tu auras une roadmap de marchands à enrichir, tu seras garant(e) de la qualité des enrichissements et de leur disponibilité à la date attendue. Tu travailleras main dans la main avec différentes équipes en interne (équipes clients, data management) pour y arriver.\\n• Amelioration et optimisation continue des outils\\n• Investigation sur les sujets liés à l'enrichissement : tu devras être capable de comprendre les problématiques liés aux enrichissements pour répondre aux besoins clients.\\n\\nRequirements\\n\\nCompétences nécessaires (ce qu'il te faut pour réussir sur ce poste) :\\n\\nSoft skills\\n• Esprit analytique et structuré\\n• Tu as la volonté d'apprendre, de contribuer spontanément à ton environnement et d'aider l'équipe à s'améliorer\\n• Tu aimes découper et optimiser des process, pour être toujours plus efficace\\n\\nHard skills\\n• Gestion de projet : tu sais t'organiser, gérer efficacement ton temps de travail, et communiquer avec des interlocuteurs divers pour faire avancer ton projet\\n• Tu es as l'aise en SQL\\n\\nCompétences bonus\\n• Maitrise de Tableau, Github, Regex\\n• Tu parles couramment allemand, espagnol ou italien ou néerlandais\\n• Tu as une première expérience en gestion de projet\\n\\nLadies\\n\\nLes études montrent que les femmes ont moins tendance à postuler à une offre d'emploi quand elles n'ont pas toutes les qualifications requises. Ladies, ne vous mettez pas de barrière et donnez-nous la chance de nous faire notre propre avis, nous serons toujours ravis d'échanger avec vous Si notre raison d'être vous parle, postulez\\n\\nSelf-made data lovers\\n\\nLes diplômes c'est bien, les skills c'est mieux et l'expérience t'en donne. Aucun diplôme n'est requis chez nous, c'est les compétences et l'énergie qui comptent\\nRecruitment process\\n• Round 0 : entretien (fit) avec un membre notre équipe HR\\n• Round 1 : entretien (expérience et fit) avec un(e) Senior Data Analyst de l'équipe\\n• Round 2 : test technique\\n• Decision Round : entretien avec notre Lead Data Analyst\\n\\nBenefits\\n\\nAvantages/ce que nous offrons\\n• Rémunération Compétitive\\n• Remote friendly (+ budget aménagement de l'espace de travail @Home)\\n• Carte ticket restaurants (Swile) pris en charge à 60% par Fox\\n• Culture forte et pratiques de management à la pointe (stratégie et résultats financiers transparents, feedback 360, grille de salaire innovante et transparente etc\n",
    "\n",
    "# {\n",
    "#   \"salaire\": {\n",
    "#     \"montant_min\": null,\n",
    "#     \"montant_max\": null\n",
    "#   },\n",
    "#   \"avantages_complementaires\": [\n",
    "#     \"Remote friendly\",\n",
    "#     \"Carte ticket restaurants\"\n",
    "#   ],\n",
    "#   \"stack_technologique\": [\n",
    "#     \"SQL\",\n",
    "#     \"Tableau\",\n",
    "#     \"Github\",\n",
    "#     \"Regex\"\n",
    "#   ],\n",
    "#   \"niveau_etude\": null,\n",
    "#   \"langues_parlees\": [\"Allemand\", \"Espagnol\", \"Italien\", \"Néerlandais\"],\n",
    "#   \"remote\": true\n",
    "# }\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### salary prompt + prompt token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load prompt\n",
    "with open(\"../db/salary_extraction_prompt-v2.txt\") as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count = len(prompt.split(' '))\n",
    "# token_count = round(word_count * 1.4) # 1.4 = approximation of french word/token conversion factor\n",
    "\n",
    "# print('Word count : ', word_count)\n",
    "# print('Token count : ', token_count)\n",
    "\n",
    "# print('\\n================== PROMPT ==================\\n', '\\n', prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training data token count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[~df.isna()]\n",
    "# token_count = df.description.str.split(' ').str.len() * 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bound1 = token_count.describe(percentiles=np.arange(0.99,1, 0.001))#[6]\n",
    "# df[token_count > bound1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request openai API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot mask with non-boolean array containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mremunération\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ggjobsearch/lib/python3.11/site-packages/pandas/core/series.py:1067\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;66;03m# Do slice check before somewhat-costly is_bool_indexer\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n\u001b[0;32m-> 1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_bool_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1068\u001b[0m     key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[1;32m   1069\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ggjobsearch/lib/python3.11/site-packages/pandas/core/common.py:133\u001b[0m, in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    129\u001b[0m     na_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot mask with non-boolean array containing NA / NaN values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_bool_array(key_array, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# Don't raise on e.g. [\"A\", \"B\", np.nan], see\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;66;03m#  test_loc_getitem_list_of_labels_categoricalindex_with_na\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(na_msg)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot mask with non-boolean array containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "df_.description[df_.description.str.contains('remunération')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_openai_json_output(prompt_list):\n",
    "  client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "  json_outputs = []\n",
    "\n",
    "  for description in prompt_list:\n",
    "    # models\n",
    "    GPT35 = \"gpt-3.5-turbo-1106\"\n",
    "    GPT4 = \"gpt-4-0613\" # doesn't support json_object response format type \n",
    "    GPT4_32K = \"gpt-4-32k-0613\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=GPT35,\n",
    "      response_format={ \"type\": \"json_object\" },\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": description}\n",
    "      ],\n",
    "    )\n",
    "    json_response = response.choices[0].message.content\n",
    "    json_outputs.append(json_response)\n",
    "    \n",
    "    print(json_response)\n",
    "    return json_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_outputs = request_openai_json_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### JSON outputs validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_extracted_salaries_json_validity(output_list):\n",
    "\n",
    "  json_validity = []\n",
    "  for json_response in output_list:\n",
    "    schema = {\n",
    "      \"type\" : \"object\",\n",
    "      \"properties\" : {\n",
    "        \"salaire\" : {\n",
    "          \"type\" : \"array\",\n",
    "          \"items\" : {\n",
    "            \"type\" : \"object\",\n",
    "            \"properties\" : {\n",
    "              \"montant_min\" : {\"type\" : [\"number\", \"null\"]},\n",
    "              \"montant_max\" : {\"type\" : [\"number\", \"null\"]},\n",
    "              \"devise\" : {\"type\" : [\"string\", \"null\"]},\n",
    "              \"frequence_versement\" : {\"type\" : [\"string\", \"null\"]}\n",
    "            },\n",
    "            \"required\": [\"montant_min\", \"montant_max\", \"devise\", \"frequence_versement\"]\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"required\" : [\"salaire\"]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        validate(instance=json.loads(json_response), schema=schema)\n",
    "        json_validity.append(True)\n",
    "        #print(True)\n",
    "    except Exception as e:\n",
    "        json_validity.append(e)\n",
    "        #print(f\"Le JSON est invalide : {e}\")\n",
    "        \n",
    "    return json_outputs_validity\n",
    "  \n",
    "  \n",
    "json_outputs_validity = check_extracted_salaries_json_validity(json_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concat ids + outputs + descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outputs = pd.DataFrame(zip(sample.id, sample.description, json_outputs, json_outputs_validity), columns=[\"id\", \"description\", \"gpt_output\", \"json_validity\"])\n",
    "\n",
    "# convert str output to json output\n",
    "df_outputs.gpt_output = df_outputs.gpt_output.map(lambda x: json.loads(x))\n",
    "\n",
    "# display invalid json outputs\n",
    "df_outputs[df_outputs.json_validity != True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert string to valid json string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_valid_json_string(input_string):\n",
    "    # Replace property names: Look for anything that matches 'word': (including nested properties)\n",
    "    # and replace with \"word\":\n",
    "    pattern_props = r\"(\\s*)'(\\w+)'\\s*:\"\n",
    "    subst_props = r'\\1\"\\2\":'\n",
    "    result = re.sub(pattern_props, subst_props, input_string)\n",
    "    \n",
    "    # Replace string values: Look for ':' followed by a single-quoted string\n",
    "    # This is a simplistic approach and might need adjustment based on your data's complexity\n",
    "    pattern_values = r\":\\s*'([^']*)'\"\n",
    "    subst_values = r': \"\\1\"'\n",
    "    result = re.sub(pattern_values, subst_values, result)\n",
    "\n",
    "    # Handle None values to be JSON compliant (convert None to null)\n",
    "    result = result.replace(\": None\", \": null\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "df['tojson'] = df['gpt_output'].map(convert_to_valid_json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to valid json/dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tojson = df['tojson'].map(json.loads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add metadata key to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_jsons = []\n",
    "for id, dicos in zip(df.id, df.tojson):\n",
    "    complete_json = {\"metadata\": {\"id\": id}, \"data\": dicos}\n",
    "    complete_jsons.append(complete_json)\n",
    "    \n",
    "df['complete_json'] = complete_jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = df['complete_json'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert data to alpaca dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../db/json_finetuning_data_alpaca_format.jsonl\", \"w\") as f:\n",
    "    for n in enumerate(json_data):\n",
    "        \n",
    "        # Extract 'input' based on 'id' from 'df'\n",
    "        input_text = df.description[df.id == json_data[n[0]]['metadata']['id']].values[0]\n",
    "        output = json_data[n[0]]['data']\n",
    "        #output_json_string = json.dumps(output, ensure_ascii=False)\n",
    "        #print(type(output_json_string))\n",
    "        # Construct the object in the desired format\n",
    "        alpaca_format = {\n",
    "            \"instruction\": prompt[0:10],  # Assuming 'prompt' is defined elsewhere\n",
    "            \"input\": input_text[0:10],\n",
    "            \"output\": str(output).replace(\"'\", '\"')\n",
    "        }\n",
    "        \n",
    "        # Convert the object to a JSON string and append a newline\n",
    "        jsonl_string = json.dumps(alpaca_format, ensure_ascii=False) + '\\n'\n",
    "        \n",
    "        # Write the JSON string directly to the file\n",
    "        f.write(jsonl_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['complete_json']].to_json('../db/json_finetuning_data.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### to local JSONL file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../db/json_finetuning_data_alpaca_format.jsonl\", \"w\") as f:\n",
    "    for n in enumerate(json_data):\n",
    "        \n",
    "        # Extract 'input' based on 'id' from 'df'\n",
    "        input_text = df.description[df.id == json_data[n[0]]['metadata']['id']].values[0]\n",
    "        output = json_data[n[0]]['data']\n",
    "        output_json_string = json.dumps(output, ensure_ascii=False)\n",
    "\n",
    "        # Correctly serialize 'output' to a JSON-formatted string\n",
    "        #output_json_string = json.dumps(output, ensure_ascii=False)\n",
    "        \n",
    "        # Construct the object in the desired format with 'output' as a JSON string\n",
    "        alpaca_format = {\n",
    "            \n",
    "            \"instruction\": prompt,  # Truncating prompt for brevity\n",
    "            \"input\": input_text,    # Truncating input for brevity\n",
    "            \"output\": output_json_string #str(output).replace(\"'\", '\"')\n",
    "        }\n",
    "        \n",
    "        # Serialize the entire object to a JSON string and append a newline\n",
    "        jsonl_string = json.dumps(alpaca_format, ensure_ascii=False) + '\\n'\n",
    "        \n",
    "        # Write the JSON string directly to the file\n",
    "        f.write(jsonl_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### to local JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../db/json_finetuning_data.json', 'w') as f:\n",
    "    json.dump(output_list, f, indent=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### to hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# read and split\n",
    "tiger_llama =  pd.read_json('../db/json_finetuning_data_alpaca_format.jsonl', lines=True)\n",
    "train_tiger, test_tiger = train_test_split(\n",
    "    tiger_llama, test_size=0.15, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# push to hub\n",
    "train_tiger = Dataset.from_pandas(train_tiger)\n",
    "test_tiger = Dataset.from_pandas(test_tiger)\n",
    "\n",
    "ds = DatasetDict()\n",
    "ds[\"train\"] = train_tiger\n",
    "ds[\"test\"] = test_tiger\n",
    "\n",
    "dataset_name = \"axel-rda/tech_job_salaries_prediction_fr\"\n",
    "ds.push_to_hub(dataset_name, branch=\"main\", private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggjobsearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
