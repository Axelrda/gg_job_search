{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef84455",
   "metadata": {},
   "source": [
    "# Scraping bullshit jobs ... a long way to hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9487d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# library for plotting data over maps\n",
    "import shapefile as shp\n",
    "\n",
    "# libraries used for tokenization\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
    "\n",
    "# Libraries used to remove similar job description based on cosine similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# libraries used for text normalization\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "# use to get USdollar / euro exchange rate from exchange rate API\n",
    "import requests\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "# import my functions\n",
    "from preprocessing import preprocess as pp\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd10655",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ced546c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/axel/ds_projects/projects/gg_job_search/data/gg_job_search_all_RAW.csv')\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e1272",
   "metadata": {},
   "source": [
    "## General overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0bcbba87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19423 entries, 0 to 19422\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      19423 non-null  int64  \n",
      " 1   title           19423 non-null  object \n",
      " 2   company_name    19423 non-null  object \n",
      " 3   location        19412 non-null  object \n",
      " 4   via             19423 non-null  object \n",
      " 5   description     19363 non-null  object \n",
      " 6   job_highlights  13090 non-null  object \n",
      " 7   related_links   19363 non-null  object \n",
      " 8   thumbnail       17657 non-null  object \n",
      " 9   extensions      19423 non-null  object \n",
      " 10  job_id          16609 non-null  object \n",
      " 11  posted_at       17430 non-null  object \n",
      " 12  schedule_type   19256 non-null  object \n",
      " 13  date_time       19423 non-null  object \n",
      " 14  search_query    19423 non-null  object \n",
      " 15  Unnamed: 0.1    6333 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 2.4+ MB\n",
      "None \n",
      "\n",
      "Unnamed: 0        6333\n",
      "title             1532\n",
      "company_name      1275\n",
      "location           354\n",
      "via                139\n",
      "description       2560\n",
      "job_highlights     627\n",
      "related_links     3932\n",
      "thumbnail          862\n",
      "extensions        1229\n",
      "job_id            2939\n",
      "posted_at           54\n",
      "schedule_type        4\n",
      "date_time         1704\n",
      "search_query         3\n",
      "Unnamed: 0.1        10\n",
      "dtype: int64 \n",
      "\n",
      "data engineer     6803\n",
      "data scientist    6446\n",
      "data analyst      6174\n",
      "Name: search_query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.info(), '\\n')\n",
    "print(data.nunique(), '\\n')\n",
    "print(data.search_query.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01234",
   "metadata": {},
   "source": [
    "## Null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b7e83723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location             11\n",
       "description          60\n",
       "job_highlights     6333\n",
       "related_links        60\n",
       "thumbnail          1766\n",
       "job_id             2814\n",
       "posted_at          1993\n",
       "schedule_type       167\n",
       "Unnamed: 0.1      13090\n",
       "dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47929c2",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "583af771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates : 546\n",
      "Number of duplicates (based on identical job_id) : 16483 \n",
      " ======================================================================\n",
      "Search query results :\n",
      " data scientist    1605\n",
      "data analyst       720\n",
      "data engineer      615\n",
      "Name: search_query, dtype: int64 \n",
      " ======================================================================\n",
      "Final number of rows :  2940\n",
      "Final number of columns :  17\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates :',data[data.duplicated()].shape[0])\n",
    "print('Number of duplicates (based on identical job_id) :', data[data.duplicated(['job_id'])].shape[0],'\\n', '='*70) \n",
    "\n",
    "# removing duplicates based on job_id\n",
    "data = data.drop_duplicates(['job_id'])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e020",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Purely based job id, we can skim off a lots of duplicates. Nonetheless i still found a few of identical / near-identical job descriptions, maybe because [sometimes recruiters or companies post the same advert for a job which results in duplicate data.](https://medium.com/analytics-vidhya/data-science-job-search-using-nlp-and-lda-in-python-12ecbfac79f9)\n",
    "\n",
    "To get rid off these ones, i could use cosine similatity between job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e082a5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates (based on cosine similarity) :  1108\n",
      "Final number of rows :  1832\n",
      "Final number of columns :  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33441/1214297801.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n"
     ]
    }
   ],
   "source": [
    "### removing duplicates based on cosine similarity between \n",
    "## job descriptions (code from Thomas Caffrey, see link above)\n",
    "\n",
    "# Defining our collection of job description texts to tokenize\n",
    "data.description = data['description'].fillna('')\n",
    "corpus = data['description']\n",
    "\n",
    "# instantiate CountVectorizer object\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Fit_transform to vectorize each job description (map terms to feature indices)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarities and put it in dataframe\n",
    "cos_df = pd.DataFrame(cosine_similarity(X_train_counts))\n",
    "\n",
    "\n",
    "## reshape dataframe for easier comparison\n",
    "\n",
    "# get arrays of rows indices and col indices from col_df.shape\n",
    "i, j = np.indices(cos_df.shape).reshape(2,-1)\n",
    "\n",
    "# reshape values to get a 1D array \n",
    "cos_values = cos_df.values.reshape(-1)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'i': i, 'j': j, 'sim':cos_values})\n",
    "\n",
    "# get cosine similarity values only above 0.98 \n",
    "cos_rem = cos_sim_df[(cos_sim_df['sim'] > 0.98) & (i!=j)]\n",
    "\n",
    "# Method to remove duplicates but keep first instance:\n",
    "# Trying to drop duplicates on i and j columns won't work as the row numbers of duplicates are either in i or j not both.\n",
    "# Setting another column that combines the i & j values ensures that duplicates can be dropped.\n",
    "\n",
    "cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n",
    "drop_rows = np.unique(cos_rem.drop_duplicates('i*j')['i'].values)\n",
    "\n",
    "# keep only non-duplicated job postings\n",
    "data = data[~data.index.isin(drop_rows)] \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Number of duplicates (based on cosine similarity) : ' ,drop_rows.shape[0])\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599a6b",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d501f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ['il y a 17 heures', 'À plein temps']\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['il y a 11 heures', '35\\xa0k\\xa0€ à 40\\xa0k\\xa0€ par an', 'À plein temps']\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ['il y a 19 heures', 'À plein temps']\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ['il y a 22 heures', 'À plein temps']\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ['il y a 19 heures', 'À plein temps']\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1827                                                                    eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAoRi9IKSIsImh0aWRvY2lkIjoiRGRvZUZrR3hEMmNBQUFBQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUdSbkpoYm1ObCIsImdsIjoiZnIiLCJobCI6ImZyIiwiZmMiOiJFc3NCQ293QlFVTlFkMnBxVVU1WVVUZGhSRU5pTkZkTlRUVnlRM0JrV0RFNGVHb3RNVk10YW14RFVWSnljM05FVW1kWGRYcFdkSEoyWkdSWlpESkxkVk5GU25VMWNuQm5kMVEzTW05cldEbFBha0pXWkd0V01YQlljemc0T0RobmFWWndjRlpqUnpCSVZuRTRhek5TUm1kdGVqSlFja3MzTjNob1pXNU1VMDlLZEMxYVprVlhPRkpyZGw5TVQwaGxVVWdTRm01WFdESlpOMFJYUXpkWGFIQjBVVkJ3WmkxWFUwRWFJa0ZHWHpaTlFVMW9XVXMzUkZwbFRFdFdkVXQ1U1hwdU9XcE5hRGxaVG1KVVRGRSIsImZjdiI6IjMiLCJmY19pZCI6ImZjXzYiLCJhcHBseV9saW5rIjp7InRpdGxlIjoiUG9zdHVsZXIgc3VyIEVuZ2FnZW1lbnQgSmV1bmVzIiwibGluayI6Imh0dHBzOi8vd3d3LmVuZ2FnZW1lbnQtamV1bmVzLmNvbS9wbGF0ZWZvcm1lL2ZyL2RldGFpbC1vZmZyZS8xMTYxNTc1My9zdGFnZS1vcmFuZ2Utc2EtZGF0YS1zY2llbnRpc3QtZi1oLmh0bWw/dXRtX2NhbXBhaWduPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9tZWRpdW09b3JnYW5pYyJ9fQ==\n",
       "1828                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            eyJqb2JfdGl0bGUiOiJEQVRBIFNDSUVOVElTVCBGL0ggLSBBaXgtZW4tUHJvdmVuY2UiLCJodGlkb2NpZCI6Iml5WTkwNXU3dHVjQUFBQUFBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lHUm5KaGJtTmwiLCJnbCI6ImZyIiwiaGwiOiJmciIsImFwcGx5X2xpbmsiOnsidGl0bGUiOiJQb3N0dWxlciBzdXIgSm9icyBUaGF0IE1ha2UgU2Vuc2UiLCJsaW5rIjoiaHR0cHM6Ly9qb2JzLm1ha2VzZW5zZS5vcmcvZW4vam9icy9vbWJyZWEtZGF0YS1zY2llbnRpc3QtZmgtNlp0b0F3b24wejNCbVZSTDhlT28/dXRtX2NhbXBhaWduPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9tZWRpdW09b3JnYW5pYyJ9fQ==\n",
       "1829    eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBlbiBEb25uw6llcyBBZ3Jvbm9taXF1ZXMgKEYvSCkgLSBGcmFuY2Ugb3UgSG9uZ3JpZSIsImh0aWRvY2lkIjoiQVhoZlF2dDNRbGtBQUFBQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUdSbkpoYm1ObCIsImdsIjoiZnIiLCJobCI6ImZyIiwiZmMiOiJFc3dCQ293QlFVTlFkMnBxVkZKeUxYUk9SVzVKVkRSVWFtdGliRWc1WDB4M1QyOTRkRTVoVTFwaVIweHFielZVU21FMmIzVXlkRlJNWVVkVGJUUklSRVJuUzBaM1pFbFlTa0p0T0ZaMFdYcHNUVTE2T0haVVdVSk9UMHBpYUhWc1NsWmhMWFp1YmpaM1MwZHROR3hzVmxOQldHY3lUakZaYmt3MVJ6Sm9NbkZVVDB4NFUySjBhazUzVjNaSFNtMU5MVk1TRjNOSFdESlpYM0pPVDE4Mlp6Vk9iMUIyVFdsbUxVRkpHaUpCUmw4MlRVRk5hRE14ZUVwc04yUnlXRFpoYjNONWQzbHFjVXhtU2sxNVgyUkIiLCJmY3YiOiIzIiwiZmNfaWQiOiJmY180IiwiYXBwbHlfbGluayI6eyJ0aXRsZSI6IlBvc3R1bGVyIHN1ciBCYXllciIsImxpbmsiOiJodHRwczovL2pvYnMuYmF5ZXIuY29tL2pvYi9QZXlyZWhvcmFkZS1EYXRhLVNjaWVudGlzdC1lbi1Eb25uJUMzJUE5ZXMtQWdyb25vbWlxdWVzLSUyOEZIJTI5LUZyYW5jZS1vdS1Ib25ncmllLUxhbmQvOTAzMTY3MDAxLz91dG1fY2FtcGFpZ249Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fc291cmNlPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX21lZGl1bT1vcmdhbmljIn19\n",
       "1830                                            eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAvLyBGdWxsIHJlbW90ZSAvLyBGbHVlbnQgRW5nbGlzaCAvLyBFY29sb2dpZSBIL0YiLCJodGlkb2NpZCI6IktrNkhhSjdkZGZzQUFBQUFBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lHUm5KaGJtTmwiLCJnbCI6ImZyIiwiaGwiOiJmciIsImZjIjoiRXVJQkNxSUJRVU5RZDJwcVVYZHZaa1V4WkV4TVJVTm1Va3BFVVdKcWNsSkRSblJ1VXpWaFMzbG9hVEp0YWsxS1NsUTBXRlZ2TUV4WWJEVnBWRVpxTFdwTFdYWjZjbDlHWDNWZlEydGpkVGh6WjBnM1JIbEpXRk51TVhsWlUzQldiR1JRU1dGYU5EUTJNMjlVY1ZjeGREVkxVVzA1Ym05bFNuVlBRMVozVUdoUlNsbDFkbDkxWlVZMlozZElkMGhuWVVjMVJUQmlhMkZ2VUZGMllsWldNRFJuYUhRelFTMUJFaGQyTWxneVdUZERYMFZOTFcxd2RGRlFiVzlEVjJ0QmN4b2lRVVpmTmsxQlQxTnJWV3BuTmpaVmFVUjVaMnByUlZSYVFWaGhNaTFQUm5CTVFRIiwiZmN2IjoiMyIsImZjX2lkIjoiZmNfMSIsImFwcGx5X2xpbmsiOnsidGl0bGUiOiJQb3N0dWxlciBzdXIgRmlnYXJvIEVtcGxvaSIsImxpbmsiOiJodHRwczovL2VtcGxvaS5sZWZpZ2Fyby5mci9vZmZyZXMtZW1wbG9pLzE1Njk0OTcxODQ3OTkxNjM5P3V0bV9jYW1wYWlnbj1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9zb3VyY2U9Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fbWVkaXVtPW9yZ2FuaWMifX0=\n",
       "1831                                                                                                                                                eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCIsImh0aWRvY2lkIjoiRnltRWFGNGRCVzRBQUFBQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUdSbkpoYm1ObCIsImdsIjoiZnIiLCJobCI6ImZyIiwiZmMiOiJFc3dCQ293QlFVTlFkMnBxVkVkUVpIWm1hRTA1V21sRlVXUXRMVzEzYjBWbWFESTBkRmw0TldOaVExcFFNRTg0WWpaR1VHNTNOVWRXVG1ObU0xbG5kM1ZqUTBoVWJGWmFRVEJDWkdWSmVGWXdNRnBYVldKcFIwMUdOVUp0ZDBGTmJsVm5NM1phZEhKUWMxQmtSR1pmWXpacFFVaExSRzl4YVhKdkxVeFBTbXMxVkRGSlNIVkVVbGh4YzNaQmQybGZhSG9TRnpKWFdESlpOVWN4UzJWRE5YRjBjMUIwYzJWbk1FRm5HaUpCUmw4MlRVRk9ORkJrVEVnMmRGVjRaalp6WWtKRk5HZG1hMVZWVmpkdWNGZEIiLCJmY3YiOiIzIiwiZmNfaWQiOiJmY18xIiwiYXBwbHlfbGluayI6eyJ0aXRsZSI6IlBvc3R1bGVyIHN1ciBMaW5rZWRJbiIsImxpbmsiOiJodHRwczovL2ZyLmxpbmtlZGluLmNvbS9qb2JzL3ZpZXcvZGF0YS1zY2llbnRpc3QtYXQtc3Vlei0zNDM4NDc2OTM2P3V0bV9jYW1wYWlnbj1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9zb3VyY2U9Z29vZ2xlX2pvYnNfYXBwbHlcdTAwMjZ1dG1fbWVkaXVtPW9yZ2FuaWMifX0=\n",
       "Name: extensions, Length: 1832, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d1851",
   "metadata": {},
   "source": [
    "## Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aaef9dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(data.title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242f91",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Based on this sample of job titles, we could create : \n",
    "    \n",
    "    * Contract_type (full-time, part-time ...) \n",
    "    * Contract_status (CDI, CDD, work-study, internship ...)\n",
    "    * Duration of Contract (Duration/Undetermined)\n",
    "    * Experience ( Senior, Junior ...)\n",
    "    * Data Specialization (Supply chain, Marketing, Clinical ...)\n",
    "    * Multiple titles (Analyst/Scientist, Scientist/ML Engineer, Manager/Analyst ...)\n",
    "    * Specific expertise asked for (Python, Power BI ...)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1679e1",
   "metadata": {},
   "source": [
    "## Explore company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3400f598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique companies : 1105\n"
     ]
    }
   ],
   "source": [
    "print('number of unique companies :', data.company_name.nunique())\n",
    "#data.company_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1583ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 20\n",
    "#data.loc[data['company_name'] == 'Unspecified', ['description']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05598b",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "When ***Unspecified***, companies name can be found in description column.\n",
    "\n",
    "Possible new columns :\n",
    "\n",
    "    * Group/Holding (Y/N/NC)\n",
    "    * Interim company (Y/N/NC)\n",
    "\n",
    "Based on the number of job posting per company we could potentially infer about : **size of company ? / Amount of data to work on** / \n",
    "\n",
    "Adding a time variable and much more data, the number of similar / identical job postings for the same company could maybe give insights on the **company's turnover rate / company's growth / magnitude of need-urgency to hire** ...  \n",
    "\n",
    "It seems like extracting additional informations without more context will be difficult. Having access to each company's structure information we could create :\n",
    "\n",
    "    * Size of company\n",
    "    * Industry\n",
    "    * Public / Private\n",
    "    \n",
    "We'll see if can extract more related informations in the following columns. Otherwise, we could try to scrap **Glassdoor databases** (or similar) to get those informations.remains to be seen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0ce67",
   "metadata": {},
   "source": [
    "## Explore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f01afb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique locations :  256\n"
     ]
    }
   ],
   "source": [
    "print('number of unique locations : ', data.location.nunique())\n",
    "#data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29d39825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "#data.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cabad",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "We could create a map of th repartition of job posting based on location provided.\n",
    "\n",
    "Some companies don't provide precise location *(ex : location = FRANCE)* and the information is not available in description column either. Further investigations will be needed for these companies, perhaps in conjunction with other databases *(GLASSDOOR / SIRENE databases for instance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2d818",
   "metadata": {},
   "source": [
    "## Explore via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "79a48ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique job plateforms :  111\n"
     ]
    }
   ],
   "source": [
    "print('number of unique job plateforms : ', data.via.nunique())\n",
    "# data.via.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b57edd",
   "metadata": {},
   "source": [
    "## Explore description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "acda0f5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1832.000000\n",
      "mean      2943.989629\n",
      "std       1643.546334\n",
      "min          0.000000\n",
      "25%       1776.750000\n",
      "50%       2678.500000\n",
      "75%       3857.250000\n",
      "max      14170.000000\n",
      "Name: description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhRUlEQVR4nO3df3BU1f3/8ddCYElosvJj2GUlQJiJgxJUDJYaGIMFYjViHab+AhGr7UD5GbH8KrZGRhJk2pi2FCyMg2lphOkIllarBKVRGhRMiEIYQccYIpKm1bgJggmQ8/2DT+7XJQGJbrJnN8/HzJ1xzz27eb8X2bw49969LmOMEQAAgEW6hbsAAACA8xFQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWiQl3Ad9Ec3OzPvnkE8XHx8vlcoW7HAAAcAmMMWpoaJDf71e3bhdfI4nIgPLJJ58oMTEx3GUAAIBvoLq6WoMGDbronIgMKPHx8ZLONZiQkBDmagAAwKWor69XYmKi83v8YiIyoLQc1klISCCgAAAQYS7l9AxOkgUAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwTrsDyuuvv67JkyfL7/fL5XLphRdeCNpvjFF2drb8fr9iY2M1fvx4VVRUBM1pbGzUvHnz1L9/f/Xu3Vu33367Pv7442/VCL7e0KUvtrkBAGCbdgeUL774Qtdcc43WrFnT5v7Vq1crLy9Pa9as0b59++Tz+TRp0iQ1NDQ4c7KysrRt2zZt3rxZu3fv1okTJ3Tbbbfp7Nmz37wTAAAQNWLa+4RbbrlFt9xyS5v7jDHKz8/X8uXLNWXKFElSQUGBvF6vCgsLNXPmTAUCAT3zzDP685//rIkTJ0qSNm3apMTERO3cuVM333zzt2gHAABEg5Ceg1JZWamamhplZGQ4Y263W+np6SopKZEklZaW6vTp00Fz/H6/UlJSnDnna2xsVH19fdAGAACiV7tXUC6mpqZGkuT1eoPGvV6vqqqqnDk9e/ZUnz59Ws1pef75cnNz9fjjj4eyVHSAC53P8tGqzE6uBAAQ6TrkKh6XyxX02BjTaux8F5uzbNkyBQIBZ6uurg5ZrQAAwD4hDSg+n0+SWq2E1NbWOqsqPp9PTU1Nqquru+Cc87ndbiUkJARtAAAgeoU0oCQlJcnn86moqMgZa2pqUnFxsdLS0iRJqamp6tGjR9Cc48eP6+DBg84cAADQtbX7HJQTJ07ogw8+cB5XVlaqvLxcffv21eDBg5WVlaWcnBwlJycrOTlZOTk5iouL09SpUyVJHo9HDz30kB555BH169dPffv21c9//nONHDnSuaoHAAB0be0OKG+//bZuuukm5/HChQslSTNmzNCzzz6rxYsX69SpU5o9e7bq6uo0ZswY7dixQ/Hx8c5znnrqKcXExOiuu+7SqVOnNGHCBD377LPq3r17CFoCAACRzmWMMeEuor3q6+vl8XgUCAQ4H6UdOvoqG67iAQBcTHt+f3MvHgAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwTrvvxYPow1fUAwBswwoKAACwDgEFAABYh4ACAACswzko6HCc4wIAaC9WUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdfgmWVwQ3wALAAgXVlAAAIB1CCgAAMA6BBQAAGAdzkGJQhc6dwQAgEjBCgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA7fJIt245tqAQAdjRUUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANbhbsYImwvdFfmjVZmdXAkAwDasoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsE7IA8qZM2f06KOPKikpSbGxsRo2bJhWrFih5uZmZ44xRtnZ2fL7/YqNjdX48eNVUVER6lIAAECECvllxk8++aSefvppFRQUaMSIEXr77bf14x//WB6PRwsWLJAkrV69Wnl5eXr22Wd1xRVX6IknntCkSZN0+PBhxcfHh7qkqHWhy3QBAIh0IV9B2bNnj374wx8qMzNTQ4cO1Y9+9CNlZGTo7bfflnRu9SQ/P1/Lly/XlClTlJKSooKCAp08eVKFhYWhLgcAAESgkAeUcePG6dVXX9WRI0ckSe+88452796tW2+9VZJUWVmpmpoaZWRkOM9xu91KT09XSUlJm6/Z2Nio+vr6oA0AAESvkB/iWbJkiQKBgIYPH67u3bvr7NmzWrlype69915JUk1NjSTJ6/UGPc/r9aqqqqrN18zNzdXjjz8e6lIBAIClQr6CsmXLFm3atEmFhYUqKytTQUGBfv3rX6ugoCBonsvlCnpsjGk11mLZsmUKBALOVl1dHeqyAQCARUK+grJo0SItXbpU99xzjyRp5MiRqqqqUm5urmbMmCGfzyfp3ErKwIEDnefV1ta2WlVp4Xa75Xa7Q10qAACwVMhXUE6ePKlu3YJftnv37s5lxklJSfL5fCoqKnL2NzU1qbi4WGlpaaEuBwAARKCQr6BMnjxZK1eu1ODBgzVixAjt379feXl5evDBByWdO7STlZWlnJwcJScnKzk5WTk5OYqLi9PUqVNDXQ4AAIhAIQ8ov//97/XLX/5Ss2fPVm1trfx+v2bOnKlf/epXzpzFixfr1KlTmj17turq6jRmzBjt2LGD70ABAACSJJcxxoS7iPaqr6+Xx+NRIBBQQkJCuMsJm2j9oraPVmWGuwQAQAdoz+9v7sUDAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsExPuAoBLNXTpi22Of7Qqs5MrAQB0NFZQAACAdQgoAADAOhziQcTj0A8ARB9WUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh7sZR4AL3a0XF8ddjgEgcrGCAgAArENAAQAA1iGgAAAA63AOCqzDOTcAAFZQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1+CZZdDnc5RgA7McKCgAAsA4BBQAAWKdDAsqxY8d03333qV+/foqLi9O1116r0tJSZ78xRtnZ2fL7/YqNjdX48eNVUVHREaUAAIAIFPKAUldXp7Fjx6pHjx765z//qUOHDuk3v/mNLrvsMmfO6tWrlZeXpzVr1mjfvn3y+XyaNGmSGhoaQl0OAACIQCE/SfbJJ59UYmKiNm7c6IwNHTrU+W9jjPLz87V8+XJNmTJFklRQUCCv16vCwkLNnDkz1CUBAIAIE/IVlO3bt2v06NG68847NWDAAI0aNUobNmxw9ldWVqqmpkYZGRnOmNvtVnp6ukpKStp8zcbGRtXX1wdtAAAgeoU8oHz44Ydat26dkpOT9corr2jWrFmaP3++/vSnP0mSampqJElerzfoeV6v19l3vtzcXHk8HmdLTEwMddkAAMAiIQ8ozc3Nuu6665STk6NRo0Zp5syZ+ulPf6p169YFzXO5XEGPjTGtxlosW7ZMgUDA2aqrq0NdNgAAsEjIA8rAgQN11VVXBY1deeWVOnr0qCTJ5/NJUqvVktra2larKi3cbrcSEhKCNgAAEL1CHlDGjh2rw4cPB40dOXJEQ4YMkSQlJSXJ5/OpqKjI2d/U1KTi4mKlpaWFuhwAABCBQn4Vz8MPP6y0tDTl5OTorrvu0t69e7V+/XqtX79e0rlDO1lZWcrJyVFycrKSk5OVk5OjuLg4TZ06NdTlAACACBTygHL99ddr27ZtWrZsmVasWKGkpCTl5+dr2rRpzpzFixfr1KlTmj17turq6jRmzBjt2LFD8fHxoS4HAABEIJcxxoS7iPaqr6+Xx+NRIBDoEuejXOjmdggtbhYIAB2rPb+/uRcPAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKwT8psF4pvjnjsAAJzDCgoAALAOAQUAAFiHQzzA17jQobePVmV2ciUA0HWwggIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0uMwb+D9/kCwD2YAUFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6MeEuAIhUQ5e+2Ob4R6syO7kSAIg+rKAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFinwwNKbm6uXC6XsrKynDFjjLKzs+X3+xUbG6vx48eroqKio0sBAAARokMDyr59+7R+/XpdffXVQeOrV69WXl6e1qxZo3379snn82nSpElqaGjoyHIAAECE6LCAcuLECU2bNk0bNmxQnz59nHFjjPLz87V8+XJNmTJFKSkpKigo0MmTJ1VYWNhR5QAAgAjSYQFlzpw5yszM1MSJE4PGKysrVVNTo4yMDGfM7XYrPT1dJSUlbb5WY2Oj6uvrgzYAABC9OuSr7jdv3qyysjLt27ev1b6amhpJktfrDRr3er2qqqpq8/Vyc3P1+OOPh75QAABgpZCvoFRXV2vBggXatGmTevXqdcF5Lpcr6LExptVYi2XLlikQCDhbdXV1SGsGAAB2CfkKSmlpqWpra5WamuqMnT17Vq+//rrWrFmjw4cPSzq3kjJw4EBnTm1tbatVlRZut1tutzvUpQIAAEuFfAVlwoQJOnDggMrLy51t9OjRmjZtmsrLyzVs2DD5fD4VFRU5z2lqalJxcbHS0tJCXQ4AAIhAIV9BiY+PV0pKStBY79691a9fP2c8KytLOTk5Sk5OVnJysnJychQXF6epU6eGuhwAABCBOuQk2a+zePFinTp1SrNnz1ZdXZ3GjBmjHTt2KD4+PhzlAAAAy7iMMSbcRbRXfX29PB6PAoGAEhISwl1OyAxd+mK4S0AIfLQqM9wlAICV2vP7m3vxAAAA6xBQAACAdQgoAADAOmE5SRaIZhc6l4hzUwDg0rGCAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA63M0YiDDcLRlAV8AKCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdbjMOAwudJkoohuXBwPApWMFBQAAWIeAAgAArENAAQAA1uEclHbgHAIAADoHKygAAMA6BBQAAGAdDvF0IC4nxqXg0CEAtMYKCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh6+6DwG+0h4AgNBiBQUAAFiHgAIAAKzDIR7AUhw6BNCVsYICAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHVCHlByc3N1/fXXKz4+XgMGDNAdd9yhw4cPB80xxig7O1t+v1+xsbEaP368KioqQl0KAACIUCEPKMXFxZozZ47efPNNFRUV6cyZM8rIyNAXX3zhzFm9erXy8vK0Zs0a7du3Tz6fT5MmTVJDQ0OoywEAABEoJtQv+PLLLwc93rhxowYMGKDS0lLdeOONMsYoPz9fy5cv15QpUyRJBQUF8nq9Kiws1MyZM0NdEgAAiDAdfg5KIBCQJPXt21eSVFlZqZqaGmVkZDhz3G630tPTVVJS0tHlAACACBDyFZSvMsZo4cKFGjdunFJSUiRJNTU1kiSv1xs01+v1qqqqqs3XaWxsVGNjo/O4vr6+gyoGAAA26NAVlLlz5+rdd9/Vc88912qfy+UKemyMaTXWIjc3Vx6Px9kSExM7pF4AAGCHDgso8+bN0/bt27Vr1y4NGjTIGff5fJL+/0pKi9ra2larKi2WLVumQCDgbNXV1R1VNgAAsEDIA4oxRnPnztXWrVv12muvKSkpKWh/UlKSfD6fioqKnLGmpiYVFxcrLS2tzdd0u91KSEgI2gAAQPQK+Tkoc+bMUWFhof72t78pPj7eWSnxeDyKjY2Vy+VSVlaWcnJylJycrOTkZOXk5CguLk5Tp04NdTkAACAChTygrFu3TpI0fvz4oPGNGzfqgQcekCQtXrxYp06d0uzZs1VXV6cxY8Zox44dio+PD3U5AAAgAoU8oBhjvnaOy+VSdna2srOzQ/3jAQBAFOBePAAAwDod+j0oADrP0KUvtjn+0arMTq4EAL49VlAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA43C2zDhW66BqA1blIIoCOwggIAAKxDQAEAANYhoAAAAOtwDgoQ5ThHBEAkYgUFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHb5IFEIRvngVgA1ZQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsw2XGAKzGZc9A18QKCgAAsA4BBQAAWIeAAgAArMM5KAAuyYXOBYkUnMsCRBZWUAAAgHUIKAAAwDoc4gHQIcJ1SCXSD0UBOIcVFAAAYB0CCgAAsA4BBQAAWIdzUIAuKlznanC5L4BLwQoKAACwDgEFAABYh0M8AKzQ3kNOXE4MRDdWUAAAgHUIKAAAwDoEFAAAYB3OQQGADsRl1cA3wwoKAACwDgEFAABYh0M8ANAOfAMv0DlYQQEAANYhoAAAAOuENaCsXbtWSUlJ6tWrl1JTU/XGG2+EsxwAAGCJsJ2DsmXLFmVlZWnt2rUaO3as/vjHP+qWW27RoUOHNHjw4HCVBQCSovur9Dmf5ZxwvQ+2vf+21dMibCsoeXl5euihh/STn/xEV155pfLz85WYmKh169aFqyQAAGCJsKygNDU1qbS0VEuXLg0az8jIUElJSav5jY2NamxsdB4HAgFJUn19fYfU19x4skNeF4B9LvQ50tGfA+39/LpQPd/kczCUrxXJwvU+2Pb+d2Y9La9pjPn6ySYMjh07ZiSZf//730HjK1euNFdccUWr+Y899piRxMbGxsbGxhYFW3V19ddmhbB+D4rL5Qp6bIxpNSZJy5Yt08KFC53Hzc3N+uyzz9SvX782538b9fX1SkxMVHV1tRISEkL62pGgK/fflXuXunb/Xbl3if67cv+d3bsxRg0NDfL7/V87NywBpX///urevbtqamqCxmtra+X1elvNd7vdcrvdQWOXXXZZR5aohISELvc/6ld15f67cu9S1+6/K/cu0X9X7r8ze/d4PJc0Lywnyfbs2VOpqakqKioKGi8qKlJaWlo4SgIAABYJ2yGehQsXavr06Ro9erRuuOEGrV+/XkePHtWsWbPCVRIAALBE2ALK3XffrU8//VQrVqzQ8ePHlZKSopdeeklDhgwJV0mSzh1Oeuyxx1odUuoqunL/Xbl3qWv335V7l+i/K/dvc+8uYy7lWh8AAIDOw714AACAdQgoAADAOgQUAABgHQIKAACwDgHlK9auXaukpCT16tVLqampeuONN8JdUrvl5ubq+uuvV3x8vAYMGKA77rhDhw8fDppjjFF2drb8fr9iY2M1fvx4VVRUBM1pbGzUvHnz1L9/f/Xu3Vu33367Pv7446A5dXV1mj59ujwejzwej6ZPn67PP/+8o1u8ZLm5uXK5XMrKynLGor33Y8eO6b777lO/fv0UFxena6+9VqWlpc7+aO7/zJkzevTRR5WUlKTY2FgNGzZMK1asUHNzszMnWvp//fXXNXnyZPn9frlcLr3wwgtB+zuzz6NHj2ry5Mnq3bu3+vfvr/nz56upqakj2nZcrP/Tp09ryZIlGjlypHr37i2/36/7779fn3zySdBrRGv/55s5c6ZcLpfy8/ODxiOi/297X51osXnzZtOjRw+zYcMGc+jQIbNgwQLTu3dvU1VVFe7S2uXmm282GzduNAcPHjTl5eUmMzPTDB482Jw4ccKZs2rVKhMfH2+ef/55c+DAAXP33XebgQMHmvr6emfOrFmzzOWXX26KiopMWVmZuemmm8w111xjzpw548z5wQ9+YFJSUkxJSYkpKSkxKSkp5rbbbuvUfi9k7969ZujQoebqq682CxYscMajuffPPvvMDBkyxDzwwAPmrbfeMpWVlWbnzp3mgw8+cOZEc/9PPPGE6devn/nHP/5hKisrzV//+lfzne98x+Tn5ztzoqX/l156ySxfvtw8//zzRpLZtm1b0P7O6vPMmTMmJSXF3HTTTaasrMwUFRUZv99v5s6dG7b+P//8czNx4kSzZcsW895775k9e/aYMWPGmNTU1KDXiNb+v2rbtm3mmmuuMX6/3zz11FNB+yKhfwLK//nud79rZs2aFTQ2fPhws3Tp0jBVFBq1tbVGkikuLjbGGNPc3Gx8Pp9ZtWqVM+fLL780Ho/HPP3008aYc3/Be/ToYTZv3uzMOXbsmOnWrZt5+eWXjTHGHDp0yEgyb775pjNnz549RpJ57733OqO1C2poaDDJycmmqKjIpKenOwEl2ntfsmSJGTdu3AX3R3v/mZmZ5sEHHwwamzJlirnvvvuMMdHb//m/oDqzz5deesl069bNHDt2zJnz3HPPGbfbbQKBQIf0e76L/YJusXfvXiPJ+QdnV+j/448/Npdffrk5ePCgGTJkSFBAiZT+OcQjqampSaWlpcrIyAgaz8jIUElJSZiqCo1AICBJ6tu3rySpsrJSNTU1Qb263W6lp6c7vZaWlur06dNBc/x+v1JSUpw5e/bskcfj0ZgxY5w53/ve9+TxeML+ns2ZM0eZmZmaOHFi0Hi09759+3aNHj1ad955pwYMGKBRo0Zpw4YNzv5o73/cuHF69dVXdeTIEUnSO++8o927d+vWW2+VFP39t+jMPvfs2aOUlJSgG7/dfPPNamxsDDq0GG6BQEAul8u5h1u099/c3Kzp06dr0aJFGjFiRKv9kdJ/WO9mbIv//e9/Onv2bKsbFXq93lY3NIwkxhgtXLhQ48aNU0pKiiQ5/bTVa1VVlTOnZ8+e6tOnT6s5Lc+vqanRgAEDWv3MAQMGhPU927x5s8rKyrRv375W+6K99w8//FDr1q3TwoUL9Ytf/EJ79+7V/Pnz5Xa7df/990d9/0uWLFEgENDw4cPVvXt3nT17VitXrtS9994rKfr//Ft0Zp81NTWtfk6fPn3Us2dPK94LSfryyy+1dOlSTZ061bkZXrT3/+STTyomJkbz589vc3+k9E9A+QqXyxX02BjTaiySzJ07V++++652797dat836fX8OW3ND+d7Vl1drQULFmjHjh3q1avXBedFY+/SuX81jR49Wjk5OZKkUaNGqaKiQuvWrdP999/vzIvW/rds2aJNmzapsLBQI0aMUHl5ubKysuT3+zVjxgxnXrT2f77O6tPm9+L06dO655571NzcrLVr137t/Gjov7S0VL/97W9VVlbW7hps659DPJL69++v7t27t0p8tbW1rdJhpJg3b562b9+uXbt2adCgQc64z+eTpIv26vP51NTUpLq6uovO+c9//tPq5/73v/8N23tWWlqq2tpapaamKiYmRjExMSouLtbvfvc7xcTEOHVFY++SNHDgQF111VVBY1deeaWOHj0qKbr/7CVp0aJFWrp0qe655x6NHDlS06dP18MPP6zc3FxJ0d9/i87s0+fztfo5dXV1On36dNjfi9OnT+uuu+5SZWWlioqKnNUTKbr7f+ONN1RbW6vBgwc7n4NVVVV65JFHNHToUEmR0z8BRVLPnj2VmpqqoqKioPGioiKlpaWFqapvxhijuXPnauvWrXrttdeUlJQUtD8pKUk+ny+o16amJhUXFzu9pqamqkePHkFzjh8/roMHDzpzbrjhBgUCAe3du9eZ89ZbbykQCITtPZswYYIOHDig8vJyZxs9erSmTZum8vJyDRs2LGp7l6SxY8e2uqT8yJEjzg04o/nPXpJOnjypbt2CP9K6d+/uXGYc7f236Mw+b7jhBh08eFDHjx935uzYsUNut1upqakd2ufFtIST999/Xzt37lS/fv2C9kdz/9OnT9e7774b9Dno9/u1aNEivfLKK5IiqP9vfZptlGi5zPiZZ54xhw4dMllZWaZ3797mo48+Cndp7fKzn/3MeDwe869//cscP37c2U6ePOnMWbVqlfF4PGbr1q3mwIED5t57723zEsRBgwaZnTt3mrKyMvP973+/zUvQrr76arNnzx6zZ88eM3LkyLBfanq+r17FY0x09753714TExNjVq5cad5//33zl7/8xcTFxZlNmzY5c6K5/xkzZpjLL7/cucx469atpn///mbx4sXOnGjpv6Ghwezfv9/s37/fSDJ5eXlm//79zlUqndVny2WmEyZMMGVlZWbnzp1m0KBBHX6Z7cX6P336tLn99tvNoEGDTHl5edDnYGNjY9T335bzr+IxJjL6J6B8xR/+8AczZMgQ07NnT3Pdddc5l+ZGEkltbhs3bnTmNDc3m8cee8z4fD7jdrvNjTfeaA4cOBD0OqdOnTJz5841ffv2NbGxsea2224zR48eDZrz6aefmmnTppn4+HgTHx9vpk2bZurq6jqhy0t3fkCJ9t7//ve/m5SUFON2u83w4cPN+vXrg/ZHc//19fVmwYIFZvDgwaZXr15m2LBhZvny5UG/lKKl/127drX593zGjBnGmM7ts6qqymRmZprY2FjTt29fM3fuXPPll192ZPsX7b+ysvKCn4O7du2K+v7b0lZAiYT+XcYY8+3XYQAAAEKHc1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsM7/A4+28P4NQmM6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.description.str.len().describe())\n",
    "\n",
    "plt.hist(data.description.str.len(), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf1f6",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "\n",
    "We could create :\n",
    "\n",
    "    * Lenght of description (dunno what informations it could provide yet)\n",
    "    * Toold required (Excel, Google Tag Manager ...)\n",
    "    * Coding languages required (R, Python, SQL ...)\n",
    "    * Skills required (reporting, data visualization)\n",
    "    * Required experience\n",
    "    * Duration of contract\n",
    "    * Avantages (ticket resto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "763cf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_employment_type(row):\n",
    "    employment_type_dict = {\n",
    "        'CDI': ['cdi',],\n",
    "        'internship': ['stage', 'internship', 'stagiaire'],\n",
    "        'apprenticeship': ['alternant', '(apprenti)', 'professionnalisation', 'alternance'],\n",
    "        'CDD': ['cdd'],\n",
    "        'freelance': ['freelance', 'prestataire', 'freelancer'],\n",
    "        'interim': ['interim'],\n",
    "        'consultant': ['consultant'],\n",
    "    }\n",
    "    for employment_type, keywords in employment_type_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return employment_type\n",
    "    return np.NaN\n",
    "\n",
    "def create_seniority_level(row):\n",
    "    seniority_level_dict = {\n",
    "        'senior': ['senior', 'advanced', 'avance', 'sr', 'experimente'],\n",
    "        'mid-level': ['confirme'],\n",
    "        'junior': ['junior', 'debutant', 'jr', 'entre-level'],\n",
    "    }\n",
    "    for seniority_level, keywords in seniority_level_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U): #or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return seniority_level\n",
    "    return np.NaN\n",
    "\n",
    "def create_executive_title(row):\n",
    "    executive_title_dict = {\n",
    "        'lead': ['lead'],\n",
    "        'Director': ['director', 'directeur'],\n",
    "        'Manager': ['manager', 'project manager', 'chef de projet'],\n",
    "        'Assistant': ['assistant'],\n",
    "        'Chief': ['chief'],\n",
    "        'Head': ['head'],\n",
    "        'Supervisor': ['supervisor'],\n",
    "    }\n",
    "    for executive_title, keywords in executive_title_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U):\n",
    "                return executive_title\n",
    "    return np.NaN\n",
    "\n",
    "def create_job_specialization(row):\n",
    "    job_specialization_dict = {\n",
    "        'career': ['career'],\n",
    "        'web': ['web'],\n",
    "        'media': ['media'],\n",
    "        'online': ['online'],\n",
    "        'marketing': ['marketing', 'market'],\n",
    "        'crm': ['crm'],\n",
    "        'assurance': ['indemnisation'],\n",
    "        'immobilier': ['immobilier'],\n",
    "        'product': ['product'],\n",
    "        'people': ['people', 'hr', 'human', 'workforce'],\n",
    "        'informatique': ['informatique'],\n",
    "        'supply_chain': ['supply'],\n",
    "        'logistique': ['logistique'],\n",
    "        'medical': ['medical', 'clinique', 'sante'],\n",
    "        'finance': ['finance'],\n",
    "        'recherche': ['recherche'],\n",
    "        'tv': ['tv'],\n",
    "        'game': ['game'],\n",
    "        'geo': ['geo'],\n",
    "    }\n",
    "    for job_specialization, keywords in job_specialization_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword,unidecode(row['title']), re.I|re.U):\n",
    "                return job_specialization\n",
    "    return np.NaN\n",
    "\n",
    "def create_remote(row):\n",
    "    remote_dict = {\n",
    "        'Y': ['remote', 'teletravail hybride', 'teletravail complet', 'jour de teletravail',\n",
    "                   'jours de teletravail', 'teletravail partiel', 'distanciel', 'teletravail', '(TT)']        \n",
    "    }\n",
    "    for remote, keywords in remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "def create_full_partial_remote(row):\n",
    "    full_partial_remote_dict = {\n",
    "        'full': ['full remote', 'teletravail complet'],\n",
    "        'partial_remote': ['teletravail de','teletravail hybride', 'jours de teletravail', 'teletravail partiel', 'jour de teletravail',]\n",
    "    }\n",
    "    for remote, keywords in full_partial_remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "\n",
    "\n",
    "#data = data.applymap(lambda x: unidecode(x) if isinstance(x, str) else x) # used for accent-insensitive search, got replaced directly in functions (see above)\n",
    "# lower every string of dataframe for easier search\n",
    "data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "data['executive_title'] = data.apply(create_executive_title, axis=1)\n",
    "data['seniority_level'] = data.apply(create_seniority_level, axis=1) \n",
    "data['employment_type'] = data.apply(create_employment_type, axis=1) \n",
    "data['job_specialization'] = data.apply(create_job_specialization, axis=1) \n",
    "data['remote'] = data.apply(create_remote, axis=1) \n",
    "data['full_partial_remote'] = data.apply(create_full_partial_remote, axis=1)\n",
    "data = pp.salary_prepro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_col(data, columns):\n",
    "    \n",
    "    data.drop(['extensions'], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a0d2",
   "metadata": {},
   "source": [
    "## Job description - Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a8e5db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common stopwords from nltk library in both french and english\n",
    "stop_words = list(set(stopwords.words('french')).union(set(stopwords.words('english'))))\n",
    "\n",
    "punct_mark = [\"•\"]\n",
    "\n",
    "apostrophes_stop_words = [\"d'\", \"c'\", \"j'\", \"m'\", \"n'\", \"s'\", \"t'\", \"l'\" ,\"qu'\", \n",
    "                          \"jusqu'\", \"lorsqu'\", \"puisqu'\", \"quoiqu'\", \"qu'il\", \n",
    "                          \"qu'on\", \"qu'un\", \"qu'une\", \"sans qu'\", \"étant qu'\",\n",
    "                         \"qu’\", \"jusqu’\", \"lorsqu’\", \"puisqu’\", \"quoiqu’\", \n",
    "                          \"qu’il\", \"qu’on\", \"qu’un\", \"qu’une\", \"sans qu’\", \"étant qu’\",\n",
    "                         \"d’\", \"c’\", \"j’\", \"m’\", \"n’\", \"s’\", \"t’\", \"d’un\", \"d’une\", \"c’est\"]\n",
    "additional_fr_stop_words = [\n",
    "    \"au\", \"aux\", \"avec\", \"ce\", \"ces\", \"dans\", \"de\", \"des\", \"du\", \"elle\",\n",
    "    \"en\", \"et\", \"eux\", \"il\", \"je\", \"la\", \"le\", \"leur\", \"lui\", \"ma\",\n",
    "    \"mais\", \"me\", \"même\", \"mes\", \"moi\", \"mon\", \"ne\", \"nos\", \"notre\",\n",
    "    \"nous\", \"on\", \"ou\", \"par\", \"pas\", \"pour\", \"qu\", \"que\", \"qui\", \"sa\",\n",
    "    \"se\", \"ses\", \"son\", \"sur\", \"ta\", \"te\", \"tes\", \"toi\", \"ton\", \"tu\",\n",
    "    \"un\", \"une\", \"vos\", \"votre\", \"vous\", \"c’\", \"d’\", \"j’\", \"l’\", \"à\", \"m’\",\n",
    "    \"n’\", \"s’\", \"t’\", \"y’\", \"été\", \"étée\", \"étées\", \"étés\", \"étant\", \"suis\",\n",
    "    \"es\", \"est\", \"sommes\", \"êtes\", \"sont\", \"serai\", \"seras\", \"sera\",\n",
    "    \"serons\", \"serez\", \"seront\", \"serais\", \"serait\", \"serions\", \"seriez\",\n",
    "    \"seraient\", \"étais\", \"était\", \"étions\", \"étiez\", \"étaient\", \"fus\",\n",
    "    \"fut\", \"fûmes\", \"fûtes\", \"furent\", \"sois\", \"soit\", \"soyons\", \"soyez\",\n",
    "    \"soient\", \"fusse\", \"fusses\", \"fût\", \"fussions\", \"fussiez\", \"fussent\",\n",
    "    \"ayant\", \"eu\", \"eue\", \"eues\", \"eus\", \"ai\", \"as\", \"avons\", \"avez\",\n",
    "    \"ont\", \"aurai\", \"auras\", \"aura\", \"aurons\", \"aurez\", \"auront\", \"aurais\",\n",
    "    \"aurait\", \"aurions\", \"auriez\", \"auraient\", \"avais\", \"avait\", \"avions\",\n",
    "    \"aviez\", \"avaient\", \"eut\", \"eûmes\", \"eûtes\", \"eurent\", \"aie\", \"aies\",\n",
    "    \"ait\", \"ayons\"]\n",
    "\n",
    "all_stop_words = set(stop_words + apostrophes_stop_words + additional_fr_stop_words + punct_mark)\n",
    "all_stop_words = list(all_stop_words)\n",
    "\n",
    "\n",
    "# Copy the original description column to a new column\n",
    "data['description_normalized'] = data['description'].copy().str.lower()\n",
    "\n",
    "pattern = r\"(\\b\\w+)[`'’](\\w+)?\\b\"\n",
    "replacement = r\"\\2\"\n",
    "\n",
    "data['description_normalized'] = data['description_normalized'].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "\n",
    "# remove stop words in data.description_normalized\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: ' '.join([word for word in text.split() if word not in all_stop_words]))\n",
    "    \n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: text.translate(translator))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5836e",
   "metadata": {},
   "source": [
    "## Job description normalized - N-grams tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c1e68340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most common trigrams are:\n",
      "\n",
      "personnes situation handicap 148\n",
      "learning deep learning 108\n",
      "machine learning deep 107\n",
      "bac 5 publiée 92\n",
      "data scientist hf 87\n",
      "\n",
      "The 5 most common bigrams are:\n",
      "\n",
      "data science 1242\n",
      "machine learning 942\n",
      "data scientist 840\n",
      "data analyst 696\n",
      "big data 574\n"
     ]
    }
   ],
   "source": [
    "trigrams = []\n",
    "bigrams = []\n",
    "\n",
    "def get_most_common_ngrams(data, n, k):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings, an integer n (for the n-gram size), \n",
    "    and an integer k (for the number of most common n-grams to return). \n",
    "    It returns a list of the k most common n-grams in the input data.\n",
    "    \"\"\"\n",
    "    ngrams_list = []  # Create an empty list to store the n-grams\n",
    "    \n",
    "    for text in data:\n",
    "        \n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate the n-grams and add them to the list\n",
    "        ngrams_list.extend(list(ngrams(words, n)))\n",
    "        \n",
    "    # Count the occurrences of each n-gram using FreqDist\n",
    "    freq_dist = FreqDist(ngrams_list)\n",
    "    \n",
    "    # Get the k most common n-grams and return them\n",
    "    return freq_dist.most_common(k)\n",
    "\n",
    "trigrams = get_most_common_ngrams(data.description_normalized, 3, 1000)\n",
    "bigrams = get_most_common_ngrams(data.description_normalized, 2, 1000)\n",
    "\n",
    "    \n",
    "# Print the most common trigrams and bigrams\n",
    "print(\"The 5 most common trigrams are:\\n\")\n",
    "for trigram, count in trigrams[:5]:\n",
    "    print(' '.join(trigram), count)\n",
    "\n",
    "print(\"\\nThe 5 most common bigrams are:\\n\")\n",
    "for bigram, count in bigrams[:5]:\n",
    "    print(' '.join(bigram), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b2ff9",
   "metadata": {},
   "source": [
    "## Tokenization w/ CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "405ce6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data       10602\n",
       "données     5345\n",
       "équipe      2655\n",
       "plus        2574\n",
       "clients     2047\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the fit_transform method to extract features from the text data\n",
    "features = vectorizer.fit_transform(data.description_normalized)\n",
    "\n",
    "# The result is a sparse matrix, which can be easily converted to a dense array using .toarray()\n",
    "features_array = features.toarray()\n",
    "\n",
    "# get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# put together term-doc matrix and feature names in pd.DataFrame\n",
    "word_description_occ = pd.DataFrame(data=features_array, columns=feature_names)\n",
    "\n",
    "# To get the most common words, we can sum up the values in each row\n",
    "word_counts = word_description_occ.sum(axis=0)\n",
    "\n",
    "# And then sort the word_counts in descending order\n",
    "most_common_words = word_counts.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "print(\"5 most common words: \\n\")\n",
    "most_common_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905215b1",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d33abb",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## replace keywords for easier keyword extraction\n",
    "\n",
    "data.description_prepro.replace(['(?i)(Google Tag Manager)|\\b(GTM)\\b', \"(?i)\\b(GA4)\\b|\\b(GA)\\b\", '(?i)\\b(Google Colab)\\b', '\\b(GCP)\\b|(google cloud plateform)\\b'], value=['Google Tag Manager', 'Google Analytics', 'Google Colaboratory', 'Google Cloud Plateform'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)\\b(AWS)\\b|Amazon Web Services'], value=['Amazon Web Services'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)SKlearn|Scikit'], value=['ScikitLearn'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)data viz'], value=['data visualisation'] ,regex=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2d4b649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "'sas', 'Spark', 'BigML', 'D3.js', 'MATLAB', 'Excel', 'ggplot2', 'Tableau', 'Jupyter', \n",
    "'Matplotlib', 'NLTK', 'TensorFlow', 'Weka', 'Google Analytics', 'KNIME', \n",
    "'Flink', 'MongoDB', 'Minitab', 'Rapidminer', 'DataRobot', 'NLTK', 'Hadoop', 'Power BI', \n",
    "'QlikView', 'MySQL', 'Neo4j', 'HANA', 'Spotfire', 'SPSS', 'STATA', 'RiverLogic', \n",
    "'Lumira', 'Pig', 'Keras', 'NumPy', 'PyTorch', 'Seaborn', 'Wolfram Mathematica', \n",
    "'WebSockets', 'Algorithms.io', 'ForecastThis', 'BigQuery', 'GitHub', \n",
    "'Pycharm', 'Visual Studio Code', 'Linux', 'Windows', 'macOS', 'Google Colaboratory', \n",
    "'Google Cloud Plateform', 'Watson Studio', 'Amazon Web Services', \n",
    "'EC2', 'Amazon Elastic Compute Cloud', 'Microsoft Azure', \n",
    "'Nvidia Jetson Nano', 'Arduino', 'Beam', 'Semantria', 'Trackur', 'Cassandra', 'OctoParse', \n",
    "'Content Grabber', 'OpenRefine', 'Google Fusion Table', 'scipy', 'pandas', 'NPM', 'Redshift', \n",
    "'Snowflake', 'Alteryx', 'Domino Data Lab', 'Kafka', 'Hbase', 'Elasticsearch', 'Maven', \n",
    "'Ansible', 'Gitlab', 'Jenkins', 'Bash', 'IntelliJ', 'MySQL', 'PostreSQL', 'Sonar', \n",
    "'Jira', 'OpenCV', 'TimescaleDB', 'Grafana', 'Google Sheet', 'Pig', 'Talend', 'MSBI',\n",
    "'SAP BO', 'Abode Campaign', 'Google Data Studio', 'Dataform', 'Looker',\n",
    "'Mode', 'Metabase', 'Power Query', 'Power Pivot', 'DataIku', 'MLFlow', 'DVC', 'Kibana', 'SageMaker',\n",
    "'Minio', 'S3', 'MQTT'\n",
    "]\n",
    "\n",
    "keywords_programming = [\n",
    "'sql', 'python', 'r', 'c', 'c#', 'javascript', 'js',  'java', 'scala', 'sas', 'matlab', \n",
    "'c++', 'c/c++', 'perl', 'go', 'typescript', 'bash', 'html', 'css', 'php', 'powershell', 'rust', \n",
    "'kotlin', 'ruby',  'dart', 'assembly', 'swift', 'vba', 'lua', 'groovy', 'delphi', 'objective-c', \n",
    "'haskell', 'elixir', 'julia', 'clojure', 'solidity', 'lisp', 'f#', 'fortran', 'erlang', 'apl', \n",
    "'cobol', 'ocaml', 'crystal', 'javascript/typescript', 'golang', 'nosql', 'mongodb', 't-sql', 'no-sql',\n",
    "'visual_basic', 'pascal', 'mongo', 'pl/sql',  'sass', 'vb.net', 'mssql', \n",
    "]\n",
    "\n",
    "keywords_libraries = [\n",
    "'scikit-learn', 'jupyter', 'theano', 'openCV', 'spark', 'nltk', 'mlpack', 'chainer', 'fann', 'shogun', \n",
    "'dlib', 'mxnet', 'node.js', 'vue', 'vue.js', 'keras', 'ember.js', 'jse/jee',\n",
    "]\n",
    "\n",
    "keywords_analyst_tools = [\n",
    "'excel', 'tableau',  'word', 'powerpoint', 'looker', 'powerbi', 'outlook', 'azure', 'jira', 'twilio',  'snowflake', \n",
    "'shell', 'linux', 'sas', 'sharepoint', 'mysql', 'visio', 'git', 'mssql', 'powerpoints', 'postgresql', 'spreadsheets',\n",
    "'seaborn', 'pandas', 'gdpr', 'spreadsheet', 'alteryx', 'github', 'postgres', 'ssis', 'numpy', 'power_bi', 'spss', 'ssrs', \n",
    "'microstrategy',  'cognos', 'dax', 'matplotlib', 'dplyr', 'tidyr', 'ggplot2', 'plotly', 'esquisse', 'rshiny', 'mlr',\n",
    "'docker', 'linux', 'jira',  'hadoop', 'airflow', 'redis', 'graphql', 'sap', 'tensorflow', 'node', 'asp.net', 'unix',\n",
    "'jquery', 'pyspark', 'pytorch', 'gitlab', 'selenium', 'splunk', 'bitbucket', 'qlik', 'terminal', 'atlassian', 'unix/linux',\n",
    "'linux/unix', 'ubuntu', 'nuix', 'datarobot',\n",
    "]\n",
    "\n",
    "keywords_cloud_tools = [\n",
    "'aws', 'azure', 'gcp', 'snowflake', 'redshift', 'bigquery', 'aurora',\n",
    "]\n",
    "\n",
    "keywords_general_tools = [\n",
    "'microsoft', 'slack', 'apache', 'ibm', 'html5', 'datadog', 'bloomberg',  'ajax', 'persicope', 'oracle', \n",
    "]\n",
    "\n",
    "keywords_general = [\n",
    "'coding', 'server', 'database', 'cloud', 'warehousing', 'scrum', 'devops', 'programming', 'saas', 'ci/cd', 'cicd', \n",
    "'ml', 'data_lake', 'frontend','front-end', 'back-end', 'backend', 'json', 'xml', 'ios', 'kanban', 'nlp',\n",
    "'iot', 'codebase', 'agile/scrum', 'agile', 'ai/ml', 'ai', 'paas', 'machine_learning', 'macros', 'iaas',\n",
    "'fullstack', 'dataops', 'scrum/agile', 'ssas', 'mlops', 'debug', 'etl', 'a/b', 'slack', 'erp', 'oop', \n",
    "'object-oriented', 'etl/elt', 'elt', 'dashboarding', 'big-data', 'twilio', 'ui/ux', 'ux/ui', 'vlookup', \n",
    "'crossover',  'data_lake', 'data_lakes', 'bi', 'pack office'\n",
    "]\n",
    "\n",
    "ml_tools = [\n",
    "'Tensorflow', 'Keras', 'PyTorch', 'ScikitLearn', 'sklearn', 'scikit'\n",
    "]\n",
    "\n",
    "big_data_tools = [\n",
    "'Hadoop', 'HDFS', 'YARN', 'Hive', 'Map', 'Reduce', 'Tez', 'Spark'\n",
    "]\n",
    "\n",
    "skills = [\n",
    "'informatique décisionnelle', 'Extraction', 'Nettoyage', 'transformation', 'ingestion', \n",
    "'data visualisation', 'modélisation', 'reporting', 'veille technologique', 'data mining',\n",
    "'kpi', 'computer vision'\n",
    "]\n",
    "\n",
    "soft_skills = [\n",
    "\"Esprit d'analyse\", \"Sens du service\", \"Rigueur\", \"communication\", 'positif', 'créatif', \n",
    "'pragmatique', 'souple', 'agile', \"Autonome\", 'Polyvalent', 'travailler en équipe', \"esprit d'équipe\"\n",
    "'esprit de synthèse', 'aisance relationnelle', 'force de proposition', \"capacité d'analyse\",\n",
    "\"anglais\", \"espagnol\", \"francais\"\n",
    "]\n",
    "\n",
    "coding_languages = [\n",
    "'SQL', 'Python', 'R', 'Julia', 'Scala', 'C++', 'Java', 'Javascript', 'Go'\n",
    "]\n",
    "\n",
    "# Add up all keyword lists\n",
    "all_kw = tools + keywords_programming + keywords_libraries + keywords_analyst_tools + keywords_cloud_tools + keywords_general_tools + keywords_general + ml_tools + big_data_tools + skills + soft_skills + coding_languages\n",
    "\n",
    "# Lower case & remove duplicates\n",
    "all_kw = list(set([word.lower() for word in all_kw]))\n",
    "\n",
    "# keyword extraction\n",
    "data['kw_extraction'] = data['description_normalized'].apply(lambda x: [])\n",
    "\n",
    "for text in enumerate(data.description_normalized):\n",
    "\n",
    "    kw_list = [word for word in all_kw if word in text[1]]\n",
    "    data.loc[text[0], \"kw_extraction\"].extend(kw_list)\n",
    "\n",
    "# number of keywords extracted\n",
    "data['len_list'] = data['kw_extraction'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893224",
   "metadata": {},
   "source": [
    "## Create columns : \"chars before / after {keywork}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e62d8ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining constants\n",
    "keywords = ['durée', 'stage', 'contrat', 'localisation|lieu|location', 'statut', 'salaire', 'formation', 'profil', \n",
    "            'mission', 'avantage', 'experience', 'soft', 'langue']\n",
    "n_chars_after = 20\n",
    "m_chars_before = 60\n",
    "#col_name = f'charsss_around_{kw}'\n",
    "\n",
    "# loop over keywords list\n",
    "for kw in keywords:\n",
    "    \n",
    "    # define our new columns\n",
    "    data[f'chars_around_{kw}'] = 'NC '\n",
    "\n",
    "    # loop over each string in data.description\n",
    "    for i, text in zip(data.index, data['description_normalized']):\n",
    "        \n",
    "        # get iterator over all keywords matches in each string\n",
    "        search_obj = re.finditer(kw, text, re.I)\n",
    "\n",
    "        if search_obj: # ... is true, then ...\n",
    "\n",
    "            # get each keyword match in string\n",
    "            for match in search_obj:\n",
    "\n",
    "                # Find the start index of the keyword\n",
    "                start = match.span()[0]\n",
    "\n",
    "                # Find the end index of the keyword\n",
    "                end = match.span()[1]\n",
    "\n",
    "                # Truncate line to get only 'n' characters before and after the keyword\n",
    "                line = text[start-m_chars_before:end+n_chars_after]\n",
    "                \n",
    "                # add up ever line containing keyword match (if several) in corresponding cells\n",
    "                data.loc[i, f'chars_around_{kw}'] += line\n",
    "                \n",
    "                # get rid of by default 'NC' string\n",
    "                data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}'].replace('NC ', '')\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # if no keyword match then keep by default value\n",
    "            data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
