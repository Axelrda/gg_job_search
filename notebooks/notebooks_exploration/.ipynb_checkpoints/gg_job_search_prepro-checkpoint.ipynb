{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef84455",
   "metadata": {},
   "source": [
    "# Scraping bullshit jobs ... a long way to hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "9487d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# library for plotting data over maps\n",
    "import shapefile as shp\n",
    "\n",
    "# libraries used for tokenization\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
    "\n",
    "# Libraries used to remove similar job description based on cosine similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# libraries used for text normalization\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get exchange rate EUR/US DOLLAR\n",
    "import requests\n",
    "\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd10655",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ced546c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/axel/ds_projects/projects/gg_job_search/data/gg_job_search_all_RAW.csv')\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e1272",
   "metadata": {},
   "source": [
    "## General overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "0bcbba87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7808 entries, 0 to 7807\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      7808 non-null   int64  \n",
      " 1   title           7808 non-null   object \n",
      " 2   company_name    7808 non-null   object \n",
      " 3   location        7797 non-null   object \n",
      " 4   via             7808 non-null   object \n",
      " 5   description     7748 non-null   object \n",
      " 6   job_highlights  1475 non-null   object \n",
      " 7   related_links   7748 non-null   object \n",
      " 8   thumbnail       6075 non-null   object \n",
      " 9   extensions      7808 non-null   object \n",
      " 10  job_id          7532 non-null   object \n",
      " 11  posted_at       7808 non-null   object \n",
      " 12  schedule_type   7736 non-null   object \n",
      " 13  date_time       7808 non-null   object \n",
      " 14  search_query    7808 non-null   object \n",
      " 15  Unnamed: 0.1    6333 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 976.1+ KB\n",
      "None \n",
      "\n",
      "Unnamed: 0        6333\n",
      "title             1420\n",
      "company_name      1186\n",
      "location           337\n",
      "via                 91\n",
      "description       2309\n",
      "job_highlights     350\n",
      "related_links     3127\n",
      "thumbnail          770\n",
      "extensions         670\n",
      "job_id            2704\n",
      "posted_at           26\n",
      "schedule_type        4\n",
      "date_time          538\n",
      "search_query         3\n",
      "Unnamed: 0.1        10\n",
      "dtype: int64 \n",
      "\n",
      "data engineer     3008\n",
      "data scientist    2421\n",
      "data analyst      2379\n",
      "Name: search_query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.info(), '\\n')\n",
    "print(data.nunique(), '\\n')\n",
    "print(data.search_query.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01234",
   "metadata": {},
   "source": [
    "## Null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "b7e83723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location            11\n",
       "description         60\n",
       "job_highlights    6333\n",
       "related_links       60\n",
       "thumbnail         1733\n",
       "job_id             276\n",
       "schedule_type       72\n",
       "Unnamed: 0.1      1475\n",
       "dtype: int64"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47929c2",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "583af771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates : 546\n",
      "Number of duplicates (based on identical job_id) : 5103 \n",
      " ======================================================================\n",
      "Search query results :\n",
      " data scientist    1370\n",
      "data analyst       720\n",
      "data engineer      615\n",
      "Name: search_query, dtype: int64 \n",
      " ======================================================================\n",
      "Final number of rows :  2705\n",
      "Final number of columns :  17\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates :',data[data.duplicated()].shape[0])\n",
    "print('Number of duplicates (based on identical job_id) :', data[data.duplicated(['job_id'])].shape[0],'\\n', '='*70) \n",
    "\n",
    "# removing duplicates based on job_id\n",
    "data = data.drop_duplicates(['job_id'])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e020",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Purely based job id, we can skim off a lots of duplicates. Nonetheless i still found a few of identical / near-identical job descriptions, maybe because [sometimes recruiters or companies post the same advert for a job which results in duplicate data.](https://medium.com/analytics-vidhya/data-science-job-search-using-nlp-and-lda-in-python-12ecbfac79f9)\n",
    "\n",
    "To get rid off these ones, i could use cosine similatity between job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e082a5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates (based on cosine similarity) :  981\n",
      "Final number of rows :  1724\n",
      "Final number of columns :  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59211/1214297801.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n"
     ]
    }
   ],
   "source": [
    "### removing duplicates based on cosine similarity between \n",
    "## job descriptions (code from Thomas Caffrey, see link above)\n",
    "\n",
    "# Defining our collection of job description texts to tokenize\n",
    "data.description = data['description'].fillna('')\n",
    "corpus = data['description']\n",
    "\n",
    "# instantiate CountVectorizer object\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Fit_transform to vectorize each job description (map terms to feature indices)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarities and put it in dataframe\n",
    "cos_df = pd.DataFrame(cosine_similarity(X_train_counts))\n",
    "\n",
    "\n",
    "## reshape dataframe for easier comparison\n",
    "\n",
    "# get arrays of rows indices and col indices from col_df.shape\n",
    "i, j = np.indices(cos_df.shape).reshape(2,-1)\n",
    "\n",
    "# reshape values to get a 1D array \n",
    "cos_values = cos_df.values.reshape(-1)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'i': i, 'j': j, 'sim':cos_values})\n",
    "\n",
    "# get cosine similarity values only above 0.98 \n",
    "cos_rem = cos_sim_df[(cos_sim_df['sim'] > 0.98) & (i!=j)]\n",
    "\n",
    "# Method to remove duplicates but keep first instance:\n",
    "# Trying to drop duplicates on i and j columns won't work as the row numbers of duplicates are either in i or j not both.\n",
    "# Setting another column that combines the i & j values ensures that duplicates can be dropped.\n",
    "\n",
    "cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n",
    "drop_rows = np.unique(cos_rem.drop_duplicates('i*j')['i'].values)\n",
    "\n",
    "# keep only non-duplicated job postings\n",
    "data = data[~data.index.isin(drop_rows)] \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Number of duplicates (based on cosine similarity) : ' ,drop_rows.shape[0])\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599a6b",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "4d501f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['il y a 17 heur...\n",
       "1       ['il y a 11 heur...\n",
       "2       ['il y a 19 heur...\n",
       "3       ['il y a 14 heur...\n",
       "4       ['il y a 22 heur...\n",
       "               ...         \n",
       "1719    ['il y a 9 heure...\n",
       "1720    ['il y a 12 heur...\n",
       "1721    ['il y a 10 heur...\n",
       "1722    ['il y a 9 heure...\n",
       "1723    ['il y a 9 heure...\n",
       "Name: extensions, Length: 1724, dtype: object"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d1851",
   "metadata": {},
   "source": [
    "## Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "aaef9dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(data.title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242f91",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Based on this sample of job titles, we could create : \n",
    "    \n",
    "    * Contract_type (full-time, part-time ...) \n",
    "    * Contract_status (CDI, CDD, work-study, internship ...)\n",
    "    * Duration of Contract (Duration/Undetermined)\n",
    "    * Experience ( Senior, Junior ...)\n",
    "    * Data Specialization (Supply chain, Marketing, Clinical ...)\n",
    "    * Multiple titles (Analyst/Scientist, Scientist/ML Engineer, Manager/Analyst ...)\n",
    "    * Specific expertise asked for (Python, Power BI ...)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1679e1",
   "metadata": {},
   "source": [
    "## Explore company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "3400f598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique companies : 1053\n"
     ]
    }
   ],
   "source": [
    "print('number of unique companies :', data.company_name.nunique())\n",
    "#data.company_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1583ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 20\n",
    "#data.loc[data['company_name'] == 'Unspecified', ['description']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05598b",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "When ***Unspecified***, companies name can be found in description column.\n",
    "\n",
    "Possible new columns :\n",
    "\n",
    "    * Group/Holding (Y/N/NC)\n",
    "    * Interim company (Y/N/NC)\n",
    "\n",
    "Based on the number of job posting per company we could potentially infer about : **size of company ? / Amount of data to work on** / \n",
    "\n",
    "Adding a time variable and much more data, the number of similar / identical job postings for the same company could maybe give insights on the **company's turnover rate / company's growth / magnitude of need-urgency to hire** ...  \n",
    "\n",
    "It seems like extracting additional informations without more context will be difficult. Having access to each company's structure information we could create :\n",
    "\n",
    "    * Size of company\n",
    "    * Industry\n",
    "    * Public / Private\n",
    "    \n",
    "We'll see if can extract more related informations in the following columns. Otherwise, we could try to scrap **Glassdoor databases** (or similar) to get those informations.remains to be seen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0ce67",
   "metadata": {},
   "source": [
    "## Explore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "f01afb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique locations :  251\n"
     ]
    }
   ],
   "source": [
    "print('number of unique locations : ', data.location.nunique())\n",
    "#data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "29d39825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "#data.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cabad",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "We could create a map of th repartition of job posting based on location provided.\n",
    "\n",
    "Some companies don't provide precise location *(ex : location = FRANCE)* and the information is not available in description column either. Further investigations will be needed for these companies, perhaps in conjunction with other databases *(GLASSDOOR / SIRENE databases for instance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2d818",
   "metadata": {},
   "source": [
    "## Explore via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "79a48ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique job plateforms :  79\n"
     ]
    }
   ],
   "source": [
    "print('number of unique job plateforms : ', data.via.nunique())\n",
    "# data.via.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b57edd",
   "metadata": {},
   "source": [
    "## Explore description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "acda0f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1724.000000\n",
      "mean      2941.443735\n",
      "std       1622.706134\n",
      "min          0.000000\n",
      "25%       1782.000000\n",
      "50%       2678.000000\n",
      "75%       3849.000000\n",
      "max      14170.000000\n",
      "Name: description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.description.str.len().describe())\n",
    "\n",
    "plt.hist(data.description.str.len(), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf1f6",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "\n",
    "We could create :\n",
    "\n",
    "    * Lenght of description (dunno what informations it could provide yet)\n",
    "    * Toold required (Excel, Google Tag Manager ...)\n",
    "    * Coding languages required (R, Python, SQL ...)\n",
    "    * Skills required (reporting, data visualization)\n",
    "    * Required experience\n",
    "    * Duration of contract\n",
    "    * Avantages (ticket resto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "763cf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_employment_type(row):\n",
    "    employment_type_dict = {\n",
    "        'CDI': ['cdi',],\n",
    "        'internship': ['stage', 'internship', 'stagiaire'],\n",
    "        'apprenticeship': ['alternant', '(apprenti)', 'professionnalisation', 'alternance'],\n",
    "        'CDD': ['cdd'],\n",
    "        'freelance': ['freelance', 'prestataire', 'freelancer'],\n",
    "        'interim': ['interim'],\n",
    "        'consultant': ['consultant'],\n",
    "    }\n",
    "    for employment_type, keywords in employment_type_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return employment_type\n",
    "    return np.NaN\n",
    "\n",
    "def create_seniority_level(row):\n",
    "    seniority_level_dict = {\n",
    "        'senior': ['senior', 'advanced', 'avance', 'sr', 'experimente'],\n",
    "        'mid-level': ['confirme'],\n",
    "        'junior': ['junior', 'debutant', 'jr', 'entre-level'],\n",
    "    }\n",
    "    for seniority_level, keywords in seniority_level_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U): #or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return seniority_level\n",
    "    return np.NaN\n",
    "\n",
    "def create_executive_title(row):\n",
    "    executive_title_dict = {\n",
    "        'lead': ['lead'],\n",
    "        'Director': ['director', 'directeur'],\n",
    "        'Manager': ['manager', 'project manager', 'chef de projet'],\n",
    "        'Assistant': ['assistant'],\n",
    "        'Chief': ['chief'],\n",
    "        'Head': ['head'],\n",
    "        'Supervisor': ['supervisor'],\n",
    "    }\n",
    "    for executive_title, keywords in executive_title_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U):\n",
    "                return executive_title\n",
    "    return np.NaN\n",
    "\n",
    "def create_job_specialization(row):\n",
    "    job_specialization_dict = {\n",
    "        'career': ['career'],\n",
    "        'web': ['web'],\n",
    "        'media': ['media'],\n",
    "        'online': ['online'],\n",
    "        'marketing': ['marketing', 'market'],\n",
    "        'crm': ['crm'],\n",
    "        'assurance': ['indemnisation'],\n",
    "        'immobilier': ['immobilier'],\n",
    "        'product': ['product'],\n",
    "        'people': ['people', 'hr', 'human', 'workforce'],\n",
    "        'informatique': ['informatique'],\n",
    "        'supply_chain': ['supply'],\n",
    "        'logistique': ['logistique'],\n",
    "        'medical': ['medical', 'clinique', 'sante'],\n",
    "        'finance': ['finance'],\n",
    "        'recherche': ['recherche'],\n",
    "        'tv': ['tv'],\n",
    "        'game': ['game'],\n",
    "        'geo': ['geo'],\n",
    "    }\n",
    "    for job_specialization, keywords in job_specialization_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword,unidecode(row['title']), re.I|re.U):\n",
    "                return job_specialization\n",
    "    return np.NaN\n",
    "\n",
    "def create_remote(row):\n",
    "    remote_dict = {\n",
    "        'Y': ['remote', 'teletravail hybride', 'teletravail complet', 'jour de teletravail',\n",
    "                   'jours de teletravail', 'teletravail partiel', 'distanciel', 'teletravail', '(TT)']        \n",
    "    }\n",
    "    for remote, keywords in remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "def create_full_partial_remote(row):\n",
    "    full_partial_remote_dict = {\n",
    "        'full': ['full remote', 'teletravail complet'],\n",
    "        'partial_remote': ['teletravail de','teletravail hybride', 'jours de teletravail', 'teletravail partiel', 'jour de teletravail',]\n",
    "    }\n",
    "    for remote, keywords in full_partial_remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "# creation of holding feature Y/N\n",
    "\n",
    "conditions = [\n",
    "    \n",
    "    (data['company_name'].str.contains('Groupe(?i)|Holdings*(?i)')),\n",
    "    (data['description'].str.contains('ESN(?i)')).astype(bool)\n",
    "    \n",
    "]\n",
    "\n",
    "choices = ['Holding', 'ESN']\n",
    "\n",
    "data['holding'] = np.select(conditions, choices, default=np.NaN)\n",
    "\n",
    "data['holding'].value_counts()\n",
    "\n",
    "\n",
    "#data = data.applymap(lambda x: unidecode(x) if isinstance(x, str) else x) # used for accent-insensitive search, got replaced directly in functions (see above)\n",
    "# lower every string of dataframe for easier search\n",
    "data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "data['executive_title'] = data.apply(create_executive_title, axis=1)\n",
    "data['seniority_level'] = data.apply(create_seniority_level, axis=1) \n",
    "data['employment_type'] = data.apply(create_employment_type, axis=1) \n",
    "data['job_specialization'] = data.apply(create_job_specialization, axis=1) \n",
    "data['remote'] = data.apply(create_remote, axis=1) \n",
    "data['full_partial_remote'] = data.apply(create_full_partial_remote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7790e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_col(df):\n",
    "    \n",
    "    # Extract salary from extensions col\n",
    "    df['salary'] = pd.Series(df.extensions.str.split(', ', expand=True)[1])\n",
    "       \n",
    "    # Replace non-salary values to np.NaN\n",
    "    df['salary'].replace([\"'à plein temps']\", \"'à temps partiel']\", \"'stage']\", \"'prestataire']\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Convert null values to Nothing_found to make parsing easier\n",
    "    df.at[df.salary.isnull(), 'salary'] = \"nothing_found\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_og_salary_currency(df):\n",
    "    \n",
    "    df['og_salary_currency'] = df.salary.str.extract(r'(\\€|\\$us)', flags=re.IGNORECASE)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_og_salary_period(df):\n",
    "    \n",
    "    df['og_salary_period'] = df.salary.str.extract(r'(\\ban\\b|\\bmois\\b|\\bjour\\b)', flags=re.IGNORECASE)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def clean_salary(df):\n",
    "    \n",
    "    \n",
    "        ####### targeted clean for easier parsing #######\n",
    "\n",
    "        for index, salary_str in enumerate(df.salary):\n",
    "                \n",
    "                if 'nothing_found' not in salary_str:\n",
    "                    \n",
    "                    # split salary period from rest of string\n",
    "                    salary_str =  salary_str.split(' par')[0]\n",
    "                    \n",
    "                    # remove single quote\n",
    "                    salary_str = salary_str.split(\"'\")[1]\n",
    "\n",
    "                    # remove unicode chars\n",
    "                    salary_str = salary_str.replace(\"\\\\xa0\", \"\")\n",
    "                    salary_str = salary_str.replace(\"\\\\u202f\", \"\")\n",
    "\n",
    "                    #remove currency\n",
    "                    salary_str = salary_str.replace(\"€\", \"\")\n",
    "                    salary_str = salary_str.replace(\"$us\", \"\")\n",
    "\n",
    "                    df.at[index, 'salary'] = salary_str\n",
    "                    \n",
    "        return df\n",
    "\n",
    "def get_salary_range_and_mean(df):                    \n",
    "                 \n",
    "                                        \n",
    "        for index, row in df.iterrows():\n",
    "    \n",
    "            # extract boundaries of YEAR salaries given as a range + calculate mean salary\n",
    "            if (row['og_salary_period'] == 'an') and 'à' in row['salary']:\n",
    "\n",
    "                # remove k \n",
    "                row['salary'] = row['salary'].replace('k', '000')\n",
    "\n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.salary.split(' à ')[0].split(',')[0])\n",
    "                upper_bound = float(row.salary.split(' à ')[1].split(',')[0])\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if lower_bound < 1000:\n",
    "                    lower_bound = lower_bound * 1000\n",
    "\n",
    "                if upper_bound < 1000:\n",
    "                    upper_bound = upper_bound * 1000\n",
    "                \n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            # extract discrete YEAR salaries        \n",
    "            elif (row['og_salary_period'] == 'an') and 'à' not in row['salary']:\n",
    "\n",
    "                # remove k\n",
    "                row['salary'] = row['salary'].replace('k', '000')\n",
    "                row['salary'] = row['salary'].replace(',', '.')\n",
    "\n",
    "                # convert value to float\n",
    "                row['salary'] = float(row['salary'])\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if row['salary'] < 1000:\n",
    "                    row['salary'] = row['salary'] * 1000\n",
    "\n",
    "                # assign result to discrete salary column\n",
    "                df.at[index, 'discrete_salary'] = row['salary']\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            # extract boundaries of MONTH salaries given as a range + calculate mean salary   \n",
    "            elif (row['og_salary_period'] == 'mois') and 'à' in row['salary']:\n",
    "\n",
    "                # remove k and replace commas\n",
    "                row['salary'] = row['salary'].replace('k', '00')\n",
    "                row['salary'] = row['salary'].replace(',', '.')\n",
    "\n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.salary.split(' à ')[0])\n",
    "                upper_bound = float(row.salary.split(' à ')[1])\n",
    "\n",
    "                # re-establish a consistent value regarding to common month salaries\n",
    "                if lower_bound < 10:\n",
    "                    lower_bound = lower_bound * 100\n",
    "\n",
    "                if lower_bound < 1000:\n",
    "                    lower_bound = lower_bound * 10\n",
    "\n",
    "                if upper_bound < 10:\n",
    "                    upper_bound = upper_bound * 100\n",
    "\n",
    "                if upper_bound < 1000:\n",
    "                    upper_bound = upper_bound * 10\n",
    "\n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            # extract discrete MONTH salaries        \n",
    "            elif (row['og_salary_period'] == 'mois') and 'à' not in row['salary']:\n",
    "\n",
    "                # remove k and replace commas\n",
    "                row['salary'] = row['salary'].replace('k', '00')\n",
    "                row['salary'] = row['salary'].replace(',', '.')\n",
    "\n",
    "                # convert value to float\n",
    "                row['salary'] = float(row['salary'])\n",
    "\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if row['salary'] < 1000:\n",
    "                    row['salary'] = row['salary'] * 1000\n",
    "\n",
    "                # assign result to discrete salary column\n",
    "                df.at[index, 'discrete_salary'] = row['salary']                 \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # extract boundaries of DAY salaries given as a range + calculate mean salary   \n",
    "            elif (row['og_salary_period'] == 'jour') and 'à' in row['salary']:\n",
    "                \n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.salary.split(' à ')[0])\n",
    "                upper_bound = float(row.salary.split(' à ')[1])\n",
    "\n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "                    \n",
    "                    \n",
    "        return df\n",
    "    \n",
    "            \n",
    " # Define a global variable to cache the exchange rate value\n",
    "cached_exchange_rate = None\n",
    "\n",
    "# Define a function to get the exchange rate value\n",
    "def get_exchange_rate():\n",
    "    # Declare the variable as global to modify its value in the function\n",
    "    global cached_exchange_rate\n",
    "    \n",
    "    try:\n",
    "        # Send a network request to get the exchange rate\n",
    "        response = requests.get('https://openexchangerates.org/api/latest.json?app_id=4820391575d04bdd8d07b7e15fb0a463')\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response and calculate the exchange rate\n",
    "        data = response.json()\n",
    "        exchange_rate = data['rates']['EUR'] / data['rates']['USD']\n",
    "        \n",
    "        # Cache the exchange rate value\n",
    "        cached_exchange_rate = exchange_rate\n",
    "        \n",
    "    except (requests.exceptions.RequestException, json.decoder.JSONDecodeError) as e:\n",
    "        # Handle any exceptions that occur during the API request\n",
    "        # If the API request fails, use the cached exchange rate value if it exists\n",
    "        if cached_exchange_rate is not None:\n",
    "            exchange_rate = cached_exchange_rate\n",
    "        else:\n",
    "            # If there is no cached exchange rate value, raise the original exception\n",
    "            raise e\n",
    "    \n",
    "    return exchange_rate           \n",
    "            \n",
    "def convert_salary_currency(df):\n",
    "    \n",
    "    mask = df['og_salary_currency'] == '$us'   \n",
    "    \n",
    "    df.loc[mask,'discrete_salary'] *= get_exchange_rate()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def convert_salary_period(df):\n",
    "    \n",
    "    n_days_per_year = 250\n",
    "    n_days_per_month = 20\n",
    "        \n",
    "    mask = df['og_salary_period'] == 'an'\n",
    "\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary']\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] / 12\n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] / n_days_per_year\n",
    "\n",
    "    mask = df['og_salary_period'] == 'mois'\n",
    "\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary'] * 12\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] \n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] / n_days_per_month\n",
    "\n",
    "    mask = df['og_salary_period'] == 'jour'\n",
    "\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary'] * n_days_per_year\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] * n_days_per_month\n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] \n",
    "\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "####################################\n",
    "#pd.options.display.max_rows = 80\n",
    "pd.options.display.max_colwidth = 20\n",
    "####################################\n",
    "\n",
    "data = get_salary_col(data)\n",
    "data = get_og_salary_period(data)\n",
    "data = get_og_salary_currency(data)\n",
    "data = clean_salary(data)\n",
    "data = get_salary_range_and_mean(data)\n",
    "data = convert_salary_period(data)\n",
    "data = convert_salary_currency(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "55b0f756",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (940884987.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[411], line 77\u001b[0;36m\u001b[0m\n\u001b[0;31m    for\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 5000\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "data.salary = data.salary.astype(str)\n",
    "\n",
    "data['salary'] = data.salary.replace(r\"\\\\xa0k\\\\xa0\", 'k', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"\\\\u...f...\\\\xa0\", 'k', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"\\\\u...f...,\", ',', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"\\\\xa0\", 'k', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"]\", '', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"'\", '', regex=True)\n",
    "\n",
    "#data['salary'] = data.salary.replace(r\"[,0-9]k\", \"00\")\n",
    "#data['salary'] = data.salary.replace(\"k€\", \"000\")\n",
    "import requests\n",
    "\n",
    "\n",
    "# Function to get exchange rate from USD to EUR\n",
    "def get_exchange_rate():\n",
    "    response = requests.get('https://openexchangerates.org/api/latest.json?app_id=4820391575d04bdd8d07b7e15fb0a463')\n",
    "    data = response.json()\n",
    "    return data['rates']['EUR'] / data['rates']['USD']\n",
    "\n",
    "# Create DataFrame with columns for lower and upper bounds and currency\n",
    "df = pd.DataFrame(columns=['lower_bound', 'upper_bound', 'currency'])\n",
    "df['lower_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['upper_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['currency'] = pd.Series([np.nan] * len(df))\n",
    "\n",
    "# Loop through series and extract salary information\n",
    "for row in series:\n",
    "    # Extract lower and upper bounds and currency\n",
    "    match = re.search(r'(\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) à (\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) (?:par mois|par an|par jour)?', row)\n",
    "    if match:\n",
    "        lower_bound = float(match.group(1).replace(',', '.')) * 1000 if match.group(2) == 'k' else float(match.group(1))\n",
    "        upper_bound = float(match.group(3).replace(',', '.')) * 1000 if match.group(4) == 'k' else float(match.group(3))\n",
    "        if match.group(2) == '$US':\n",
    "            exchange_rate = get_exchange_rate()\n",
    "            lower_bound *= exchange_rate\n",
    "            upper_bound *= exchange_rate\n",
    "        if 'par mois' in row:\n",
    "            lower_bound *= 12\n",
    "            upper_bound *= 12\n",
    "        currency = 'EUR' if match.group(2) in ['k', '€'] else 'USD'\n",
    "        # Assign values to DataFrame\n",
    "        df.loc[df.index.max() + 1] = [lower_bound, upper_bound, currency]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for indx in data.salary:\n",
    "    \n",
    "    # store salary_min and salary_max in column for further analysis\n",
    "    data.loc[i, 'lower_upper_salary_bound'].append((salary_min, salary_max))\n",
    "    # store mean salary in column for further analysis\n",
    "    data.loc[i, 'mean_salary_per_year'] = average  \n",
    "        \n",
    "        \n",
    "    # Remove any non-numeric characters\n",
    "    salary_str = re.sub('[^0-9]+', '', salary_str)\n",
    "    \n",
    "    # Split the salary range\n",
    "    salary_range = salary_str.split('à')\n",
    "    \n",
    "    # Convert the salaries to a common unit\n",
    "    # For example, if salaries are in euros per year:\n",
    "    salary_range = [int(s) for s in salary_range]\n",
    "    \n",
    "    # Calculate the average salary\n",
    "    avg_salary = sum(salary_range) / 2\n",
    "    \n",
    "    return avg_salary\n",
    "\n",
    "\n",
    "for \n",
    "\n",
    "\n",
    "salary_ranges = [\"35k€ to 45k€ per year\", \"40k€ to 50k€ per year\", \"\", \"50k€ to 60k€ per year\"]\n",
    "valid_salaries = []\n",
    "mean_valid_salaries = []\n",
    "\n",
    "for i, string in enumerate(data.salary)\n",
    "        if salary_range:\n",
    "            salary_range = salary_range.replace(\"k€\", \"000\")\n",
    "            \n",
    "            salary_range = salary_range.split('per year')[0]\n",
    "            \n",
    "            salary_min, salary_max = map(int, salary_range.split(\" to \"))\n",
    "            \n",
    "            valid_salaries.append((salary_min, salary_max))\n",
    "            \n",
    "            average = sum([salary_min, salary_max])/ len([salary_min, salary_max])\n",
    "            \n",
    "            data.loc[i, 'mean_salary_per_year'] = average\n",
    "\n",
    "data.mean_salary_per_year.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the exchange rate\n",
    "usd_to_eur = 0.82\n",
    "usd_to_eur_1000 = 900\n",
    "\n",
    "# Define a function to convert salary to euros\n",
    "def convert_to_eur(salary_str):\n",
    "    if \"k$US\" in salary_str:\n",
    "        salary = int(float(salary_str.replace(\"k$US\", \"\").replace(\",\", \"\")) * 1000 / usd_to_eur_1000)\n",
    "    elif \"$US\" in salary_str:\n",
    "        salary = int(float(salary_str.replace(\"$US\", \"\").replace(\",\", \"\")) / usd_to_eur)\n",
    "    elif \"k€\" in salary_str:\n",
    "        salary = int(float(salary_str.replace(\"k€\", \"\").replace(\",\", \"\")) * 1000)\n",
    "    else:\n",
    "        salary = int(float(salary_str.replace(\"€\", \"\").replace(\",\", \"\")))\n",
    "    return salary\n",
    "\n",
    "# Define a function to parse the salary string\n",
    "def parse_salary(salary_str):\n",
    "    if \"par jour\" in salary_str:\n",
    "        match = re.search(r\"(\\d+)k€ à (\\d+)k€ par jour\", salary_str)\n",
    "        if match:\n",
    "            min_salary = int(match.group(1)) * 1000\n",
    "            max_salary = int(match.group(2)) * 1000\n",
    "    elif \"par mois\" in salary_str:\n",
    "        match = re.search(r\"(\\d+(?:,\\d+)?)k€ à (\\d+(?:,\\d+)?)k€ par mois\", salary_str)\n",
    "        if match:\n",
    "            min_salary = int(float(match.group(1).replace(\",\", \"\")) * 1000)\n",
    "            max_salary = int(float(match.group(2).replace(\",\", \"\")) * 1000)\n",
    "    elif \"par an\" in salary_str:\n",
    "        match = re.search(r\"(\\d+(?:,\\d+)?)k€ à (\\d+(?:,\\d+)?)k€ par an\", salary_str)\n",
    "        if match:\n",
    "            min_salary = int(float(match.group(1).replace(\",\", \"\")) * 1000)\n",
    "            max_salary = int(float(match.group(2).replace(\",\", \"\")) * 1000)\n",
    "        else:\n",
    "            match = re.search(r\"(\\d+(?:,\\d+)?)k€ par an\", salary_str)\n",
    "            if match:\n",
    "                min_salary = int(float(match.group(1).replace(\",\", \"\")) * 1000)\n",
    "                max_salary = min_salary\n",
    "            else:\n",
    "                match = re.search(r\"(\\d+(?:,\\d+)?)\n",
    "\n",
    "#cezf \n",
    "                                  \n",
    "                                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define conversion factors for different salary units\n",
    "CONVERSION_FACTORS = {\"par an\": 1, \"par mois\": 12, \"par jour\": 250}\n",
    "\n",
    "# Define a function to process the salary string\n",
    "def process_salary(salary_str):\n",
    "    # Check if the salary is missing or not applicable\n",
    "    if (salary_str == \"NC\") or (salary_str == \"None\"):\n",
    "        return pd.Series({\"average_salary\": None, \"salary_unit\": None})\n",
    "    \n",
    "    # Extract the salary range and units\n",
    "    salary_str = salary_str.strip()\n",
    "    salary_range, salary_unit = salary_str.split(\" par \")\n",
    "    \n",
    "    # Extract the minimum and maximum salaries\n",
    "    if \"à\" in salary_range:\n",
    "        salary_min, salary_max = salary_range.split(\" à \")\n",
    "    else:\n",
    "        salary_min = salary_max = salary_range\n",
    "    \n",
    "    # Clean up the salary values\n",
    "    salary_min = salary_min.replace(\",\", \".\").replace(\"k€\", \"000\").replace(\"k$\", \"000\")\n",
    "    salary_max = salary_max.replace(\",\", \".\").replace(\"k€\", \"000\").replace(\"k$\", \"000\")\n",
    "    \n",
    "    # Convert the salary values to numbers\n",
    "    salary_min = float(salary_min)\n",
    "    salary_max = float(salary_max)\n",
    "    \n",
    "    # Calculate the average salary\n",
    "    average_salary = (salary_min + salary_max) / 2\n",
    "    \n",
    "    # Convert the salary to a consistent unit\n",
    "    if salary_unit in CONVERSION_FACTORS:\n",
    "        conversion_factor = CONVERSION_FACTORS[salary_unit]\n",
    "        average_salary *= conversion_factor\n",
    "    \n",
    "    return pd.Series({\"average_salary\": average_salary, \"salary_unit\": salary_unit})\n",
    "\n",
    "# Define the series of salaries\n",
    "salaries = pd.Series({\n",
    "    7: \"40k€ à 48k€ par an\",\n",
    "    13: \"48k€ à 65k€ par an\",\n",
    "    51: \"20k€ par an\",\n",
    "    127: \"40k€ à 45k€ par an\",\n",
    "    128: \"1,8k€ à 2,5k€ par mois\",\n",
    "    130: \"2k€ par mois\",\n",
    "    151: \"50k€ à 65k€ par an\",\n",
    "    157: \"50k€ à 55k€ par an\",\n",
    "    163: \"40k€ à 65k€ par an\",\n",
    "    267: \"42k$US à 60k$US par an\",\n",
    "    283: \"125k€ à 150k€ par an\",\n",
    "    290: \"50k€ à 60k€ par an\",\n",
    "    292: \"50k€ à 60k€ par an\",\n",
    "    302: \"65k€ à 85k€ par an\",\n",
    "    399: \"45k€ à 55k€ par an\",\n",
    "    400: \"34k€ à 50k€ par an\",\n",
    "    428: \"34k€ à 50k€ par an\",\n",
    "    435: \"2,10k€ à 2,16k€ par mois\",\n",
    "    470:\n",
    "\n",
    "    \n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get exchange rate from USD to EUR\n",
    "def get_exchange_rate():\n",
    "    response = requests.get('https://openexchangerates.org/api/latest.json?app_id=<your_app_id>')\n",
    "    data = response.json()\n",
    "    return data['rates']['EUR'] / data['rates']['USD']\n",
    "\n",
    "# Create DataFrame with columns for lower and upper bounds and currency\n",
    "df = pd.DataFrame(columns=['lower_bound', 'upper_bound', 'currency'])\n",
    "df['lower_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['upper_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['currency'] = pd.Series([np.nan] * len(df))\n",
    "\n",
    "# Loop through series and extract salary information\n",
    "for row in series:\n",
    "    # Extract lower and upper bounds and currency\n",
    "    match = re.search(r'(\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) à (\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) (?:par mois|par an)?', row)\n",
    "    if match:\n",
    "        lower_bound = float(match.group(1).replace(',', '.')) * 1000 if match.group(2) == 'k' else float(match.group(1))\n",
    "        upper_bound = float(match.group(3).replace(',', '.')) * 1000 if match.group(4) == 'k' else float(match.group(3))\n",
    "        if match.group(2) == '$US':\n",
    "            exchange_rate = get_exchange_rate()\n",
    "            lower_bound *= exchange_rate\n",
    "            upper_bound *= exchange_rate\n",
    "        if 'par mois' in row:\n",
    "            lower_bound *= 12\n",
    "            upper_bound *= 12\n",
    "        currency = 'EUR' if match.group(2) in ['k', '€'] else 'USD'\n",
    "        # Assign values to DataFrame\n",
    "        df.loc[df.index.max() + 1] = [lower_bound, upper_bound, currency]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "71a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_col(data):\n",
    "    \n",
    "    data.drop(['extensions'], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "d96642d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[395], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fds \u001b[38;5;241m=\u001b[39m \u001b[43mfz\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fz' is not defined"
     ]
    }
   ],
   "source": [
    "fds = fz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a0d2",
   "metadata": {},
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep original and work on data;description copy\n",
    "data.loc[:, 'description_prepro'] = data.description.copy()\n",
    "\n",
    "# case conversion\n",
    "data.description_prepro = data.description_prepro.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717cb90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "# get common stopwords from nltk library in both french and english\n",
    "stop_words = list(stopwords.words('french')) + list(stopwords.words('english'))\n",
    "\n",
    "punct_mark = [\"•\"]\n",
    "\n",
    "apostrophes_stop_words = [\"d'\", \"c'\", \"j'\", \"m'\", \"n'\", \"s'\", \"t'\", \"qu'\", \n",
    "                          \"jusqu'\", \"lorsqu'\", \"puisqu'\", \"quoiqu'\", \"qu'il\", \n",
    "                          \"qu'on\", \"qu'un\", \"qu'une\", \"sans qu'\", \"étant qu'\",\n",
    "                         \"qu’\", \"jusqu’\", \"lorsqu’\", \"puisqu’\", \"quoiqu’\", \n",
    "                          \"qu’il\", \"qu’on\", \"qu’un\", \"qu’une\", \"sans qu’\", \"étant qu’\",\n",
    "                         \"d’\", \"c’\", \"j’\", \"m’\", \"n’\", \"s’\", \"t’\", \"d’un\", \"d’une\", \"c’est\"]\n",
    "additional_fr_stop_words = [\n",
    "    \"au\", \"aux\", \"avec\", \"ce\", \"ces\", \"dans\", \"de\", \"des\", \"du\", \"elle\",\n",
    "    \"en\", \"et\", \"eux\", \"il\", \"je\", \"la\", \"le\", \"leur\", \"lui\", \"ma\",\n",
    "    \"mais\", \"me\", \"même\", \"mes\", \"moi\", \"mon\", \"ne\", \"nos\", \"notre\",\n",
    "    \"nous\", \"on\", \"ou\", \"par\", \"pas\", \"pour\", \"qu\", \"que\", \"qui\", \"sa\",\n",
    "    \"se\", \"ses\", \"son\", \"sur\", \"ta\", \"te\", \"tes\", \"toi\", \"ton\", \"tu\",\n",
    "    \"un\", \"une\", \"vos\", \"votre\", \"vous\", \"c’\", \"d’\", \"j’\", \"l’\", \"à\", \"m’\",\n",
    "    \"n’\", \"s’\", \"t’\", \"y’\", \"été\", \"étée\", \"étées\", \"étés\", \"étant\", \"suis\",\n",
    "    \"es\", \"est\", \"sommes\", \"êtes\", \"sont\", \"serai\", \"seras\", \"sera\",\n",
    "    \"serons\", \"serez\", \"seront\", \"serais\", \"serait\", \"serions\", \"seriez\",\n",
    "    \"seraient\", \"étais\", \"était\", \"étions\", \"étiez\", \"étaient\", \"fus\",\n",
    "    \"fut\", \"fûmes\", \"fûtes\", \"furent\", \"sois\", \"soit\", \"soyons\", \"soyez\",\n",
    "    \"soient\", \"fusse\", \"fusses\", \"fût\", \"fussions\", \"fussiez\", \"fussent\",\n",
    "    \"ayant\", \"eu\", \"eue\", \"eues\", \"eus\", \"ai\", \"as\", \"avons\", \"avez\",\n",
    "    \"ont\", \"aurai\", \"auras\", \"aura\", \"aurons\", \"aurez\", \"auront\", \"aurais\",\n",
    "    \"aurait\", \"aurions\", \"auriez\", \"auraient\", \"avais\", \"avait\", \"avions\",\n",
    "    \"aviez\", \"avaient\", \"eut\", \"eûmes\", \"eûtes\", \"eurent\", \"aie\", \"aies\",\n",
    "    \"ait\", \"ayons\"]\n",
    "\n",
    "all_stop_words = set(stop_words + apostrophes_stop_words + additional_fr_stop_words + punct_mark)\n",
    "all_stop_words = list(all_stop_words)\n",
    "\n",
    "# remove stop words in data.description_prepro\n",
    "for text in enumerate(data.description_prepro):\n",
    "    \n",
    "    filtered_string = filter(lambda word: word not in all_stop_words, text[1].split()) \n",
    "    filtered_string = \" \".join(list(filtered_string))\n",
    "    data.loc[text[0], 'description_prepro'] = filtered_string\n",
    "    \n",
    "# Punctuation removal\n",
    "\n",
    "def punctuation_translator(text):\n",
    "\n",
    "    # Make a translation table to remove punctuation characters (third argument (ASCII characters mapped to none)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Use the translate method to remove punctuation characters\n",
    "    no_punctuation_text = text.translate(translator)\n",
    "    \n",
    "    return no_punctuation_text\n",
    "    \n",
    "data.description_prepro = data.description_prepro.apply(punctuation_translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description_prepro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5836e",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfc291",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4f603",
   "metadata": {},
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "trigrams_df = pd.DataFrame(columns=[\"word1\", \"word2\", 'word3'])\n",
    "\n",
    "for text in data.description_prepro:\n",
    "    \n",
    "    # split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # create the bigrams\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "\n",
    "    # convert the bigrams to a dataframe\n",
    "    new_trigrams_df = pd.DataFrame(trigrams, columns=[\"word1\", \"word2\", 'word3'])\n",
    "    trigrams_df = pd.concat([trigrams_df, new_trigrams_df])\n",
    "    \n",
    "most_common_trigrams = trigrams_df[\"word1\"] + \" \" + trigrams_df[\"word2\"]+ \" \" + trigrams_df[\"word3\"]\n",
    "most_common_trigrams= most_common_trigrams.value_counts().head(1000)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "print(\"The 5 most common trigrams are:\\n\")\n",
    "print(most_common_trigrams)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727c467",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d37b5",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "bigram_df = pd.DataFrame(columns=[\"word1\", \"word2\"])\n",
    "\n",
    "for text in data.description_prepro:\n",
    "    \n",
    "    # split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # create the bigrams\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "\n",
    "    # convert the bigrams to a dataframe\n",
    "    new_bigrams_df = pd.DataFrame(bigrams, columns=[\"word1\", \"word2\"])\n",
    "    bigram_df = pd.concat([bigram_df, new_bigrams_df])\n",
    "    \n",
    "most_common_bigrams = bigram_df[\"word1\"] + \" \" + bigram_df[\"word2\"]\n",
    "most_common_bigrams = most_common_bigrams.value_counts().head(1000)\n",
    "    \n",
    "print(\"The 5 most common bigrams are:\\n\")\n",
    "print(most_common_bigrams)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b2ff9",
   "metadata": {},
   "source": [
    "## Get most common words w/ CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0d292",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Initialize the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the fit_transform method to extract features from the text data\n",
    "features = vectorizer.fit_transform(data.description_prepro)\n",
    "\n",
    "# The result is a sparse matrix, which can be easily converted to a dense array using .toarray()\n",
    "features_array = features.toarray()\n",
    "\n",
    "# get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# put together term-doc matrix and feature names in pd.DataFrame\n",
    "word_description_occ = pd.DataFrame(data=features_array, columns=feature_names)\n",
    "\n",
    "# To get the most common words, we can sum up the values in each row\n",
    "word_counts = word_description_occ.sum(axis=0)\n",
    "\n",
    "# And then sort the word_counts in descending order\n",
    "most_common_words = word_counts.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "print(\"Most common words: \\n\")\n",
    "most_common_words.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905215b1",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62715032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## replace keywords for easier keyword extraction\n",
    "\n",
    "data.description_prepro.replace(['(?i)(Google Tag Manager)|\\b(GTM)\\b', \"(?i)\\b(GA4)\\b|\\b(GA)\\b\", '(?i)\\b(Google Colab)\\b', '\\b(GCP)\\b|(google cloud plateform)\\b'], value=['Google Tag Manager', 'Google Analytics', 'Google Colaboratory', 'Google Cloud Plateform'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)\\b(AWS)\\b|Amazon Web Services'], value=['Amazon Web Services'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)SKlearn|Scikit'], value=['ScikitLearn'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)data viz'], value=['data visualisation'] ,regex=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "'sas', 'Spark', 'BigML', 'D3.js', 'MATLAB', 'Excel', 'ggplot2', 'Tableau', 'Jupyter', \n",
    "'Matplotlib', 'NLTK', 'TensorFlow', 'Weka', 'Google Analytics', 'KNIME', \n",
    "'Flink', 'MongoDB', 'Minitab', 'Rapidminer', 'DataRobot', 'NLTK', 'Hadoop', 'Power BI', \n",
    "'QlikView', 'MySQL', 'Neo4j', 'HANA', 'Spotfire', 'SPSS', 'STATA', 'RiverLogic', \n",
    "'Lumira', 'Pig', 'Keras', 'NumPy', 'PyTorch', 'Seaborn', 'Wolfram Mathematica', \n",
    "'WebSockets', 'Algorithms.io', 'ForecastThis', 'BigQuery', 'GitHub', \n",
    "'Pycharm', 'Visual Studio Code', 'Linux', 'Windows', 'macOS', 'Google Colaboratory', \n",
    "'Google Cloud Plateform', 'Watson Studio', 'Amazon Web Services', \n",
    "'EC2', 'Amazon Elastic Compute Cloud', 'Microsoft Azure', \n",
    "'Nvidia Jetson Nano', 'Arduino', 'Beam', 'Semantria', 'Trackur', 'Cassandra', 'OctoParse', \n",
    "'Content Grabber', 'OpenRefine', 'Google Fusion Table', 'scipy', 'pandas', 'NPM', 'Redshift', \n",
    "'Snowflake', 'Alteryx', 'Domino Data Lab', 'Kafka', 'Hbase', 'Elasticsearch', 'Maven', \n",
    "'Ansible', 'Gitlab', 'Jenkins', 'Bash', 'IntelliJ', 'MySQL', 'PostreSQL', 'Sonar', \n",
    "'Jira', 'OpenCV', 'TimescaleDB', 'Grafana', 'Google Sheet', 'Pig', 'Talend', 'MSBI',\n",
    "'SAP BO', 'Abode Campaign', 'Google Data Studio', 'Dataform', 'Looker',\n",
    "'Mode', 'Metabase', 'Power Query', 'Power Pivot', 'DataIku', 'MLFlow', 'DVC', 'Kibana', 'SageMaker',\n",
    "'Minio', 'S3', 'MQTT'\n",
    "]\n",
    "\n",
    "keywords_programming = [\n",
    "'sql', 'python', 'r', 'c', 'c#', 'javascript', 'js',  'java', 'scala', 'sas', 'matlab', \n",
    "'c++', 'c/c++', 'perl', 'go', 'typescript', 'bash', 'html', 'css', 'php', 'powershell', 'rust', \n",
    "'kotlin', 'ruby',  'dart', 'assembly', 'swift', 'vba', 'lua', 'groovy', 'delphi', 'objective-c', \n",
    "'haskell', 'elixir', 'julia', 'clojure', 'solidity', 'lisp', 'f#', 'fortran', 'erlang', 'apl', \n",
    "'cobol', 'ocaml', 'crystal', 'javascript/typescript', 'golang', 'nosql', 'mongodb', 't-sql', 'no-sql',\n",
    "'visual_basic', 'pascal', 'mongo', 'pl/sql',  'sass', 'vb.net', 'mssql', \n",
    "]\n",
    "\n",
    "keywords_libraries = [\n",
    "'scikit-learn', 'jupyter', 'theano', 'openCV', 'spark', 'nltk', 'mlpack', 'chainer', 'fann', 'shogun', \n",
    "'dlib', 'mxnet', 'node.js', 'vue', 'vue.js', 'keras', 'ember.js', 'jse/jee',\n",
    "]\n",
    "\n",
    "keywords_analyst_tools = [\n",
    "'excel', 'tableau',  'word', 'powerpoint', 'looker', 'powerbi', 'outlook', 'azure', 'jira', 'twilio',  'snowflake', \n",
    "'shell', 'linux', 'sas', 'sharepoint', 'mysql', 'visio', 'git', 'mssql', 'powerpoints', 'postgresql', 'spreadsheets',\n",
    "'seaborn', 'pandas', 'gdpr', 'spreadsheet', 'alteryx', 'github', 'postgres', 'ssis', 'numpy', 'power_bi', 'spss', 'ssrs', \n",
    "'microstrategy',  'cognos', 'dax', 'matplotlib', 'dplyr', 'tidyr', 'ggplot2', 'plotly', 'esquisse', 'rshiny', 'mlr',\n",
    "'docker', 'linux', 'jira',  'hadoop', 'airflow', 'redis', 'graphql', 'sap', 'tensorflow', 'node', 'asp.net', 'unix',\n",
    "'jquery', 'pyspark', 'pytorch', 'gitlab', 'selenium', 'splunk', 'bitbucket', 'qlik', 'terminal', 'atlassian', 'unix/linux',\n",
    "'linux/unix', 'ubuntu', 'nuix', 'datarobot',\n",
    "]\n",
    "\n",
    "keywords_cloud_tools = [\n",
    "'aws', 'azure', 'gcp', 'snowflake', 'redshift', 'bigquery', 'aurora',\n",
    "]\n",
    "\n",
    "keywords_general_tools = [\n",
    "'microsoft', 'slack', 'apache', 'ibm', 'html5', 'datadog', 'bloomberg',  'ajax', 'persicope', 'oracle', \n",
    "]\n",
    "\n",
    "keywords_general = [\n",
    "'coding', 'server', 'database', 'cloud', 'warehousing', 'scrum', 'devops', 'programming', 'saas', 'ci/cd', 'cicd', \n",
    "'ml', 'data_lake', 'frontend','front-end', 'back-end', 'backend', 'json', 'xml', 'ios', 'kanban', 'nlp',\n",
    "'iot', 'codebase', 'agile/scrum', 'agile', 'ai/ml', 'ai', 'paas', 'machine_learning', 'macros', 'iaas',\n",
    "'fullstack', 'dataops', 'scrum/agile', 'ssas', 'mlops', 'debug', 'etl', 'a/b', 'slack', 'erp', 'oop', \n",
    "'object-oriented', 'etl/elt', 'elt', 'dashboarding', 'big-data', 'twilio', 'ui/ux', 'ux/ui', 'vlookup', \n",
    "'crossover',  'data_lake', 'data_lakes', 'bi', 'pack office'\n",
    "]\n",
    "\n",
    "ml_tools = [\n",
    "'Tensorflow', 'Keras', 'PyTorch', 'ScikitLearn', 'sklearn', 'scikit'\n",
    "]\n",
    "\n",
    "big_data_tools = [\n",
    "'Hadoop', 'HDFS', 'YARN', 'Hive', 'Map', 'Reduce', 'Tez', 'Spark'\n",
    "]\n",
    "\n",
    "skills = [\n",
    "'informatique décisionnelle', 'Extraction', 'Nettoyage', 'transformation', 'ingestion', \n",
    "'data visualisation', 'modélisation', 'reporting', 'veille technologique', 'data mining',\n",
    "'kpi', 'computer vision'\n",
    "]\n",
    "\n",
    "soft_skills = [\n",
    "\"Esprit d'analyse\", \"Sens du service\", \"Rigueur\", \"communication\", 'positif', 'créatif', \n",
    "'pragmatique', 'souple', 'agile', \"Autonome\", 'Polyvalent', 'travailler en équipe', \"esprit d'équipe\"\n",
    "'esprit de synthèse', 'aisance relationnelle', 'force de proposition', \"capacité d'analyse\",\n",
    "\"anglais\", \"espagnol\", \"francais\"\n",
    "]\n",
    "\n",
    "coding_languages = [\n",
    "'SQL', 'Python', 'R', 'Julia', 'Scala', 'C++', 'Java', 'Javascript', 'Go'\n",
    "]\n",
    "\n",
    "# Add up all keyword lists\n",
    "all_kw = tools + keywords_programming + keywords_libraries + keywords_analyst_tools + keywords_cloud_tools + keywords_general_tools + keywords_general + ml_tools + big_data_tools + skills + soft_skills + coding_languages\n",
    "\n",
    "# Lower case & remove duplicates\n",
    "all_kw = list(set([word.lower() for word in all_kw]))\n",
    "\n",
    "# keyword extraction\n",
    "data['kw_extraction'] = data['description_prepro'].apply(lambda x: [])\n",
    "\n",
    "for text in enumerate(data.description_prepro):\n",
    "\n",
    "    kw_list = [word for word in all_kw if word in text[1]]\n",
    "    data.loc[text[0], \"kw_extraction\"].extend(kw_list)\n",
    "\n",
    "# number of keywords extracted\n",
    "data['len_list'] = data['kw_extraction'].map(lambda x: len(x))\n",
    "data['len_list'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893224",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d8ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining constants\n",
    "keywords = ['durée', 'stage', 'contrat', 'localisation|lieu|location', 'statut', 'salaire', 'formation', 'profil', \n",
    "            'mission', 'avantage', 'experience', 'soft', 'langue']\n",
    "n_chars_after = 20\n",
    "m_chars_before = 60\n",
    "#col_name = f'charsss_around_{kw}'\n",
    "\n",
    "# loop over keywords list\n",
    "for kw in keywords:\n",
    "    \n",
    "    # define our new columns\n",
    "    data[f'chars_around_{kw}'] = 'NC '\n",
    "\n",
    "    # loop over each string in data.description\n",
    "    for i, text in zip(data.index, data['description_prepro']):\n",
    "        \n",
    "        # get iterator over all keywords matches in each string\n",
    "        search_obj = re.finditer(kw, text, re.I)\n",
    "\n",
    "        if search_obj: # ... is true, then ...\n",
    "\n",
    "            # get each keyword match in string\n",
    "            for match in search_obj:\n",
    "\n",
    "                # Find the start index of the keyword\n",
    "                start = match.span()[0]\n",
    "\n",
    "                # Find the end index of the keyword\n",
    "                end = match.span()[1]\n",
    "\n",
    "                # Truncate line to get only 'n' characters before and after the keyword\n",
    "                line = text[start-m_chars_before:end+n_chars_after]\n",
    "                \n",
    "                # add up ever line containing keyword match (if several) in corresponding cells\n",
    "                data.loc[i, f'chars_around_{kw}'] += line\n",
    "                \n",
    "                # get rid of by default 'NC' string\n",
    "                data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}'].replace('NC ', '')\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # if no keyword match then keep by default value\n",
    "            data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1c642",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a2339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc98e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
