{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef84455",
   "metadata": {},
   "source": [
    "# Scraping bullshit jobs ... a long way to hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9487d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# library for plotting data over maps\n",
    "import shapefile as shp\n",
    "\n",
    "# libraries used for tokenization\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
    "\n",
    "# Libraries used to remove similar job description based on cosine similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# libraries used for text normalization\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# use to get USdollar / euro exchange rate from exchange rate API\n",
    "import requests\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "# import my functions\n",
    "from preprocessing import preprocess as pp\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd10655",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ced546c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/axel/ds_projects/projects/gg_job_search/data/gg_job_search_all_RAW.csv')\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e1272",
   "metadata": {},
   "source": [
    "## General overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bcbba87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19423 entries, 0 to 19422\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      19423 non-null  int64  \n",
      " 1   title           19423 non-null  object \n",
      " 2   company_name    19423 non-null  object \n",
      " 3   location        19412 non-null  object \n",
      " 4   via             19423 non-null  object \n",
      " 5   description     19363 non-null  object \n",
      " 6   job_highlights  13090 non-null  object \n",
      " 7   related_links   19363 non-null  object \n",
      " 8   thumbnail       17657 non-null  object \n",
      " 9   extensions      19423 non-null  object \n",
      " 10  job_id          16609 non-null  object \n",
      " 11  posted_at       17430 non-null  object \n",
      " 12  schedule_type   19256 non-null  object \n",
      " 13  date_time       19423 non-null  object \n",
      " 14  search_query    19423 non-null  object \n",
      " 15  Unnamed: 0.1    6333 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 2.4+ MB\n",
      "None \n",
      "\n",
      "Unnamed: 0        6333\n",
      "title             1532\n",
      "company_name      1275\n",
      "location           354\n",
      "via                139\n",
      "description       2560\n",
      "job_highlights     627\n",
      "related_links     3932\n",
      "thumbnail          862\n",
      "extensions        1229\n",
      "job_id            2939\n",
      "posted_at           54\n",
      "schedule_type        4\n",
      "date_time         1704\n",
      "search_query         3\n",
      "Unnamed: 0.1        10\n",
      "dtype: int64 \n",
      "\n",
      "data engineer     6803\n",
      "data scientist    6446\n",
      "data analyst      6174\n",
      "Name: search_query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.info(), '\\n')\n",
    "print(data.nunique(), '\\n')\n",
    "print(data.search_query.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01234",
   "metadata": {},
   "source": [
    "## Null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7e83723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location             11\n",
       "description          60\n",
       "job_highlights     6333\n",
       "related_links        60\n",
       "thumbnail          1766\n",
       "job_id             2814\n",
       "posted_at          1993\n",
       "schedule_type       167\n",
       "Unnamed: 0.1      13090\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47929c2",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "49b4837f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2023-02-17 11:50:44.355114\n",
       "1        2023-02-17 11:50:44.355114\n",
       "2        2023-02-17 11:50:44.355114\n",
       "3        2023-02-17 11:50:44.355114\n",
       "4        2023-02-17 11:50:44.355114\n",
       "                    ...            \n",
       "19418    2023-02-22 19:58:57.714324\n",
       "19419    2023-02-22 19:58:57.714324\n",
       "19420    2023-02-22 19:58:58.218200\n",
       "19421    2023-02-22 19:58:58.218200\n",
       "19422    2023-02-22 19:58:58.218200\n",
       "Name: date_time, Length: 19423, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "583af771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates : 546\n",
      "Number of duplicates (based on identical job_id) : 16483 \n",
      " ======================================================================\n",
      "Search query results :\n",
      " data scientist    1605\n",
      "data analyst       720\n",
      "data engineer      615\n",
      "Name: search_query, dtype: int64 \n",
      " ======================================================================\n",
      "Final number of rows :  2940\n",
      "Final number of columns :  17\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates :',data[data.duplicated()].shape[0])\n",
    "print('Number of duplicates (based on identical job_id) :', data[data.duplicated(['job_id'])].shape[0],'\\n', '='*70) \n",
    "\n",
    "# removing duplicates based on job_id\n",
    "data = data.drop_duplicates(['job_id'])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e020",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Purely based job id, we can skim off a lots of duplicates. Nonetheless i still found a few of identical / near-identical job descriptions, maybe because [sometimes recruiters or companies post the same advert for a job which results in duplicate data.](https://medium.com/analytics-vidhya/data-science-job-search-using-nlp-and-lda-in-python-12ecbfac79f9)\n",
    "\n",
    "To get rid off these ones, i could use cosine similatity between job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e082a5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates (based on cosine similarity) :  1108\n",
      "Final number of rows :  1832\n",
      "Final number of columns :  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33441/1214297801.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n"
     ]
    }
   ],
   "source": [
    "### removing duplicates based on cosine similarity between \n",
    "## job descriptions (code from Thomas Caffrey, see link above)\n",
    "\n",
    "# Defining our collection of job description texts to tokenize\n",
    "data.description = data['description'].fillna('')\n",
    "corpus = data['description']\n",
    "\n",
    "# instantiate CountVectorizer object\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Fit_transform to vectorize each job description (map terms to feature indices)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarities and put it in dataframe\n",
    "cos_df = pd.DataFrame(cosine_similarity(X_train_counts))\n",
    "\n",
    "\n",
    "## reshape dataframe for easier comparison\n",
    "\n",
    "# get arrays of rows indices and col indices from col_df.shape\n",
    "i, j = np.indices(cos_df.shape).reshape(2,-1)\n",
    "\n",
    "# reshape values to get a 1D array \n",
    "cos_values = cos_df.values.reshape(-1)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'i': i, 'j': j, 'sim':cos_values})\n",
    "\n",
    "# get cosine similarity values only above 0.98 \n",
    "cos_rem = cos_sim_df[(cos_sim_df['sim'] > 0.98) & (i!=j)]\n",
    "\n",
    "# Method to remove duplicates but keep first instance:\n",
    "# Trying to drop duplicates on i and j columns won't work as the row numbers of duplicates are either in i or j not both.\n",
    "# Setting another column that combines the i & j values ensures that duplicates can be dropped.\n",
    "\n",
    "cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n",
    "drop_rows = np.unique(cos_rem.drop_duplicates('i*j')['i'].values)\n",
    "\n",
    "# keep only non-duplicated job postings\n",
    "data = data[~data.index.isin(drop_rows)] \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Number of duplicates (based on cosine similarity) : ' ,drop_rows.shape[0])\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599a6b",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d501f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ['il y a 17 heures', 'À plein temps']\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                               ['il y a 11 heures', '35\\xa0k\\xa0€ à 40\\xa0k\\xa0€ par an', 'À plein temps']\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ['il y a 19 heures', 'À plein temps']\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ['il y a 22 heures', 'À plein temps']\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ['il y a 19 heures', 'À plein temps']\n",
       "                                                                                                                                                                                                                                                               ...                                                                                                                                                                                                                                                         \n",
       "1827    eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAoRi9IKSIsImh0aWRvY2lkIjoiRGRvZUZrR3hEMmNBQUFBQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUdSbkpoYm1ObCIsImdsIjoiZnIiLCJobCI6ImZyIiwiZmMiOiJFc3NCQ293QlFVTlFkMnBxVVU1WVVUZGhSRU5pTkZkTlRUVnlRM0JrV0RFNGVHb3RNVk10YW14RFVWSnljM05FVW1kWGRYcFdkSEoyWkdSWlpESkxkVk5GU25VMWNuQm5kMVEzTW05cldEbFBha0pXWkd0V01YQlljemc0T0RobmFWWndjRlpqUnpCSVZuRTRhek5TUm1kdGVqSlFja3MzTjNob1pXNU1VMDlLZEMxYVprVlhPRkpyZGw5TVQwaGxVVWdTRm01WFdESlpOMFJYUXpkWGFIQjBVVkJ3WmkxWFUwRWFJa0ZHWHpaTlFVMW9XVXMz...\n",
       "1828    eyJqb2JfdGl0bGUiOiJEQVRBIFNDSUVOVElTVCBGL0ggLSBBaXgtZW4tUHJvdmVuY2UiLCJodGlkb2NpZCI6Iml5WTkwNXU3dHVjQUFBQUFBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lHUm5KaGJtTmwiLCJnbCI6ImZyIiwiaGwiOiJmciIsImFwcGx5X2xpbmsiOnsidGl0bGUiOiJQb3N0dWxlciBzdXIgSm9icyBUaGF0IE1ha2UgU2Vuc2UiLCJsaW5rIjoiaHR0cHM6Ly9qb2JzLm1ha2VzZW5zZS5vcmcvZW4vam9icy9vbWJyZWEtZGF0YS1zY2llbnRpc3QtZmgtNlp0b0F3b24wejNCbVZSTDhlT28/dXRtX2NhbXBhaWduPWdvb2dsZV9qb2JzX2FwcGx5XHUwMDI2dXRtX3NvdXJjZT1nb29nbGVfam9ic19hcHBseVx1MDAyNnV0bV9tZWRpdW09b3JnYW5p...\n",
       "1829    eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBlbiBEb25uw6llcyBBZ3Jvbm9taXF1ZXMgKEYvSCkgLSBGcmFuY2Ugb3UgSG9uZ3JpZSIsImh0aWRvY2lkIjoiQVhoZlF2dDNRbGtBQUFBQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUdSbkpoYm1ObCIsImdsIjoiZnIiLCJobCI6ImZyIiwiZmMiOiJFc3dCQ293QlFVTlFkMnBxVkZKeUxYUk9SVzVKVkRSVWFtdGliRWc1WDB4M1QyOTRkRTVoVTFwaVIweHFielZVU21FMmIzVXlkRlJNWVVkVGJUUklSRVJuUzBaM1pFbFlTa0p0T0ZaMFdYcHNUVTE2T0haVVdVSk9UMHBpYUhWc1NsWmhMWFp1YmpaM1MwZHROR3hzVmxOQldHY3lUakZaYmt3MVJ6Sm9NbkZVVDB4NFUySjBhazUzVjNaSFNtMU5MVk1TRjNO...\n",
       "1830    eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAvLyBGdWxsIHJlbW90ZSAvLyBGbHVlbnQgRW5nbGlzaCAvLyBFY29sb2dpZSBIL0YiLCJodGlkb2NpZCI6IktrNkhhSjdkZGZzQUFBQUFBQUFBQUE9PSIsInV1bGUiOiJ3K0NBSVFJQ0lHUm5KaGJtTmwiLCJnbCI6ImZyIiwiaGwiOiJmciIsImZjIjoiRXVJQkNxSUJRVU5RZDJwcVVYZHZaa1V4WkV4TVJVTm1Va3BFVVdKcWNsSkRSblJ1VXpWaFMzbG9hVEp0YWsxS1NsUTBXRlZ2TUV4WWJEVnBWRVpxTFdwTFdYWjZjbDlHWDNWZlEydGpkVGh6WjBnM1JIbEpXRk51TVhsWlUzQldiR1JRU1dGYU5EUTJNMjlVY1ZjeGREVkxVVzA1Ym05bFNuVlBRMVozVUdoUlNsbDFkbDkxWlVZMlozZElkMGhuWVVjMVJUQmlh...\n",
       "1831    eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCIsImh0aWRvY2lkIjoiRnltRWFGNGRCVzRBQUFBQUFBQUFBQT09IiwidXVsZSI6IncrQ0FJUUlDSUdSbkpoYm1ObCIsImdsIjoiZnIiLCJobCI6ImZyIiwiZmMiOiJFc3dCQ293QlFVTlFkMnBxVkVkUVpIWm1hRTA1V21sRlVXUXRMVzEzYjBWbWFESTBkRmw0TldOaVExcFFNRTg0WWpaR1VHNTNOVWRXVG1ObU0xbG5kM1ZqUTBoVWJGWmFRVEJDWkdWSmVGWXdNRnBYVldKcFIwMUdOVUp0ZDBGTmJsVm5NM1phZEhKUWMxQmtSR1pmWXpacFFVaExSRzl4YVhKdkxVeFBTbXMxVkRGSlNIVkVVbGh4YzNaQmQybGZhSG9TRnpKWFdESlpOVWN4UzJWRE5YRjBjMUIwYzJWbk1FRm5HaUpCUmw4MlRVRk9ORkJrVEVnMmRG...\n",
       "Name: extensions, Length: 1832, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d1851",
   "metadata": {},
   "source": [
    "## Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aaef9dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(data.title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242f91",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Based on this sample of job titles, we could create : \n",
    "    \n",
    "    * Contract_type (full-time, part-time ...) \n",
    "    * Contract_status (CDI, CDD, work-study, internship ...)\n",
    "    * Duration of Contract (Duration/Undetermined)\n",
    "    * Experience ( Senior, Junior ...)\n",
    "    * Data Specialization (Supply chain, Marketing, Clinical ...)\n",
    "    * Multiple titles (Analyst/Scientist, Scientist/ML Engineer, Manager/Analyst ...)\n",
    "    * Specific expertise asked for (Python, Power BI ...)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1679e1",
   "metadata": {},
   "source": [
    "## Explore company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3400f598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique companies : 1105\n"
     ]
    }
   ],
   "source": [
    "print('number of unique companies :', data.company_name.nunique())\n",
    "#data.company_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1583ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 20\n",
    "#data.loc[data['company_name'] == 'Unspecified', ['description']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05598b",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "When ***Unspecified***, companies name can be found in description column.\n",
    "\n",
    "Possible new columns :\n",
    "\n",
    "    * Group/Holding (Y/N/NC)\n",
    "    * Interim company (Y/N/NC)\n",
    "\n",
    "Based on the number of job posting per company we could potentially infer about : **size of company ? / Amount of data to work on** / \n",
    "\n",
    "Adding a time variable and much more data, the number of similar / identical job postings for the same company could maybe give insights on the **company's turnover rate / company's growth / magnitude of need-urgency to hire** ...  \n",
    "\n",
    "It seems like extracting additional informations without more context will be difficult. Having access to each company's structure information we could create :\n",
    "\n",
    "    * Size of company\n",
    "    * Industry\n",
    "    * Public / Private\n",
    "    \n",
    "We'll see if can extract more related informations in the following columns. Otherwise, we could try to scrap **Glassdoor databases** (or similar) to get those informations.remains to be seen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0ce67",
   "metadata": {},
   "source": [
    "## Explore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f01afb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique locations :  256\n"
     ]
    }
   ],
   "source": [
    "print('number of unique locations : ', data.location.nunique())\n",
    "#data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29d39825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "#data.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cabad",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "We could create a map of th repartition of job posting based on location provided.\n",
    "\n",
    "Some companies don't provide precise location *(ex : location = FRANCE)* and the information is not available in description column either. Further investigations will be needed for these companies, perhaps in conjunction with other databases *(GLASSDOOR / SIRENE databases for instance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2d818",
   "metadata": {},
   "source": [
    "## Explore via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "79a48ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique job plateforms :  111\n"
     ]
    }
   ],
   "source": [
    "print('number of unique job plateforms : ', data.via.nunique())\n",
    "# data.via.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b57edd",
   "metadata": {},
   "source": [
    "## Explore description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acda0f5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1832.000000\n",
      "mean      2943.989629\n",
      "std       1643.546334\n",
      "min          0.000000\n",
      "25%       1776.750000\n",
      "50%       2678.500000\n",
      "75%       3857.250000\n",
      "max      14170.000000\n",
      "Name: description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.description.str.len().describe())\n",
    "\n",
    "plt.hist(data.description.str.len(), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf1f6",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "\n",
    "We could create :\n",
    "\n",
    "    * Lenght of description (dunno what informations it could provide yet)\n",
    "    * Toold required (Excel, Google Tag Manager ...)\n",
    "    * Coding languages required (R, Python, SQL ...)\n",
    "    * Skills required (reporting, data visualization)\n",
    "    * Required experience\n",
    "    * Duration of contract\n",
    "    * Avantages (ticket resto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "763cf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_employment_type(row):\n",
    "    employment_type_dict = {\n",
    "        'CDI': ['cdi',],\n",
    "        'internship': ['stage', 'internship', 'stagiaire'],\n",
    "        'apprenticeship': ['alternant', '(apprenti)', 'professionnalisation', 'alternance'],\n",
    "        'CDD': ['cdd'],\n",
    "        'freelance': ['freelance', 'prestataire', 'freelancer'],\n",
    "        'interim': ['interim'],\n",
    "        'consultant': ['consultant'],\n",
    "    }\n",
    "    for employment_type, keywords in employment_type_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return employment_type\n",
    "    return np.NaN\n",
    "\n",
    "def create_seniority_level(row):\n",
    "    seniority_level_dict = {\n",
    "        'senior': ['senior', 'advanced', 'avance', 'sr', 'experimente'],\n",
    "        'mid-level': ['confirme'],\n",
    "        'junior': ['junior', 'debutant', 'jr', 'entre-level'],\n",
    "    }\n",
    "    for seniority_level, keywords in seniority_level_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U): #or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return seniority_level\n",
    "    return np.NaN\n",
    "\n",
    "def create_executive_title(row):\n",
    "    executive_title_dict = {\n",
    "        'lead': ['lead'],\n",
    "        'Director': ['director', 'directeur'],\n",
    "        'Manager': ['manager', 'project manager', 'chef de projet'],\n",
    "        'Assistant': ['assistant'],\n",
    "        'Chief': ['chief'],\n",
    "        'Head': ['head'],\n",
    "        'Supervisor': ['supervisor'],\n",
    "    }\n",
    "    for executive_title, keywords in executive_title_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U):\n",
    "                return executive_title\n",
    "    return np.NaN\n",
    "\n",
    "def create_job_specialization(row):\n",
    "    job_specialization_dict = {\n",
    "        'career': ['career'],\n",
    "        'web': ['web'],\n",
    "        'media': ['media'],\n",
    "        'online': ['online'],\n",
    "        'marketing': ['marketing', 'market'],\n",
    "        'crm': ['crm'],\n",
    "        'assurance': ['indemnisation'],\n",
    "        'immobilier': ['immobilier'],\n",
    "        'product': ['product'],\n",
    "        'people': ['people', 'hr', 'human', 'workforce'],\n",
    "        'informatique': ['informatique'],\n",
    "        'supply_chain': ['supply'],\n",
    "        'logistique': ['logistique'],\n",
    "        'medical': ['medical', 'clinique', 'sante'],\n",
    "        'finance': ['finance'],\n",
    "        'recherche': ['recherche'],\n",
    "        'tv': ['tv'],\n",
    "        'game': ['game'],\n",
    "        'geo': ['geo'],\n",
    "    }\n",
    "    for job_specialization, keywords in job_specialization_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword,unidecode(row['title']), re.I|re.U):\n",
    "                return job_specialization\n",
    "    return np.NaN\n",
    "\n",
    "def create_remote(row):\n",
    "    remote_dict = {\n",
    "        'Y': ['remote', 'teletravail hybride', 'teletravail complet', 'jour de teletravail',\n",
    "                   'jours de teletravail', 'teletravail partiel', 'distanciel', 'teletravail', '(TT)']        \n",
    "    }\n",
    "    for remote, keywords in remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "def create_full_partial_remote(row):\n",
    "    full_partial_remote_dict = {\n",
    "        'full': ['full remote', 'teletravail complet'],\n",
    "        'partial_remote': ['teletravail de','teletravail hybride', 'jours de teletravail', 'teletravail partiel', 'jour de teletravail',]\n",
    "    }\n",
    "    for remote, keywords in full_partial_remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "\n",
    "\n",
    "#data = data.applymap(lambda x: unidecode(x) if isinstance(x, str) else x) # used for accent-insensitive search, got replaced directly in functions (see above)\n",
    "# lower every string of dataframe for easier search\n",
    "data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "data['executive_title'] = data.apply(create_executive_title, axis=1)\n",
    "data['seniority_level'] = data.apply(create_seniority_level, axis=1) \n",
    "data['employment_type'] = data.apply(create_employment_type, axis=1) \n",
    "data['job_specialization'] = data.apply(create_job_specialization, axis=1) \n",
    "data['remote'] = data.apply(create_remote, axis=1) \n",
    "data['full_partial_remote'] = data.apply(create_full_partial_remote, axis=1)\n",
    "data = pp.salary_prepro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_col(data, columns):\n",
    "    \n",
    "    data.drop(['extensions'], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a0d2",
   "metadata": {},
   "source": [
    "## Job description - Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a7675a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common stopwords from nltk library in both french and english\n",
    "stop_words = list(set(stopwords.words('french')).union(set(stopwords.words('english'))))\n",
    "\n",
    "punct_mark = [\"•\"]\n",
    "\n",
    "apostrophes_stop_words = [\"d'\", \"c'\", \"j'\", \"m'\", \"n'\", \"s'\", \"t'\", \"l'\" ,\"qu'\", \n",
    "                          \"jusqu'\", \"lorsqu'\", \"puisqu'\", \"quoiqu'\", \"qu'il\", \n",
    "                          \"qu'on\", \"qu'un\", \"qu'une\", \"sans qu'\", \"étant qu'\",\n",
    "                         \"qu’\", \"jusqu’\", \"lorsqu’\", \"puisqu’\", \"quoiqu’\", \n",
    "                          \"qu’il\", \"qu’on\", \"qu’un\", \"qu’une\", \"sans qu’\", \"étant qu’\",\n",
    "                         \"d’\", \"c’\", \"j’\", \"m’\", \"n’\", \"s’\", \"t’\", \"d’un\", \"d’une\", \"c’est\"]\n",
    "additional_fr_stop_words = [\n",
    "    \"au\", \"aux\", \"avec\", \"ce\", \"ces\", \"dans\", \"de\", \"des\", \"du\", \"elle\",\n",
    "    \"en\", \"et\", \"eux\", \"il\", \"je\", \"la\", \"le\", \"leur\", \"lui\", \"ma\",\n",
    "    \"mais\", \"me\", \"même\", \"mes\", \"moi\", \"mon\", \"ne\", \"nos\", \"notre\",\n",
    "    \"nous\", \"on\", \"ou\", \"par\", \"pas\", \"pour\", \"qu\", \"que\", \"qui\", \"sa\",\n",
    "    \"se\", \"ses\", \"son\", \"sur\", \"ta\", \"te\", \"tes\", \"toi\", \"ton\", \"tu\",\n",
    "    \"un\", \"une\", \"vos\", \"votre\", \"vous\", \"c’\", \"d’\", \"j’\", \"l’\", \"à\", \"m’\",\n",
    "    \"n’\", \"s’\", \"t’\", \"y’\", \"été\", \"étée\", \"étées\", \"étés\", \"étant\", \"suis\",\n",
    "    \"es\", \"est\", \"sommes\", \"êtes\", \"sont\", \"serai\", \"seras\", \"sera\",\n",
    "    \"serons\", \"serez\", \"seront\", \"serais\", \"serait\", \"serions\", \"seriez\",\n",
    "    \"seraient\", \"étais\", \"était\", \"étions\", \"étiez\", \"étaient\", \"fus\",\n",
    "    \"fut\", \"fûmes\", \"fûtes\", \"furent\", \"sois\", \"soit\", \"soyons\", \"soyez\",\n",
    "    \"soient\", \"fusse\", \"fusses\", \"fût\", \"fussions\", \"fussiez\", \"fussent\",\n",
    "    \"ayant\", \"eu\", \"eue\", \"eues\", \"eus\", \"ai\", \"as\", \"avons\", \"avez\",\n",
    "    \"ont\", \"aurai\", \"auras\", \"aura\", \"aurons\", \"aurez\", \"auront\", \"aurais\",\n",
    "    \"aurait\", \"aurions\", \"auriez\", \"auraient\", \"avais\", \"avait\", \"avions\",\n",
    "    \"aviez\", \"avaient\", \"eut\", \"eûmes\", \"eûtes\", \"eurent\", \"aie\", \"aies\",\n",
    "    \"ait\", \"ayons\"]\n",
    "\n",
    "all_stop_words = set(stop_words + apostrophes_stop_words + additional_fr_stop_words + punct_mark)\n",
    "all_stop_words = list(all_stop_words)\n",
    "\n",
    "\n",
    "# Copy the original description column to a new column\n",
    "data['description_normalized'] = data['description'].copy().str.lower()\n",
    "\n",
    "pattern = r\"(\\b\\w+)[`'’](\\w+)?\\b\"\n",
    "replacement = r\"\\2\"\n",
    "\n",
    "data['description_normalized'] = data['description_normalized'].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "\n",
    "# remove stop words in data.description_normalized\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: ' '.join([word for word in text.split() if word not in all_stop_words]))\n",
    "    \n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: text.translate(translator))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5836e",
   "metadata": {},
   "source": [
    "## Job description normalized - N-grams tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2050bc75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most common trigrams are:\n",
      "\n",
      "personnes situation handicap 148\n",
      "learning deep learning 108\n",
      "machine learning deep 107\n",
      "bac 5 publiée 92\n",
      "data scientist hf 87\n",
      "\n",
      "The 5 most common bigrams are:\n",
      "\n",
      "data science 1242\n",
      "machine learning 942\n",
      "data scientist 840\n",
      "data analyst 696\n",
      "big data 574\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "trigrams = []\n",
    "bigrams = []\n",
    "\n",
    "def get_most_common_ngrams(data, n, k):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings, an integer n (for the n-gram size), \n",
    "    and an integer k (for the number of most common n-grams to return). \n",
    "    It returns a list of the k most common n-grams in the input data.\n",
    "    \"\"\"\n",
    "    ngrams_list = []  # Create an empty list to store the n-grams\n",
    "    \n",
    "    for text in data:\n",
    "        \n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate the n-grams and add them to the list\n",
    "        ngrams_list.extend(list(ngrams(words, n)))\n",
    "        \n",
    "    # Count the occurrences of each n-gram using FreqDist\n",
    "    freq_dist = FreqDist(ngrams_list)\n",
    "    \n",
    "    # Get the k most common n-grams and return them\n",
    "    return freq_dist.most_common(k)\n",
    "\n",
    "trigrams = get_most_common_ngrams(data.description_normalized, 3, 1000)\n",
    "bigrams = get_most_common_ngrams(data.description_normalized, 2, 1000)\n",
    "\n",
    "    \n",
    "# Print the most common trigrams and bigrams\n",
    "print(\"The 5 most common trigrams are:\\n\")\n",
    "for trigram, count in trigrams[:5]:\n",
    "    print(' '.join(trigram), count)\n",
    "\n",
    "print(\"\\nThe 5 most common bigrams are:\\n\")\n",
    "for bigram, count in bigrams[:5]:\n",
    "    print(' '.join(bigram), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b2ff9",
   "metadata": {},
   "source": [
    "## Tokenization w/ CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "239fe580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common words: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data       10602\n",
       "données     5345\n",
       "équipe      2655\n",
       "plus        2574\n",
       "clients     2047\n",
       "dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the fit_transform method to extract features from the text data\n",
    "features = vectorizer.fit_transform(data.description_normalized)\n",
    "\n",
    "# The result is a sparse matrix, which can be easily converted to a dense array using .toarray()\n",
    "features_array = features.toarray()\n",
    "\n",
    "# get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# put together term-doc matrix and feature names in pd.DataFrame\n",
    "word_description_occ = pd.DataFrame(data=features_array, columns=feature_names)\n",
    "\n",
    "# To get the most common words, we can sum up the values in each row\n",
    "word_counts = word_description_occ.sum(axis=0)\n",
    "\n",
    "# And then sort the word_counts in descending order\n",
    "most_common_words = word_counts.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "print(\"5 most common words: \\n\")\n",
    "most_common_words.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905215b1",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b54c16",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## replace keywords for easier keyword extraction\n",
    "\n",
    "data.description_prepro.replace(['(?i)(Google Tag Manager)|\\b(GTM)\\b', \"(?i)\\b(GA4)\\b|\\b(GA)\\b\", '(?i)\\b(Google Colab)\\b', '\\b(GCP)\\b|(google cloud plateform)\\b'], value=['Google Tag Manager', 'Google Analytics', 'Google Colaboratory', 'Google Cloud Plateform'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)\\b(AWS)\\b|Amazon Web Services'], value=['Amazon Web Services'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)SKlearn|Scikit'], value=['ScikitLearn'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)data viz'], value=['data visualisation'] ,regex=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d4b649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26737"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [\n",
    "'sas', 'Spark', 'BigML', 'D3.js', 'MATLAB', 'Excel', 'ggplot2', 'Tableau', 'Jupyter', \n",
    "'Matplotlib', 'NLTK', 'TensorFlow', 'Weka', 'Google Analytics', 'KNIME', \n",
    "'Flink', 'MongoDB', 'Minitab', 'Rapidminer', 'DataRobot', 'NLTK', 'Hadoop', 'Power BI', \n",
    "'QlikView', 'MySQL', 'Neo4j', 'HANA', 'Spotfire', 'SPSS', 'STATA', 'RiverLogic', \n",
    "'Lumira', 'Pig', 'Keras', 'NumPy', 'PyTorch', 'Seaborn', 'Wolfram Mathematica', \n",
    "'WebSockets', 'Algorithms.io', 'ForecastThis', 'BigQuery', 'GitHub', \n",
    "'Pycharm', 'Visual Studio Code', 'Linux', 'Windows', 'macOS', 'Google Colaboratory', \n",
    "'Google Cloud Plateform', 'Watson Studio', 'Amazon Web Services', \n",
    "'EC2', 'Amazon Elastic Compute Cloud', 'Microsoft Azure', \n",
    "'Nvidia Jetson Nano', 'Arduino', 'Beam', 'Semantria', 'Trackur', 'Cassandra', 'OctoParse', \n",
    "'Content Grabber', 'OpenRefine', 'Google Fusion Table', 'scipy', 'pandas', 'NPM', 'Redshift', \n",
    "'Snowflake', 'Alteryx', 'Domino Data Lab', 'Kafka', 'Hbase', 'Elasticsearch', 'Maven', \n",
    "'Ansible', 'Gitlab', 'Jenkins', 'Bash', 'IntelliJ', 'MySQL', 'PostreSQL', 'Sonar', \n",
    "'Jira', 'OpenCV', 'TimescaleDB', 'Grafana', 'Google Sheet', 'Pig', 'Talend', 'MSBI',\n",
    "'SAP BO', 'Abode Campaign', 'Google Data Studio', 'Dataform', 'Looker',\n",
    "'Mode', 'Metabase', 'Power Query', 'Power Pivot', 'DataIku', 'MLFlow', 'DVC', 'Kibana', 'SageMaker',\n",
    "'Minio', 'S3', 'MQTT'\n",
    "]\n",
    "\n",
    "keywords_programming = [\n",
    "'sql', 'python', 'r', 'c', 'c#', 'javascript', 'js',  'java', 'scala', 'sas', 'matlab', \n",
    "'c++', 'c/c++', 'perl', 'go', 'typescript', 'bash', 'html', 'css', 'php', 'powershell', 'rust', \n",
    "'kotlin', 'ruby',  'dart', 'assembly', 'swift', 'vba', 'lua', 'groovy', 'delphi', 'objective-c', \n",
    "'haskell', 'elixir', 'julia', 'clojure', 'solidity', 'lisp', 'f#', 'fortran', 'erlang', 'apl', \n",
    "'cobol', 'ocaml', 'crystal', 'javascript/typescript', 'golang', 'nosql', 'mongodb', 't-sql', 'no-sql',\n",
    "'visual_basic', 'pascal', 'mongo', 'pl/sql',  'sass', 'vb.net', 'mssql', \n",
    "]\n",
    "\n",
    "keywords_libraries = [\n",
    "'scikit-learn', 'jupyter', 'theano', 'openCV', 'spark', 'nltk', 'mlpack', 'chainer', 'fann', 'shogun', \n",
    "'dlib', 'mxnet', 'node.js', 'vue', 'vue.js', 'keras', 'ember.js', 'jse/jee',\n",
    "]\n",
    "\n",
    "keywords_analyst_tools = [\n",
    "'excel', 'tableau',  'word', 'powerpoint', 'looker', 'powerbi', 'outlook', 'azure', 'jira', 'twilio',  'snowflake', \n",
    "'shell', 'linux', 'sas', 'sharepoint', 'mysql', 'visio', 'git', 'mssql', 'powerpoints', 'postgresql', 'spreadsheets',\n",
    "'seaborn', 'pandas', 'gdpr', 'spreadsheet', 'alteryx', 'github', 'postgres', 'ssis', 'numpy', 'power_bi', 'spss', 'ssrs', \n",
    "'microstrategy',  'cognos', 'dax', 'matplotlib', 'dplyr', 'tidyr', 'ggplot2', 'plotly', 'esquisse', 'rshiny', 'mlr',\n",
    "'docker', 'linux', 'jira',  'hadoop', 'airflow', 'redis', 'graphql', 'sap', 'tensorflow', 'node', 'asp.net', 'unix',\n",
    "'jquery', 'pyspark', 'pytorch', 'gitlab', 'selenium', 'splunk', 'bitbucket', 'qlik', 'terminal', 'atlassian', 'unix/linux',\n",
    "'linux/unix', 'ubuntu', 'nuix', 'datarobot',\n",
    "]\n",
    "\n",
    "keywords_cloud_tools = [\n",
    "'aws', 'azure', 'gcp', 'snowflake', 'redshift', 'bigquery', 'aurora',\n",
    "]\n",
    "\n",
    "keywords_general_tools = [\n",
    "'microsoft', 'slack', 'apache', 'ibm', 'html5', 'datadog', 'bloomberg',  'ajax', 'persicope', 'oracle', \n",
    "]\n",
    "\n",
    "keywords_general = [\n",
    "'coding', 'server', 'database', 'cloud', 'warehousing', 'scrum', 'devops', 'programming', 'saas', 'ci/cd', 'cicd', \n",
    "'ml', 'data_lake', 'frontend','front-end', 'back-end', 'backend', 'json', 'xml', 'ios', 'kanban', 'nlp',\n",
    "'iot', 'codebase', 'agile/scrum', 'agile', 'ai/ml', 'ai', 'paas', 'machine_learning', 'macros', 'iaas',\n",
    "'fullstack', 'dataops', 'scrum/agile', 'ssas', 'mlops', 'debug', 'etl', 'a/b', 'slack', 'erp', 'oop', \n",
    "'object-oriented', 'etl/elt', 'elt', 'dashboarding', 'big-data', 'twilio', 'ui/ux', 'ux/ui', 'vlookup', \n",
    "'crossover',  'data_lake', 'data_lakes', 'bi', 'pack office'\n",
    "]\n",
    "\n",
    "ml_tools = [\n",
    "'Tensorflow', 'Keras', 'PyTorch', 'ScikitLearn', 'sklearn', 'scikit'\n",
    "]\n",
    "\n",
    "big_data_tools = [\n",
    "'Hadoop', 'HDFS', 'YARN', 'Hive', 'Map', 'Reduce', 'Tez', 'Spark'\n",
    "]\n",
    "\n",
    "skills = [\n",
    "'informatique décisionnelle', 'Extraction', 'Nettoyage', 'transformation', 'ingestion', \n",
    "'data visualisation', 'modélisation', 'reporting', 'veille technologique', 'data mining',\n",
    "'kpi', 'computer vision'\n",
    "]\n",
    "\n",
    "soft_skills = [\n",
    "\"Esprit d'analyse\", \"Sens du service\", \"Rigueur\", \"communication\", 'positif', 'créatif', \n",
    "'pragmatique', 'souple', 'agile', \"Autonome\", 'Polyvalent', 'travailler en équipe', \"esprit d'équipe\"\n",
    "'esprit de synthèse', 'aisance relationnelle', 'force de proposition', \"capacité d'analyse\",\n",
    "\"anglais\", \"espagnol\", \"francais\"\n",
    "]\n",
    "\n",
    "coding_languages = [\n",
    "'SQL', 'Python', 'R', 'Julia', 'Scala', 'C++', 'Java', 'Javascript', 'Go'\n",
    "]\n",
    "\n",
    "# Add up all keyword lists\n",
    "all_kw = tools + keywords_programming + keywords_libraries + keywords_analyst_tools + keywords_cloud_tools + keywords_general_tools + keywords_general + ml_tools + big_data_tools + skills + soft_skills + coding_languages\n",
    "\n",
    "# Lower case & remove duplicates\n",
    "all_kw = list(set([word.lower() for word in all_kw]))\n",
    "\n",
    "# keyword extraction\n",
    "data['kw_extraction'] = data['description_normalized'].apply(lambda x: [])\n",
    "\n",
    "for text in enumerate(data.description_normalized):\n",
    "\n",
    "    kw_list = [word for word in all_kw if word in text[1]]\n",
    "    data.loc[text[0], \"kw_extraction\"].extend(kw_list)\n",
    "\n",
    "# number of keywords extracted\n",
    "data['len_list'] = data['kw_extraction'].map(lambda x: len(x))\n",
    "data['len_list'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893224",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d8ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining constants\n",
    "keywords = ['durée', 'stage', 'contrat', 'localisation|lieu|location', 'statut', 'salaire', 'formation', 'profil', \n",
    "            'mission', 'avantage', 'experience', 'soft', 'langue']\n",
    "n_chars_after = 20\n",
    "m_chars_before = 60\n",
    "#col_name = f'charsss_around_{kw}'\n",
    "\n",
    "# loop over keywords list\n",
    "for kw in keywords:\n",
    "    \n",
    "    # define our new columns\n",
    "    data[f'chars_around_{kw}'] = 'NC '\n",
    "\n",
    "    # loop over each string in data.description\n",
    "    for i, text in zip(data.index, data['description_prepro']):\n",
    "        \n",
    "        # get iterator over all keywords matches in each string\n",
    "        search_obj = re.finditer(kw, text, re.I)\n",
    "\n",
    "        if search_obj: # ... is true, then ...\n",
    "\n",
    "            # get each keyword match in string\n",
    "            for match in search_obj:\n",
    "\n",
    "                # Find the start index of the keyword\n",
    "                start = match.span()[0]\n",
    "\n",
    "                # Find the end index of the keyword\n",
    "                end = match.span()[1]\n",
    "\n",
    "                # Truncate line to get only 'n' characters before and after the keyword\n",
    "                line = text[start-m_chars_before:end+n_chars_after]\n",
    "                \n",
    "                # add up ever line containing keyword match (if several) in corresponding cells\n",
    "                data.loc[i, f'chars_around_{kw}'] += line\n",
    "                \n",
    "                # get rid of by default 'NC' string\n",
    "                data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}'].replace('NC ', '')\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # if no keyword match then keep by default value\n",
    "            data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f6a2339",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [{'items': ['contexte : dans le cadre de plusieurs projets d’analyse de données / data science de notre client spécialisé dans les hautes technologies , il souhaite se renforcer par un accompagnement ponctuel d’expertise data science / architecture de chaine de traitement de données. l’objectif de la prestation est de contribuer au développement et à la mise en oeuvre plusieurs chaînes de traitement de données à vocation d’amélioration de l’intégration et du déploiement de système puis de leur maintenance et de leur exploitation. mission :\\n• ces chaînes de traitements devront s’intégrer dans une architecture de type\\n\\natelier de données\\n\\n(a priori la solution dataiku data science studio et la stack elastic) nécessitant une cohérence technique entre les différentes analyses et chaînes de traitements. -une réunion de lancement permettra de partager les objectifs de la prestation, partager les données d’entrées disponibles et à collecter et décrire les livrables attendus. tâches à réaliser (intitulé... et description détaillée) : -exploiter, préparer et modéliser les données émanant du système d’information (logs système et applicatif) ainsi que celles produites par les opérateurs et exploitants (tickets, base de connaissance) afin de réaliser des outils intelligents d’aide à l’exploitation et à l’investigation. l’objectif de la tâche est de contribuer à la conception d’une chaine de traitement de préparation, de mise en cohérence des données et de modélisation de donnée...\n",
      "1                                                                                                                                                                  [{'items': [\"consept informatique recherche …\\n\\nintégré(e) dans l'équipe projet de notre client grand compte, vous êtes accompagné(e) par un parcours d'intégration au sein de notre entité de proximité de 20 consultants avec l'appui d'un groupe indépendant de 100 personnes. vous intervenez sur les fonctions suivantes :\\n- réaliser l'extraction de données afin de réaliser les analyses (web scraping),\\n- procéder au data mining afin de faire des prédictions stratégiques,\\n- créer et effectuer les tests d'algorithmes (machine learning),\\n- effectuer le traitement d'images et de texte (pattern matching),\\n- effectuer un reporting des resultats en s'appuyant des outils de dataviz, dashboards,\\n- participer aux vérifications lors des phases de recette,\\n- assurer une veille technologique sur les outils de data science.\\n\\nenvironnement technique :\\n- langage : python, sql,\\n- frameworks : tensorflow, pytorch, hadoop, spark,\\n- intégration continue : docker, kubernetes, jenkins,\\n\\nde formation supérieure en ingénierie... informatique (cursus universitaire ou ecole d'ingénieurs), vous justifiez d'une expérience significatives en machine learning sur des données structurées.\\nen résumé ...\\n• nantes - 44\\n• cdi\\n• 35\\u202f000 - 40\\u202f000 eur par an\\n• secteur informatique • esn\\n• bac +5\\npubliée le 16/02/2023. réf : 149ghpw\"]}]\n",
      "2       [{'items': ['as part of the data science team, you work closely collaboration with the project and engineering teams to move the data projects of our customers forward, contribute to the evolution of the product, and participate in the r&d effort within fieldbox.\\n\\nas such, you have a particular interest in one of the following areas:\\n• data engineering,\\n• machine learning engineering,\\n• predictive modeling,\\n• data visualisation.\\n\\non a daily basis, your missions can be:\\n• transform customer issues into relevant data science issues, formulate the question that needs an answer,\\n• identify and extract the data needed to solve the problem,\\n• perform in-depth analysis of datasets of all sizes,\\n• build training datasets and validation,\\n• create predictive models, put them in production and manage their life cycle,\\n• set up visualizations of the results obtained,\\n• contribute to the reflection on the evolution of the product,\\n• set up exploratory prototypes,\\n• contribute to the implementation of a robust... efficient and easy to use tool set,\\n• contribute to the scientific influence of society through participation in conferences, colloquia, or publications.\\n\\nprofil :\\n\\nyou like challenge and adventure and are looking for an experience in a dynamic start-up working in an innovative and demanding industrial context. you are autonomous and flexible, enjoy learning as much as doing, and know how to communicate your ideas.\\n\\nwe are looking for phd / phd, grande ...\n",
      "3       [{'items': ['le pôle data, composé de 2 data scientist senior, a pour mission de développer des outils et des modèles permettant la simulation et le pilotage de projets agrivoltaïques. ces modèles cherchent à identifier des synergies entre deux systèmes (agronomique et photovoltaïque). les défis sont multiples : la donnée est nombreuse et variée (capteurs terrain, satellites, données météo tierce), le pilotage se fait en temps réel et les modèles intègrent aussi bien des logiques agronomiques que photovoltaïques. tu auras la responsabilité de mener des projets de data science en toute autonomie, de la modélisation jusqu’à la mise en production. tu participeras donc, en collaboration avec tes managers, à développer et mettre en œuvre des solutions intelligentes et des outils innovants dans une logique de création de valeur pour l’agrivoltaïsme (l’agriculteur et les producteurs d’énergie). ces projets étant au cœur de la stratégie d’ombrea tu devras avoir une bonne vision des enjeux de l’entreprise... afin d’avoir un grand impact sur sa trajectoire et sa croissance. tu travailleras notamment avec des chercheurs en agronomie dans la modélisation du comportement des cultures et avec une équipe de développeurs logiciels dans la mise en production des outils de data science. tes principales missions : travailler sur des problèmes complexes alliant des données physiques, climatiques et agronomiques provenant de sources multiples ainsi que sur des séries temporelles, des images o...\n",
      "4       [{'items': [\"resumé : annonceur recrute un data scientist - sql / python (h/f) en contrat pro / alternance à massy (91377). le profil idéal ? formation et expérience de 1 - 2 ans. salaire : selon profil\\n\\ndescription de l'offre :\\n\\na propos de nous:\\n\\ncréateur de l'hypermarché et pionnier de la consommation de masse, carrefour reste fidèle à ses racines tout en se réinventant chaque jour pour permettre à chacun, de manger mieux : plus sain, plus local, plus responsable.\\n\\nnos atouts pour y parvenir ? un réseau multiformat de magasins, la création de services et d'une offre digitale de référence, une coopération renforcée avec les acteurs du monde agricole, de la chaîne alimentaire, de la tech... et aussi\\n\\ndes collaborateurs passionnés, qui s'engagent, challengent leur métier et grandissent ensemble pour réussir la transition alimentaire pour tous.\\n\\nporteuse de cette ambition, la direction carrefour links recrute un(e) : data scientist (f/h) - en alternance\\n\\nles missions :\\n\\nce poste est rattaché à la... directrice data de carrefour links. au sein d'une équipe composée de data analysts intégrée aux rituels product, vous participez à l'évolution fonctionnelle et technique de nos produits data.\\n\\nvous aurez l'opportunité de collaborer avec l'ensemble des équipes product, tech et métiers autour des missions suivantes :\\n\\n● explorer et analyser les données du data lake du groupe carrefour\\n\\ncréer les scriptings d'automatisation de nos routines (recette…)\\n\\ncréer...\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      "1827    [{'items': ['la mission s\\'effectuera avec les équipes en charge du développement des services financiers d\\'orange en afrique: orange bank africa et orange money.\\n\\nces services sont en forte croissance et fonctionne d\\'une manière différente des services bancaires occidentaux, avec notamment l\\'attribution de micro-crédits en temps réel. ce contexte demande une évaluation (ou scoring) très rapide du risque de non-remboursement de chaque client au moment de la demande de prêt.\\n\\ncette évaluation est faite à partir des données préexistantes du client dans son utilisation de l\\'ensemble des services financiers d\\'orange.\\n\\ncomme elle doit être faite en temps réel, on s\\'appuie sur des algorithmes avancés à base de machine learning.\\n\\nil importe ensuite de suivre le comportement de ces algorithmes de comparer leurs prévisions à la réalité observée, puis de chercher à améliorer leur fonctionnement en conséquence.\\n\\nla mission de ce stage aura donc deux axes:\\n\\ndata analysis: analyse des données d\\'exploitation des... services de crédit d\\'orange bank africa, dans une optique de support au pilotage opérationnel des risques sur le crédit. l\\'objectif sera de surveiller la stabilité du modèle de scoring, les évolutions de la clientèle par rapport à la phase d\\'apprentissage, les autres dérives éventuelles.\\n\\ndata science: evaluer les meilleures approches pour améliorer le modèle de scoring: ajustement de paramètres, mais surtout nouvelles données à prendre en compte (ou,...\n",
      "1828    [{'items': [\"ombrea, l’excellence de l’agrivoltaïsme au service des cultures.\\n\\nnous avons pour ambition de développer des meilleurs systèmes et savoir-faire agrivoltaïques pour les agriculteurs. nous combinons les systèmes agricoles et photovoltaïques, cherchons des synergies, afin de produire de l’énergie électrique tout en garantissant des rendements agricoles stables et élevés. selon l’ademe, le développement du photovoltaïque (pv) dans les exploitations agricoles contribuera largement à l’atteinte de l’objectif de 40 % d’énergies renouvelables dans la production d’électricité d’ici 2030. nous croyons que la technologie, l’intelligence collective et l’audace d’entreprendre peuvent accélérer la transition énergétique.\\n\\nle pôle data, composé de 2 data scientist senior, a pour mission de développer des outils et des modèles permettant la simulation et le pilotage de projets agrivoltaïques. ces modèles cherchent à identifier des synergies entre deux systèmes (agronomique et photovoltaïque). les... défis sont multiples : la donnée est nombreuse et variée (capteurs terrain, satellites, données météo tierce), le pilotage se fait en temps réel et les modèles intègrent aussi bien des logiques agronomiques que photovoltaïques.\\n\\ntu auras la responsabilité de mener des projets de data science en toute autonomie, de la modélisation jusqu’à la mise en production. tu participeras donc, en collaboration avec tes managers, à développer et mettre en œuvre des solutions intelligent...\n",
      "1829    [{'items': [\"chez bayer nous sommes des visionnaires motivés pour résoudre les défis de notre époque. nous œuvrons pour un monde où la santé pour tous, la faim pour personne n’est plus un rêve, mais bien une possibilité. nous agissons avec énergie, curiosité et un engagement absolu, en apprenant en permanence grâce à l’éclairage unique de ceux qui nous entourent, en élargissant notre réflexion, en développant nos compétences et en redéfinissant le champ des possibles. il y a tellement de raisons de nous rejoindre. vous avez envie d’une carrière pleine de sens, aux horizons diversifiés ? vous souhaitez rejoindre une communauté qui se distingue par les esprits les plus brillants afin de contribuer au changement ? le choix est simple !\\n\\ndata scientist en données agronomiques (f/h) - france ou hongrie\\n\\nnotre équipe de recherche en production de semences grandes cultures emea (europe, middle east & africa) ouvre un poste de data scientist (f/h).\\n\\nce poste est basé de préférence en france (peyrehorade... (40) ou monbéqui (82)) mais nous sommes également ouverts à le localiser en hongrie si nous trouvons un candidat avec les compétences attendues dans ce pays.\\n\\nles principales responsabilités de ce rôle sont les suivantes :\\n• développer des algorithmes en r et/ou python pour les processus de qualité des données, analyser et visualiser de grands ensembles de données multivariées ;\\n• concevoir et prototyper des modèles statistiques à utiliser pour les étapes d'exploratio...\n",
      "1830    [{'items': [\"quelles sont les missions ?\\nentreprise\\n\\nl'entreprise évolue dans un éco système autour des energies propres.\\nl'équipe data est composée de 3 personnes.\\nmission\\n\\nle rôle du data scientist est de transformer les données en informations précieuses. cela répondra aux besoins commerciaux pour l'aide aux prises de décision complexe.\\n\\nvous travaillerez sur le développement d'algorithmes et de modèles d'optimisation en mettant l'accent sur le domaine opérationnel de notre entreprise. pour bien faire cela, vous devez prendre soin de vos parties prenantes et de vos utilisateurs. une concentration et une amélioration continues de la qualité des données (entrantes)\\nprofil\\n\\nvous détenez un diplôme universitaire (m.sc.) en mathématiques, statistiques ou études quantitatives connexes\\nvous avez une expérience avérée dans la construction ou l'utilisation de modèles prédictifs et prescriptifs\\nvous avez au moins 1 à 3 ans d'expérience avec un langage de programmation comme r ou python\\nvous êtes... expérimenté dans le travail avec de grands ensembles de données (> 1 million de lignes) et des processus de prise de décision basés sur les données\\nvous avez de l'expérience dans\\n\\nqui a publié cette offre ?\\nsept lieues est un cabinet de recrutement spécialisé sur les secteurs digital et electronics.\\n\\ngrâce à notre expertise du digital et nos spécialisations technologiques, nous vous accompagnons dans vos recherches de talents ou d'opportunités en cdi sur toute la ...\n",
      "1831    [{'items': [\"type de contrat\\n\\ncdi\\n\\nqui sommes-nous\\n\\navec 90 000 collaborateurs présents sur les cinq continents, suez est un leader mondial dans la gestion intelligente et durable des ressources. le groupe fournit des solutions de gestion de l'eau et des déchets qui permettent aux villes et aux industries d’optimiser la gestion de leurs ressources et d’améliorer leurs performances environnementale et économique.\\n\\nvotre mission\\n\\nles missions de la data office visent à accélérer les usages de la data au sein de suez pour apporter de la valeur à nos métiers à travers les données, en travaillant sur 4 piliers : architecture, rôle & responsabilité, analytics et culture.\\n\\nles fondamentaux suivant ont déjà été posés chez suez\\u202f\\n\\npour atteindre ses objectifs de création de nouveaux services et d’amélioration de la performance opérationnelle, l’ia (modélisation, traitement et analyse des données, apprentissage, deep learning) joue un rôle central de levier et d’accélérateur.\\n• une communauté de data... scientists constituée et animée.\\n• une plateforme collaborative de pointe dans une environnement azure\\n• une gouvernance de l’ia depuis l’idéation jusqu’à la production\\n• une forte connexion entre les métiers (du mode des déchets ou de l’eau) et l’équipe data via des projets ou des équipes hybrides.\\n\\npour poursuivre cette démarche, la data office recherche un senior data scientist, intégré à l’équipe data science pour contribuer au développement des solutions...\n",
      "Name: job_highlights, Length: 1832, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.job_highlights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc98e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
