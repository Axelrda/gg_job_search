{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b0e086",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "3ba96d83-40fb-44da-afdf-a8b407c9453c",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "# Preparation of job postings for analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6147d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "50200be7-91b2-4942-80b5-4254f4430d9f",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "In this notebook, we'll be preparing data for analysis in a project focused on predicting salaries from job postings. We'll be working with a dataset of scraped job postings from Google Job search results. \n",
    "\n",
    "The goal of this notebook is to clean and transform the data so that it is ready for modeling and further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe775b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "cb3d5e4f-59d2-4369-8126-042f7044a6af",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9487d252",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "2a68fb3e-6a83-4da6-b9c7-4041a91b3a91",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# libraries used for tokenization\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
    "\n",
    "# Libraries used to remove similar job description based on cosine similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# libraries used for text normalization\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "# lemmatization w/ spacy\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA\n",
    "from spacy import displacy\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# use to get USdollar / euro exchange rate from exchange rate API\n",
    "import requests\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "# import my customs functions\n",
    "from preprocessing import preprocess as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd10655",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "f812b570-77bc-4fec-8d39-f57ea0ffcb31",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced546c5",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "d147cfb5-c20c-4316-b77e-7f72f2839663",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>via</th>\n",
       "      <th>description</th>\n",
       "      <th>job_highlights</th>\n",
       "      <th>related_links</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>extensions</th>\n",
       "      <th>job_id</th>\n",
       "      <th>posted_at</th>\n",
       "      <th>schedule_type</th>\n",
       "      <th>date_time</th>\n",
       "      <th>search_query</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Data Scientist Lead H/F</td>\n",
       "      <td>Fifty-Five</td>\n",
       "      <td>Paris</td>\n",
       "      <td>via HelloWork</td>\n",
       "      <td>Fifty-Five recherche …\\n\\nDescription\\nData Sc...</td>\n",
       "      <td>[{'items': [\"Fifty-Five recherche …\\n\\nDescrip...</td>\n",
       "      <td>[{'link': 'https://www.google.fr/search?gl=fr&amp;...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>['il y a 16 heures', 'À plein temps']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBMZWFkIE...</td>\n",
       "      <td>il y a 16 heures</td>\n",
       "      <td>À plein temps</td>\n",
       "      <td>2023-02-17 11:50:44.355114</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>DATA SCIENTIST (IT) / Freelance</td>\n",
       "      <td>Société Free-Work</td>\n",
       "      <td>Gennevilliers</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Contexte : Dans le cadre de plusieurs projets ...</td>\n",
       "      <td>[{'items': ['Contexte : Dans le cadre de plusi...</td>\n",
       "      <td>[{'link': 'https://www.google.fr/search?gl=fr&amp;...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>['il y a 17 heures', 'À plein temps']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEQVRBIFNDSUVOVElTVCAoSVQpIC...</td>\n",
       "      <td>il y a 17 heures</td>\n",
       "      <td>À plein temps</td>\n",
       "      <td>2023-02-17 11:50:44.355114</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Data analyst / Data scientist H/F</td>\n",
       "      <td>GrandVision France</td>\n",
       "      <td>Châtenay-Malabry</td>\n",
       "      <td>via Figaro Emploi</td>\n",
       "      <td>Quelles sont les missions ?\\n\\nVous\\navez beso...</td>\n",
       "      <td>[{'items': [\"Quelles sont les missions ?\\n\\nVo...</td>\n",
       "      <td>[{'link': 'https://www.google.fr/search?gl=fr&amp;...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>['il y a 11 heures', 'À plein temps']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIGFuYWx5c3QgLyBEYXRhIH...</td>\n",
       "      <td>il y a 11 heures</td>\n",
       "      <td>À plein temps</td>\n",
       "      <td>2023-02-17 11:50:44.355114</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist H/F</td>\n",
       "      <td>Consept Informatique</td>\n",
       "      <td>Nantes</td>\n",
       "      <td>via HelloWork</td>\n",
       "      <td>Consept Informatique recherche …\\n\\nIntégré(e)...</td>\n",
       "      <td>[{'items': [\"Consept Informatique recherche …\\...</td>\n",
       "      <td>[{'link': 'https://www.google.fr/search?gl=fr&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['il y a 11 heures', '35\\xa0k\\xa0€ à 40\\xa0k\\x...</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBIL0YiLC...</td>\n",
       "      <td>il y a 11 heures</td>\n",
       "      <td>À plein temps</td>\n",
       "      <td>2023-02-17 11:50:44.355114</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist - Industrial Field</td>\n",
       "      <td>Fieldbox</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>As part of the Data Science team, you work clo...</td>\n",
       "      <td>[{'items': ['As part of the Data Science team,...</td>\n",
       "      <td>[{'link': 'https://www.google.fr/search?gl=fr&amp;...</td>\n",
       "      <td>https://encrypted-tbn0.gstatic.com/images?q=tb...</td>\n",
       "      <td>['il y a 19 heures', 'À plein temps']</td>\n",
       "      <td>eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAtIEluZH...</td>\n",
       "      <td>il y a 19 heures</td>\n",
       "      <td>À plein temps</td>\n",
       "      <td>2023-02-17 11:50:44.355114</td>\n",
       "      <td>data engineer</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              title          company_name  \\\n",
       "0           0            Data Scientist Lead H/F            Fifty-Five   \n",
       "1           1    DATA SCIENTIST (IT) / Freelance     Société Free-Work   \n",
       "2           2  Data analyst / Data scientist H/F    GrandVision France   \n",
       "3           3                 Data Scientist H/F  Consept Informatique   \n",
       "4           4  Data Scientist - Industrial Field              Fieldbox   \n",
       "\n",
       "           location                via  \\\n",
       "0             Paris      via HelloWork   \n",
       "1     Gennevilliers       via LinkedIn   \n",
       "2  Châtenay-Malabry  via Figaro Emploi   \n",
       "3            Nantes      via HelloWork   \n",
       "4          Bordeaux       via LinkedIn   \n",
       "\n",
       "                                         description  \\\n",
       "0  Fifty-Five recherche …\\n\\nDescription\\nData Sc...   \n",
       "1  Contexte : Dans le cadre de plusieurs projets ...   \n",
       "2  Quelles sont les missions ?\\n\\nVous\\navez beso...   \n",
       "3  Consept Informatique recherche …\\n\\nIntégré(e)...   \n",
       "4  As part of the Data Science team, you work clo...   \n",
       "\n",
       "                                      job_highlights  \\\n",
       "0  [{'items': [\"Fifty-Five recherche …\\n\\nDescrip...   \n",
       "1  [{'items': ['Contexte : Dans le cadre de plusi...   \n",
       "2  [{'items': [\"Quelles sont les missions ?\\n\\nVo...   \n",
       "3  [{'items': [\"Consept Informatique recherche …\\...   \n",
       "4  [{'items': ['As part of the Data Science team,...   \n",
       "\n",
       "                                       related_links  \\\n",
       "0  [{'link': 'https://www.google.fr/search?gl=fr&...   \n",
       "1  [{'link': 'https://www.google.fr/search?gl=fr&...   \n",
       "2  [{'link': 'https://www.google.fr/search?gl=fr&...   \n",
       "3  [{'link': 'https://www.google.fr/search?gl=fr&...   \n",
       "4  [{'link': 'https://www.google.fr/search?gl=fr&...   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "1  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "2  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "3                                                NaN   \n",
       "4  https://encrypted-tbn0.gstatic.com/images?q=tb...   \n",
       "\n",
       "                                          extensions  \\\n",
       "0              ['il y a 16 heures', 'À plein temps']   \n",
       "1              ['il y a 17 heures', 'À plein temps']   \n",
       "2              ['il y a 11 heures', 'À plein temps']   \n",
       "3  ['il y a 11 heures', '35\\xa0k\\xa0€ à 40\\xa0k\\x...   \n",
       "4              ['il y a 19 heures', 'À plein temps']   \n",
       "\n",
       "                                              job_id         posted_at  \\\n",
       "0  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBMZWFkIE...  il y a 16 heures   \n",
       "1  eyJqb2JfdGl0bGUiOiJEQVRBIFNDSUVOVElTVCAoSVQpIC...  il y a 17 heures   \n",
       "2  eyJqb2JfdGl0bGUiOiJEYXRhIGFuYWx5c3QgLyBEYXRhIH...  il y a 11 heures   \n",
       "3  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCBIL0YiLC...  il y a 11 heures   \n",
       "4  eyJqb2JfdGl0bGUiOiJEYXRhIFNjaWVudGlzdCAtIEluZH...  il y a 19 heures   \n",
       "\n",
       "   schedule_type                   date_time   search_query  Unnamed: 0.1  \n",
       "0  À plein temps  2023-02-17 11:50:44.355114  data engineer           NaN  \n",
       "1  À plein temps  2023-02-17 11:50:44.355114  data engineer           NaN  \n",
       "2  À plein temps  2023-02-17 11:50:44.355114  data engineer           NaN  \n",
       "3  À plein temps  2023-02-17 11:50:44.355114  data engineer           NaN  \n",
       "4  À plein temps  2023-02-17 11:50:44.355114  data engineer           NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/axel/ds_projects/projects/gg_job_search/data/gg_job_search_all_RAW.csv')\n",
    "data = df.copy()\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e1272",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "92e1a8ae-b74b-491d-a28f-6c806ee94933",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## General overview of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcbba87",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "ac51a94d-0cb2-47dc-8c3e-51a254cb023d",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26561 entries, 0 to 26560\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      26561 non-null  int64  \n",
      " 1   title           26558 non-null  object \n",
      " 2   company_name    26558 non-null  object \n",
      " 3   location        26523 non-null  object \n",
      " 4   via             26561 non-null  object \n",
      " 5   description     26498 non-null  object \n",
      " 6   job_highlights  20225 non-null  object \n",
      " 7   related_links   26478 non-null  object \n",
      " 8   thumbnail       23054 non-null  object \n",
      " 9   extensions      26561 non-null  object \n",
      " 10  job_id          23439 non-null  object \n",
      " 11  posted_at       24421 non-null  object \n",
      " 12  schedule_type   26297 non-null  object \n",
      " 13  date_time       26561 non-null  object \n",
      " 14  search_query    26561 non-null  object \n",
      " 15  Unnamed: 0.1    6333 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 3.2+ MB\n",
      "None \n",
      "\n",
      "====================================================================== \n",
      "\n",
      " Number of unique values :  \n",
      "\n",
      " Unnamed: 0        6333\n",
      "title             2594\n",
      "company_name      2060\n",
      "location           491\n",
      "via                262\n",
      "description       4719\n",
      "job_highlights    2888\n",
      "related_links     6953\n",
      "thumbnail         1422\n",
      "extensions        2127\n",
      "job_id            5365\n",
      "posted_at           60\n",
      "schedule_type       33\n",
      "date_time         2451\n",
      "search_query         5\n",
      "Unnamed: 0.1        10\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "====================================================================== \n",
      "\n",
      " Search query results : \n",
      "\n",
      " data engineer                8999\n",
      "data analyst                 8513\n",
      "data scientist               8419\n",
      "machine learning engineer     510\n",
      "data scientist jobs           120\n",
      "Name: search_query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.info(), '\\n')\n",
    "print('='*70, '\\n'*2, 'Number of unique values : ', '\\n'*2,data.nunique(), '\\n'*2)\n",
    "print('='*70, '\\n'*2,'Search query results :', '\\n'*2, data.search_query.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01234",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "b90ed404-0bc1-4ffc-b117-df362aca9514",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "### Checking for null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e83723",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "0a0b0ab5-8866-4104-914a-465c5fefa0c1",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a238ecd-65c7-47c4-b5dd-c840e0a4f182",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "1990cc9c-b788-426c-997e-af0275d9442e",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.fillna('unknown' , inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47929c2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "46659ac6-66b8-4737-949b-f05ce626c590",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583af771",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "3a4d97d4-486e-4a1d-886a-7adcc69bdb98",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates : 546\n",
      "Number of duplicates (based on identical job_id) : 21195 \n",
      " ======================================================================\n",
      "Search query results :\n",
      " data scientist               2124\n",
      "data analyst                  844\n",
      "data engineer                 780\n",
      "machine learning engineer     224\n",
      "data scientist jobs            87\n",
      "Name: search_query, dtype: int64 \n",
      " ======================================================================\n",
      "Final number of rows :  4059\n",
      "Final number of columns :  17\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates :',data[data.duplicated()].shape[0])\n",
    "print('Number of duplicates (based on identical job_id) :', data[data.duplicated(['job_id'])].shape[0],'\\n', '='*70) \n",
    "\n",
    "# removing duplicates based on job_id and description\n",
    "data.drop_duplicates(['description'], inplace=True)\n",
    "data.drop_duplicates(['job_id'], inplace=True)\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e020",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "cf7c9502-c34d-4bcd-ad0f-5b3ca8819245",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "Purely based job id, we can skim off a lots of duplicates. Nonetheless i still found a few of identical / near-identical job descriptions, maybe because [sometimes recruiters or companies post the same advert for a job which results in duplicate data.](https://medium.com/analytics-vidhya/data-science-job-search-using-nlp-and-lda-in-python-12ecbfac79f9)\n",
    "\n",
    "To get rid off these ones, i will use cosine similatity between job descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdfec2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "98aa8195-2b74-4f5b-bfc9-c9494f55e881",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "### Removing remaining duplicates w/ Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c825e4e-8626-4298-8b29-ead56daa7a7a",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "dd85e257-4a31-4bfa-9b5c-5c36fe07bde9",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "There are several methods to consider to identify the remaining job postings not captured. We could group them in 3 :\n",
    "\n",
    "    * Fuzzy mathing methods\n",
    "    * Clustering methods\n",
    "    * Topic modeling methods\n",
    " \n",
    "Fuzzy matching methods can be a better choice for identifying job postings with minor variations because it can account for small spelling mistakes, typos and slight variations in the wording or phrasing of the job description. \n",
    "\n",
    "So i will go with cosine similarity which is a measure of the similarity between two non-zero vectors in a high-dimensional space, and is commonly used for comparing text documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145dad6-92f4-4373-95c5-3747bc311bf3",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "71fb7c30-867b-4a53-ba7e-d7f18ff26f5a",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### removing duplicates based on cosine similarity between \n",
    "## job descriptions (code from Thomas Caffrey, see link above)\n",
    "\n",
    "# Defining our collection of job description texts to tokenize\n",
    "data.description = data['description'].fillna('')\n",
    "corpus = data['description']\n",
    "\n",
    "# instantiate CountVectorizer object\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Fit_transform to vectorize each job description (map terms to feature indices)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarities and put it in dataframe\n",
    "cos_df = pd.DataFrame(cosine_similarity(X_train_counts))\n",
    "\n",
    "\n",
    "## reshape dataframe for easier comparison\n",
    "\n",
    "# get arrays of rows indices and col indices from col_df.shape\n",
    "i, j = np.indices(cos_df.shape).reshape(2,-1)\n",
    "\n",
    "# reshape values to get a 1D array \n",
    "cos_values = cos_df.values.reshape(-1)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'i': i, 'j': j, 'sim':cos_values})\n",
    "\n",
    "# get cosine similarity values only above 0.98 \n",
    "cos_rem = cos_sim_df[(cos_sim_df['sim'] > 0.90) & (i!=j)]\n",
    "\n",
    "# Method to remove duplicates but keep first instance:\n",
    "# Trying to drop duplicates on i and j columns won't work as the row numbers of duplicates are either in i or j not both.\n",
    "# Setting another column that combines the i & j values ensures that duplicates can be dropped.\n",
    "\n",
    "cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n",
    "drop_rows = np.unique(cos_rem.drop_duplicates('i*j')['i'].values)\n",
    "\n",
    "# keep only non-duplicated job postings\n",
    "data = data[~data.index.isin(drop_rows)] \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Number of duplicates (based on cosine similarity) : ' ,drop_rows.shape[0])\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242f91",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "35cb596f-91fa-41f4-a157-463a8d814e0e",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Based on this sample of job titles, we could create : \n",
    "    \n",
    "    * Contract_type (full-time, part-time ...) \n",
    "    * Contract_status (CDI, CDD, work-study, internship ...)\n",
    "    * Duration of Contract (Duration/Undetermined)\n",
    "    * Experience ( Senior, Junior ...)\n",
    "    * Data Specialization (Supply chain, Marketing, Clinical ...)\n",
    "    * Multiple titles (Analyst/Scientist, Scientist/ML Engineer, Manager/Analyst ...)\n",
    "    * Specific expertise asked for (Python, Power BI ...)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1679e1",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "c48c73be-79c0-4a27-9180-c81296120997",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Explore company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3400f598",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "d3c32ed0-27e2-47fc-8a2e-d57367794e26",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique companies : 1684\n"
     ]
    }
   ],
   "source": [
    "print('number of unique companies :', data.company_name.nunique())\n",
    "#data.company_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1583ec6d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "f0eb150b-3e79-4738-afd3-1273aff40b1b",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "#data.loc[data['company_name'] == 'Unspecified', ['description']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05598b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "2ca538cb-1fbc-4a61-9e38-699165d3d17f",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "When ***Unspecified***, companies name can be found in description column.\n",
    "\n",
    "Possible new columns :\n",
    "\n",
    "    * Group/Holding (Y/N/NC)\n",
    "    * Interim company (Y/N/NC)\n",
    "\n",
    "Based on the number of job posting per company we could potentially infer about : **size of company ? / Amount of data to work on** / \n",
    "\n",
    "Adding a time variable and much more data, the number of similar / identical job postings for the same company could maybe give insights on the **company's turnover rate / company's growth / magnitude of need-urgency to hire** ...  \n",
    "\n",
    "It seems like extracting additional informations without more context will be difficult. Having access to each company's structure information we could create :\n",
    "\n",
    "    * Size of company\n",
    "    * Industry\n",
    "    * Public / Private\n",
    "    \n",
    "We'll see if can extract more related informations in the following columns. Otherwise, we could try to scrap **Glassdoor databases** (or similar) to get those informations.remains to be seen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0ce67",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "6e1ce413-3955-40bf-97f5-d460045ef60f",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Explore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01afb6d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "29b89ccc-8381-42b5-982d-9bc55a955193",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique locations :  410\n"
     ]
    }
   ],
   "source": [
    "print('number of unique locations : ', data.location.nunique())\n",
    "#data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d39825",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "6591b71f-0bf3-4016-bcf9-3094a7b67f52",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "#data.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cabad",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "6868c971-6ec6-4226-a78d-2c1d082494df",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "We could create a map of th repartition of job posting based on location provided.\n",
    "\n",
    "Some companies don't provide precise location *(ex : location = FRANCE)* and the information is not available in description column either. Further investigations will be needed for these companies, perhaps in conjunction with other databases *(GLASSDOOR / SIRENE databases for instance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2d818",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "bdd3aaa9-7ed4-4f21-accb-59ed18840435",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Explore via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a48ca8",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "c10426ed-a9f4-4776-8c8c-fa0fb5597387",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique job plateforms :  208\n"
     ]
    }
   ],
   "source": [
    "print('number of unique job plateforms : ', data.via.nunique())\n",
    "# data.via.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d3e6d-aecd-4c78-9b32-9e0a8d62625d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "678827ed-12b1-401d-9a91-ae35c30d0ab2",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5b57edd",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "47825c1b-0277-4b20-be9d-5889f3af2ac3",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "## Explore description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acda0f5c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "0dfac044-8ff1-4967-b8e6-35aa307c25ad",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     4059.000000\n",
      "mean      3061.993594\n",
      "std       1639.034912\n",
      "min          7.000000\n",
      "25%       1879.000000\n",
      "50%       2833.000000\n",
      "75%       3971.000000\n",
      "max      14172.000000\n",
      "Name: description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.description.str.len().describe())\n",
    "\n",
    "plt.hist(data.description.str.len(), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf1f6",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "cffe4e39-6b95-434c-837d-2088f930f55b",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "\n",
    "We could create :\n",
    "\n",
    "    * Lenght of description (dunno what informations it could provide yet)\n",
    "    * Toold required (Excel, Google Tag Manager ...)\n",
    "    * Coding languages required (R, Python, SQL ...)\n",
    "    * Skills required (reporting, data visualization)\n",
    "    * Required experience\n",
    "    * Duration of contract\n",
    "    * Avantages (ticket resto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3d5e5-f1a9-4f1f-a1c9-a11ea8ce97dc",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "256a270e-0fad-4b36-b3cf-e69f98bd621d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "data[\"all_bullets_string\"] = [' '.join(map(str, re.findall('•(.+)',data.description[i]))) for i, _ in enumerate(data.description)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4bcd7f-cf03-4f8f-b7ad-cbf74fa852a0",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "3cad0e9a-fb38-44ac-8adb-735758f1be15",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Quick text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aac9330-d79c-4a6e-a1e0-b2e354147345",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "a6b154c4-2bc2-42a7-b194-eab6198f54e9",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "Before creating new features and extractings keywords i'm going to do some simple text normalization for easier parsing and processing : lower cases, remove accents and useless columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df76bedb-f2af-428a-800a-423002ac0a9c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "f67dd88e-94e6-4683-96c1-6c7a62bc8323",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing useless columns\n",
    "def drop_useless_cols(df, cols):\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "cols_to_remove = ['index', 'Unnamed: 0', 'Unnamed: 0.1', 'related_links', 'job_id', 'posted_at', 'thumbnail', 'job_highlights']\n",
    "\n",
    "data = drop_useless_cols(data, cols_to_remove)\n",
    "\n",
    "# lowering cases\n",
    "data = data.applymap(lambda x: x.lower() if type(x) == str else x )\n",
    "\n",
    "# removing accents\n",
    "def remove_accents(text):\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    without_accents = [c for c in normalized if not unicodedata.combining(c)]\n",
    "    return ''.join(without_accents)\n",
    "\n",
    "# get columns to remove accents\n",
    "object_cols = set(data.select_dtypes(include='O').columns)\n",
    "accents_keeped_cols = {'extensions'} # i already worked on extensions before, don't wanna remake my code ...\n",
    "accents_removed_cols = list(object_cols - accents_keeped_cols)\n",
    "\n",
    "data[accents_removed_cols] = data[accents_removed_cols].applymap(lambda x: remove_accents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf429b2-0ef1-4c55-b777-d8398d4b1dd2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "876dcebc-5b8e-42ec-8a75-6f4454570635",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a propos d'alstef group ?\\n\\ngroupe international en pleine croissance, alstef group concoit, integre et maintient des solutions automatisees pour l'amelioration des performances des processus logistiques.\\n\\nnotre mission : creer des solutions intelligentes qui, au-dela de repondre aux besoins de nos clients, leur font beneficier durablement d’un systeme automatise sur-mesure performant, evolutif et innovant.\\n\\nnos activites\\n\\nnous proposons aux aeroports du monde entier des solutions completes et innovantes pour le traitement des bagages de soute.\\n\\nnous offrons egalement des solutions de manutention integrees completes qui s’etendent des chariots automatiques aux entrepots entierement automatises.\\n\\nen quelques chiffres alstef group, c’est\\n• 165 m€ de chiffre d’affaires\\n• 800 collaborateurs\\n• 13 filiales a l’etranger\\n• 26% des salaries actionnaires du groupe\\n\\net si vous commenciez une nouvelle aventure avec alstef group ?\\n\\nvos missions ?\\n\\nau cours de cette alternance, vous participerez aux... differentes phases des projets informatiques dans le cadre de la mise en place d'outils de data visualisation a la dimension d'alstef group (monde).\\n\\nsous la responsabilite d’un chef de projet, vous participerez aux projets de data analyse avec notamment la creation d’un bi groupe. vous aurez aussi la chance de prendre part a la mise en place des logiciels groupe satellites aux erp des entreprises du groupe (ged, crm, gmao, bi, ...) dans un environnement windows.\\n\\na ce titre, vous interviendrez sur les missions suivantes :\\n• identifier, collecter et qualifier les donnees pertinentes pour l’analyse,\\n• manipuler d’importants volumes de donnees en provenance des differents etablissements du groupe,\\n• developper et documenter un programme dans le cadre de votre projet,\\n• echanger avec des responsables metier pour vous assurer des cas d'usages,\\n• formaliser et documenter votre travail a travers un rapport detaille afin d’en assurer une utilisation perenne au sein de la direction.\\n\\ncompetences : manipulation de bdd, methodes de tests, programmation (legere), capacite d’analyse et de restitution.\\n\\net vous ?\\n\\nvous allez integrer une licence, un master ou une ecole d'ingenieur dans le domaine de l'informatique - data analyste / data science (dont la formation est orientee sur la gestion d'entreprise).\\n\\nau-dela de vos competences techniques ce sont aussi vos qualites personnelles qui nous interessent pour cette alternance.\\n\\nvous etes rigoureux(se), curieux(se) et vous etes reconnu(e) pour votre capacite d'analyse et de restitution, alors cette alternance est faite pour vous !\\n\\nce que nous vous proposons ?\\n• alternance (1 a 3 ans) a boigny-sur-bionne (proche d’orleans - 45),\\n• une aventure dans laquelle vous pourrez vous epanouir, apprendre et entreprendre, avec une grande variete de missions et beaucoup d’autonomie,\\n• une equipe ambitieuse, bienveillante et passionnee,\\n• cadre de travail privilegie (petit open space, espaces de reunion,...).\\n\\nattentifs a l’egalite professionnelle, nous nous engageons a favoriser la mixite dans tous les metiers de l’entreprise.\\n\\na competences egales, priorite aux personnes handicapees et autres beneficiaires de l’obligation d’emploi\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.description[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eb7016-0d6c-42ba-a288-6906e42d7db5",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "6d45708b-d872-45de-973d-f8bdd134122d",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Extracting features from job titles & job descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4673551f-e3b3-44a4-bcb2-be79bfd3d7da",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "f45adcec-79de-4a30-b8db-a7eb65336ed6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "As keywords meaning in job descriptions tends to vary a lot more depending on context, i will only extract employment-related kw from it (after investigations employment kws seem to be quite constant in their meaning even in job descriptions).\n",
    "\n",
    "For example, senerioty level keywords contained in job titles are pretty straightforward in term of meaning (employer looking for a specific seniority level) but in job descriptions the same keyword could explain that \"as a junior data analyst you will be supervised by several senior data analyst\". \n",
    "\n",
    "To avoid extracting meaningless keywords i will leave aside job descriptions for now. It'll require fancier extracting methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19a7620b-f19e-4e53-9678-aba9b09f6e1f",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "0eaaec6d-4bcb-4839-aa9e-6fd9ec344c93",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_employment_type(row):\n",
    "    employment_type_dict = {\n",
    "        'CDI': {'cdi'},\n",
    "        'internship': {'stage', 'internship', 'stagiaire'},\n",
    "        'apprenticeship': {'alternant', \"contrat d'apprentissage\", 'contrat de professionnalisation', 'alternance',\n",
    "                           \"(apprenti)\", \"en apprentissage\"},\n",
    "        'CDD': {'cdd'},\n",
    "        'freelance': {'freelance', 'prestataire', 'freelancer'},\n",
    "        'interim': {'poste en Intérim'}\n",
    "}\n",
    "    for employment_type, keywords in employment_type_dict.items():\n",
    "        regex = re.compile(r'\\b(?:%s)\\b' % '|'.join(keywords), re.I)\n",
    "        if regex.search(row['title']) or regex.search(row['description']):\n",
    "            return employment_type\n",
    "    return np.nan\n",
    "\n",
    "def create_seniority_level(row):\n",
    "    seniority_level_dict = {\n",
    "        'senior': {'senior', 'advanced', 'avance', 'sr', 'experimente'},\n",
    "        'mid-level': {'confirme'},\n",
    "        'junior': {'junior', 'debutant', 'jr', 'entry-level'},\n",
    "}\n",
    "    \n",
    "    for seniority_level, keywords in seniority_level_dict.items():\n",
    "        regex = re.compile(r'\\b(?:%s)\\b' % '|'.join(keywords), re.I)\n",
    "        if regex.search(row['title']):\n",
    "                return seniority_level\n",
    "    return np.nan\n",
    "\n",
    "def create_executive_title(row):\n",
    "    executive_title_dict = {\n",
    "        'lead': {'lead'},\n",
    "        'Director': {'director', 'directeur'},\n",
    "        'Manager': {'manager', 'project manager', 'chef de projet'},\n",
    "        'Assistant': {'assistant'},\n",
    "        'Chief': {'chief'},\n",
    "        'Head': {'head'},\n",
    "        'Supervisor': {'supervisor'},\n",
    "}\n",
    "    for executive_title, keywords in executive_title_dict.items():\n",
    "        regex = re.compile(r'\\b(?:%s)\\b' % '|'.join(keywords), re.I)\n",
    "        if regex.search(row['title']):\n",
    "            return executive_title\n",
    "    return np.nan\n",
    "\n",
    "def create_remote_availability(row):\n",
    "    remote_availability_dict = {\n",
    "        'full': {'100 % teletravail', 'remote role', 'full remote', 'teletravail complet', 'fully remote', \n",
    "                 'full-time remote', \"l'entreprise fonctionne en remote\", 'full tt' },\n",
    "        'partial_remote': {'teletravail/semaine', 'teletravail possible', 'jours / mois', '(2j / semaine)', \n",
    "                           'teletravail de','teletravail hybride', 'jours de teletravail','teletravail\\n\\npartiel',\n",
    "                           'teletravail partiel', 'jour de teletravail', 'teletravail par semaine', 'format hybride',\n",
    "                          'jour par semaine', 'jours par semaine', 'j de TT', 'j TT', 'j par semaine', 'jours de tt', 'jours de remote',\n",
    "                          'jour de remote'}\n",
    "}\n",
    "    for remote, keywords in remote_availability_dict.items():\n",
    "        regex = re.compile(r'(?:%s)' % '|'.join(keywords), re.I)\n",
    "        if regex.search(row['title']) or regex.search(row['description']):\n",
    "              return remote\n",
    "    return np.NaN\n",
    "\n",
    "data['employment_type'] = data.apply(create_employment_type, axis=1) \n",
    "data['seniority_level'] = data.apply(create_seniority_level, axis=1)\n",
    "data['executive_title'] = data.apply(create_executive_title, axis=1)\n",
    "data['remote_availability'] = data.apply(create_remote_availability, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23211e26-02b6-4492-b6ef-62be99d543da",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "34df29d6-e4a9-46f7-a74e-ea7883287b5e",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "## Extracting and preprocessing salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b576ae3-5f4d-4387-85c9-20e2e71df59a",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "ea9b7f10-b16a-41b5-bef9-3a14aa7897f1",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_salary_col(df):\n",
    "    \n",
    "    # Extract salary from extensions col\n",
    "    df['extracted_salary'] = pd.Series(df.extensions.str.split(', ', expand=True)[1])\n",
    "       \n",
    "    # Replace non-salary values to np.NaN\n",
    "    df['extracted_salary'].replace([\"'à plein temps']\", \"'à temps partiel']\", \"'stage']\", \"'prestataire']\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Convert null values to Nothing_found to make parsing easier\n",
    "    df.at[df.extracted_salary.isnull(), 'extracted_salary'] = \"nothing_found\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_og_salary_currency(df):\n",
    "    df['og_salary_currency'] = df.extracted_salary.str.extract(r'(\\€|\\$us)', flags=re.IGNORECASE)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_og_salary_period(df):\n",
    "    df['og_salary_period'] = df.extracted_salary.str.extract(r'(\\ban\\b|\\bmois\\b|\\bjour\\b)', flags=re.IGNORECASE)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def clean_salary(df):\n",
    "    \n",
    "    \n",
    "        ####### targeted clean for easier parsing #######\n",
    "\n",
    "        for index, salary_str in enumerate(df.extracted_salary):\n",
    "                \n",
    "                if 'nothing_found' not in salary_str:\n",
    "                    \n",
    "                    # split salary period from rest of string\n",
    "                    salary_str =  salary_str.split(' par')[0]\n",
    "                    \n",
    "                    # remove single quote\n",
    "                    salary_str = salary_str.split(\"'\")[1]\n",
    "\n",
    "                    # remove unicode chars\n",
    "                    salary_str = salary_str.replace(\"\\\\xa0\", \"\")\n",
    "                    salary_str = salary_str.replace(\"\\\\u202f\", \"\")\n",
    "\n",
    "                    #remove currency\n",
    "                    salary_str = salary_str.replace(\"€\", \"\")\n",
    "                    salary_str = salary_str.replace(\"$us\", \"\")\n",
    "                    salary_str = salary_str.replace(\"$US\", \"\")\n",
    "\n",
    "                    df.at[index, 'extracted_salary'] = salary_str\n",
    "                    \n",
    "        return df\n",
    "\n",
    "def get_salary_range_and_mean(df):                    \n",
    "                 \n",
    "                                        \n",
    "        for index, row in df.iterrows():\n",
    "    \n",
    "            # extract boundaries of YEAR salaries given as a range + calculate mean salary\n",
    "            if (row['og_salary_period'] == 'an') and 'à' in row['extracted_salary']:\n",
    "\n",
    "                # remove k \n",
    "                row['extracted_salary'] = row['extracted_salary'].replace('k', '000')\n",
    "\n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.extracted_salary.split(' à ')[0].split(',')[0])\n",
    "                upper_bound = float(row.extracted_salary.split(' à ')[1].split(',')[0])\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if lower_bound < 200:\n",
    "                    lower_bound = lower_bound * 1000\n",
    "                elif lower_bound > 200:\n",
    "                    lower_bound = lower_bound * 100\n",
    "                    \n",
    "                if upper_bound < 200:\n",
    "                    upper_bound = upper_bound * 1000\n",
    "                elif upper_bound > 200:\n",
    "                    upper_bound = upper_bound * 100\n",
    "                \n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            # extract discrete YEAR salaries        \n",
    "            elif (row['og_salary_period'] == 'an') and 'à' not in row['extracted_salary']:\n",
    "\n",
    "                # remove k\n",
    "                row['extracted_salary'] = row['extracted_salary'].replace('k', '000')\n",
    "                row['extracted_salary'] = row['extracted_salary'].replace(',', '.')\n",
    "\n",
    "                # convert value to float\n",
    "                row['extracted_salary'] = float(row['extracted_salary'])\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if row['extracted_salary'] < 1000:\n",
    "                    row['extracted_salary'] = row['extracted_salary'] * 1000\n",
    "\n",
    "                # assign result to discrete salary column\n",
    "                df.at[index, 'discrete_salary'] = row['extracted_salary']\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            # extract boundaries of MONTH salaries given as a range + calculate mean salary   \n",
    "            elif (row['og_salary_period'] == 'mois') and 'à' in row['extracted_salary']:\n",
    "\n",
    "                # remove k and replace commas\n",
    "                row['extracted_salary'] = row['extracted_salary'].replace('k', '00')\n",
    "                row['extracted_salary'] = row['extracted_salary'].replace(',', '.')\n",
    "\n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.extracted_salary.split(' à ')[0])\n",
    "                upper_bound = float(row.extracted_salary.split(' à ')[1])\n",
    "\n",
    "                # re-establish a consistent value regarding to common month salaries\n",
    "                if lower_bound < 10:\n",
    "                    lower_bound = lower_bound * 100\n",
    "\n",
    "                if lower_bound < 1000:\n",
    "                    lower_bound = lower_bound * 10\n",
    "\n",
    "                if upper_bound < 10:\n",
    "                    upper_bound = upper_bound * 100\n",
    "\n",
    "                if upper_bound < 1000:\n",
    "                    upper_bound = upper_bound * 10\n",
    "\n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            # extract discrete MONTH salaries        \n",
    "            elif (row['og_salary_period'] == 'mois') and 'à' not in row['extracted_salary']:\n",
    "\n",
    "                # remove k and replace commas\n",
    "                row['extracted_salary'] = row['extracted_salary'].replace('k', '00')\n",
    "                row['extracted_salary'] = row['extracted_salary'].replace(',', '.')\n",
    "\n",
    "                # convert value to float\n",
    "                row['extracted_salary'] = float(row['extracted_salary'])\n",
    "\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries \n",
    "                if row['extracted_salary'] < 1000 and type(row['employment_type']) == str and row['employment_type'] not in ['internship', 'apprenticeship']:\n",
    "                    row['extracted_salary'] = row['extracted_salary'] * 1000\n",
    "                 \n",
    "                elif row['extracted_salary'] < 100:\n",
    "                    row['extracted_salary'] = row['extracted_salary'] * 1000\n",
    "            \n",
    "                elif row['extracted_salary'] < 1000 and type(row['employment_type']) == str and row['employment_type'] in ['internship', 'apprenticeship']:\n",
    "                    row['extracted_salary'] = row['extracted_salary']\n",
    "                \n",
    "                # assign result to discrete salary column\n",
    "                df.at[index, 'discrete_salary'] = row['extracted_salary']\n",
    "            \n",
    "            # extract boundaries of DAY salaries given as a range + calculate mean salary   \n",
    "            elif (row['og_salary_period'] == 'jour') and 'à' in row['extracted_salary']:\n",
    "                \n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.extracted_salary.split(' à ')[0])\n",
    "                upper_bound = float(row.extracted_salary.split(' à ')[1])\n",
    "\n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "                    \n",
    "                    \n",
    "        return df\n",
    "    \n",
    "            \n",
    " # Define a global variable to cache the exchange rate value\n",
    "cached_exchange_rate = None\n",
    "\n",
    "# Define a function to get the exchange rate value\n",
    "def get_exchange_rate():\n",
    "    # Declare the variable as global to modify its value in the function\n",
    "    global cached_exchange_rate\n",
    "    \n",
    "    try:\n",
    "        # Send a network request to get the exchange rate\n",
    "        response = requests.get('https://openexchangerates.org/api/latest.json?app_id=4820391575d04bdd8d07b7e15fb0a463')\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response and calculate the exchange rate\n",
    "        data = response.json()\n",
    "        exchange_rate = data['rates']['EUR'] / data['rates']['USD']\n",
    "        \n",
    "        # Cache the exchange rate value\n",
    "        cached_exchange_rate = exchange_rate\n",
    "        \n",
    "    except (requests.exceptions.RequestException, json.decoder.JSONDecodeError) as e:\n",
    "        # Handle any exceptions that occur during the API request\n",
    "        # If the API request fails, use the cached exchange rate value if it exists\n",
    "        if cached_exchange_rate is not None:\n",
    "            exchange_rate = cached_exchange_rate\n",
    "        else:\n",
    "            # If there is no cached exchange rate value, raise the original exception\n",
    "            raise e\n",
    "    \n",
    "    return exchange_rate           \n",
    "            \n",
    "# This function converts salary values in US dollars to euro currency\n",
    "def convert_salary_currency(df):\n",
    "    \n",
    "    # Create a boolean mask to select rows where salary is in US dollars\n",
    "    mask = df['og_salary_currency'] == '$us'   \n",
    "    \n",
    "    # Multiply the 'discrete_salary' column by the exchange rate\n",
    "    df.loc[mask,'discrete_salary'] *= get_exchange_rate()\n",
    "    \n",
    "    # Return the modified dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "# This function converts salary values from their original period to yearly, monthly, and daily rates\n",
    "def convert_salary_period(df):\n",
    "    \n",
    "    # Define constants used in the conversion calculations\n",
    "    n_days_per_year = 250\n",
    "    n_days_per_month = 20\n",
    "        \n",
    "    # Create a boolean mask to select rows where salary is reported annually\n",
    "    mask = df['og_salary_period'] == 'an'\n",
    "\n",
    "    # Convert the 'discrete_salary' values in the selected rows to yearly, monthly, and daily rates\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary']\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] / 12\n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] / n_days_per_year\n",
    "\n",
    "    # Create a boolean mask to select rows where salary is reported monthly\n",
    "    mask = df['og_salary_period'] == 'mois'\n",
    "\n",
    "    # Convert the 'discrete_salary' values in the selected rows to yearly, monthly, and daily rates\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary'] * 12\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] \n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] / n_days_per_month\n",
    "\n",
    "    # Create a boolean mask to select rows where salary is reported daily\n",
    "    mask = df['og_salary_period'] == 'jour'\n",
    "\n",
    "    # Convert the 'discrete_salary' values in the selected rows to yearly, monthly, and daily rates\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary'] * n_days_per_year\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] * n_days_per_month\n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] \n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df\n",
    "    \n",
    "def salary_prepro(df):\n",
    "    \n",
    "    df = get_salary_col(df)\n",
    "    df = get_og_salary_period(df)\n",
    "    df = get_og_salary_currency(df)\n",
    "    df = clean_salary(df)\n",
    "    df = get_salary_range_and_mean(df)\n",
    "    df = convert_salary_period(df)\n",
    "    df = convert_salary_currency(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "data = salary_prepro(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bacc47cc-4ee7-4c7b-9999-973c5faf2cc4",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "fdd67a66-7be5-4b89-bd1b-edfb72a916ce",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             NaN\n",
       "1             NaN\n",
       "2             NaN\n",
       "3       3750000.0\n",
       "4             NaN\n",
       "          ...    \n",
       "4054          NaN\n",
       "4055          NaN\n",
       "4056          NaN\n",
       "4057          NaN\n",
       "4058          NaN\n",
       "Name: year_salary, Length: 4059, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.year_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61b1b1df-5d31-47f7-b216-4eb70faf2d56",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "inherit",
     "id": "4ac35c2d-05d7-4f7b-a76a-278fc461613c",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fzg\u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "fzg= f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893224",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "702f5f80-e124-43ad-88e0-033db4c04edc",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "## Finding remaining salaries hidden in job descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ab559-9163-43f9-a586-3cef570b6a5b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "38d67a85-250a-4317-bf6f-bf64219f2405",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "After thorough research, some salaries are not mentionned in extensions column but in the job descriptions one. Problem is that the salary information comes in many forms, hence the difficulty to extract it. To get a better sense of the many forms it can take i will extract n number of characters before and after a proxy keyword ('salaire').\n",
    "\n",
    "As per i hope to get the full sentence in which the salary information is for each description containing the proxy keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d8ca1",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "a072efa0-66de-4606-856d-d1a8bd4bf85d",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining constants\n",
    "keywords = ['salaire', 'remuneration']#, 'durée', 'stage', 'contrat', 'localisation|lieu|location', 'statut', 'formation', 'profil', \n",
    "            #'mission', 'avantage', 'experience', 'soft', 'langue']\n",
    "n_chars_after = 200\n",
    "m_chars_before = 150\n",
    "#col_name = f'charsss_around_{kw}'\n",
    "\n",
    "# loop over keywords list\n",
    "for kw in keywords:\n",
    "    \n",
    "    # define our new columns\n",
    "    data[f'chars_around_{kw}'] = 'NC '\n",
    "\n",
    "    # loop over each string in data.description\n",
    "    for i, text in zip(data.index, data['description']):\n",
    "        \n",
    "        # get iterator over all keywords matches in each string\n",
    "        search_obj = re.finditer(kw, text, re.I)\n",
    "\n",
    "        if search_obj: # ... is true, then ...\n",
    "\n",
    "            # get each keyword match in string\n",
    "            for match in search_obj:\n",
    "\n",
    "                # Find the start index of the keyword\n",
    "                start = match.span()[0]\n",
    "\n",
    "                # Find the end index of the keyword\n",
    "                end = match.span()[1]\n",
    "\n",
    "                # Truncate line to get only 'n' characters before and after the keyword\n",
    "                line = text[start-m_chars_before:end+n_chars_after]\n",
    "                \n",
    "                # add up ever line containing keyword match (if several) in corresponding cells\n",
    "                data.loc[i, f'chars_around_{kw}'] += line\n",
    "                \n",
    "                # get rid of by default 'NC' string\n",
    "                data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}'].replace('NC ', '')\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # if no keyword match then keep by default value\n",
    "            data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc52a2-ae00-4c08-90b7-b5413ebeca47",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "2a4ce602-4638-4a9e-8d66-af90c08ecca2",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 4500\n",
    "pd.options.display.max_rows = 144\n",
    "\n",
    "mask1 = data.discrete_salary.isnull()\n",
    "mask2 = ~data.discrete_salary.isnull()\n",
    "\n",
    "mask3 = data.description.str.contains(r'remuneration')\n",
    "\n",
    "mask4 = ~data.description.str.contains(r'salaire')\n",
    "mask5 = data.description.str.contains(r'salaire')\n",
    "\n",
    "mask6 = data.chars_around_salaire != 'NC '\n",
    "mask7 = data.chars_around_salaire == 'NC '\n",
    "\n",
    "mask8 = data.chars_around_remuneration != 'NC '\n",
    "mask9 = data.chars_around_remuneration == 'NC '\n",
    "\n",
    "\n",
    "print('Number of salaire word found in descriptions : ', mask5.sum())\n",
    "print('Number of remuneration word found in descriptions : ', mask3.sum())\n",
    "\n",
    "print('NUmber of processed discrete salaries : ', mask2.sum())\n",
    "print('Unexploited salary info w/ \"salaire\" : ',data[ mask1 & mask6]['chars_around_salaire'].shape[0])\n",
    "print('Unexploited salary info w/ \"rémunération\" : ', data[ mask1 & mask7 & mask3].shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550d5b1-9aeb-4508-a68c-dba9181f26d0",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "d2c63639-1526-4538-b6c3-ecb30e7cfae0",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask9 = data.description.str.contains(r'€')\n",
    "\n",
    "data.loc[ mask1 & mask6 & mask3 & mask9, ['chars_around_salaire']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38e611-45a2-47af-86b5-d27be66c8a22",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "a9caf84b-3510-47ea-9941-a19133a977f9",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "The already formated data in discrete_salary being scarce, it is crucial to extract the most of salary information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b3015-c84f-4515-a0d4-92f56fdc796f",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "f724111b-07a7-4472-9f8c-106a85be2fdf",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get null rows in discrete_salary AND not null rows in chars_around_salary\n",
    "pd.options.display.max_rows = 268\n",
    "data[(data.discrete_salary.isnull()) & (data.chars_around_salaire != 'NC ')]['chars_around_salaire']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0af31-d021-4ed8-ae17-2f90860b5c58",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "6f0435fd-762e-48f9-9ae7-6a3ca5b3057b",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "After investigation, salaries informations do come from in several differents forms. To that high variablity in salary information we need to add typing errors, incomplete salary ranges or inconsistant salary periods.\n",
    "\n",
    "And to make matters even worse, in several job descriptions we can find a lots of similar informations to salaries (bonus, incentives, profit sharing ...) which add another difficulty to discriminate and extract salaries only.\n",
    "\n",
    "Using rule-based approaches (for loops) to extract and clean up salary data would to be tideous and error-prone as new data is collected, while using regex expressions would quickly become a hassle and readability nightmare.\n",
    "\n",
    "To solve these problems, i will rely on pre-trained QA model languages, which can retrieve informations using question asked about the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160d158-ee80-4b3f-87c1-370da3e310be",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": "576395f6-16c5-452e-a34e-97b560a66e48",
     "diskcache": false,
     "headerColor": "inherit",
     "id": "142e7042-4768-49ca-850a-704011cbf96e",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"What is the pay or salary range for the role?\"\n",
    "texts = [\"etes a l'aise sur du cloud (azure, aws ou gcp)\\n• une curiosite technique a toute epreuve et l’envie de prendre part a un projet data from scratch\\n\\nle salaire & avantages\\n• 60-65 k€ selon experience\\n• carte lunchr & mutuelle\\n• 2-3 jours de teletravail par semaine\\n• incentives organises par la direction\\n\\net plus encore...\\n\\nce qu’on prefere\\n• etre implique a\", \n",
    "         \"lle et bordeaux, nous poursuivons notre maillage local en ouvrant 3 agences a lyon, aix et nantes.\\n\\nstatut : cadre\\n\\ntype d'emploi : temps plein, cdi\\n\\nsalaire : 25 000,00€ a 45 000,00€ /an\",\n",
    "        \"ics recherche pour son site de satolas (38) un data analyst h/f. autres offres de l'entreprise personne en charge du recrutement marc low - dirigeant salaire a negocier prise de poste des que possible experience minimum 3 ans metier data analyst statut du poste cadre du secteur prive zone de deplacement pas de deplacement secteur d’activite du poste entre\",\n",
    "        \"tachement : ​​​​​​equipe analyseniveau hierarchique : operationnelremuneration : de 50k a 60k annuel brut + 8% de variable + interessement (1 mois de salaire)teletravail : 2 jours possible par semaine​​​​​​​demarrage : 1 a 90 jours cette opportunite vous correspond ?adressez-nous votre candidature et un consultant vous contactera dans les plus brefs delai\",\n",
    "        \"ale (8 personnes).- des evolutions possibles au sein du groupe.- des avantages : statut cadre, 24 jours de rtt/an, interessement entre 2 et 4 mois de salaire, mutuelle prise en charge a 90%, avantages salaries sur certains produits bancaires et d’assurance, un accord de teletravail, une restauration d’entreprise sur place, une prime variable de 973€/an et produits et un 13eme mois.salaire fixe sur 13 mois en fonction de votre experience\",\n",
    "        \"l’entreprise et son processus de recrutement n’hesitez pas a consulter notre site internet : decilia.fr type d'emploi : temps plein, cdistatut : cadresalaire: 38000,00€a45000,00€par anavantages: horaires flexibles rttprogrammation: du lundi au vendredi horaires flexibles travail en journeetypes de primes et de gratifications:* primeslieu du poste : teletr\"]\n",
    "ranges = [qa_model(question = question, context = x)['answer'] for x in texts if qa_model(question = question, context = x)['score'] > 0.0005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72982e-454a-4ea6-b88f-c19f48fbdb4e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": "dc4f4f14-96d6-480a-8910-dcc0eda49eef",
     "diskcache": false,
     "headerColor": "inherit",
     "id": "495da39d-0cd8-4a02-8116-413bab079a9f",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20a043-4559-4041-b4c6-6197e739b887",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "95e4d4db-50c1-4541-8a44-9c83d7aed84d",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# capturing salary information using regex\n",
    "pattern1 = r'salaire\\s*\\s*((?:.|\\n)*?)\\n'\n",
    "for index, text in enumerate(data.description):\n",
    "    data.at[index, 'salary_in_description'] = \"\".join(re.findall(pattern1, text))\n",
    "\n",
    "data.salary_in_description.replace('', np.nan, inplace=True)\n",
    "\n",
    "print('number of captured salaries info from description :', data.salary_in_description.notnull().sum(), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf11837-9d65-4440-8588-05231ba26dfb",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "11b4ac7a-e8db-4b09-a647-a80dcd3fb4b5",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "mask = data.salary_in_description.notnull()\n",
    "\n",
    "data.salary_in_description.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58f2a4-9c51-4ca3-9c90-48b851a2c9b7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "71580805-4fc5-48bc-b231-5949c5018cc3",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "Using a regex pattern we can extract a substantial amount of salaries. Let's proceed with cleaning and assigning values to each corresponding columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ea8ae-13b0-47c6-bba2-4fc81d4d37c7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "3ab9dbae-1dd8-4188-ab54-d0c008942318",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "# FIRST CLEAN using vectorized operations \n",
    "data.salary_in_description = data.salary_in_description.astype(str) #cast column type for easier iteration\n",
    "data.salary_in_description = data.salary_in_description.replace(['nan', 'None'], 'NC', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r'[a|à] partir de ', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r'^.*?entre\\s*', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('/an', 'par an', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('brut annuel', 'par an', regex=True)\n",
    "\n",
    "# get salary period information and assign it to og_salary_period column, it'll serve for later conversion\n",
    "data.og_salary_period.fillna(data.salary_in_description.str.extract(r'(\\ban\\b|\\bmois\\b|\\bjour\\b)', expand=False), axis=0, inplace=True)\n",
    "data.og_salary_currency.fillna(data.salary_in_description.str.extract(r'(\\€|\\$us)', expand=False), axis=0, inplace=True)\n",
    "\n",
    "data.salary_in_description = data.salary_in_description.str.replace('brut', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r'selon.*', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r'^.*?:\\s*', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r'^.*?:\\s*', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(' par an.*', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(' par mois.*', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('eur', '€', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(' k', '000', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('et', 'à', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('/', ' à ', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('-', ' à ', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('+', '', regex=True) \n",
    "data.salary_in_description = data.salary_in_description.str.replace('•', '', regex=True) \n",
    "\n",
    "\n",
    "data.salary_in_description = data.salary_in_description.str.replace('€', '', regex=True)\n",
    "\n",
    "# formating string like '3035k' to '30 à 35k' \n",
    "pattern = r'\\b(\\d{2})(\\d{2})[kK]\\b'\n",
    "add_a = r'\\1 à \\2k'\n",
    "\n",
    "data['salary_in_description'] = data['salary_in_description'].apply(lambda x: re.sub(pattern, add_a, x))\n",
    "\n",
    "# replace 'k' by '000' in strings like '45 à 55k' \n",
    "def add_zeros(match):\n",
    "    num1 = match.group(1)\n",
    "    num2 = match.group(2)\n",
    "    return str(int(num1) * 1000) + \" à \" + num2 + \"k\"\n",
    "data['salary_in_description'] = data['salary_in_description'].apply(lambda x: re.sub(r'(\\d+) à (\\d+)k', add_zeros, x))\n",
    "\n",
    "# remove space between thousands and hundreds units in strings like '2 292,00 à 4 774,00'\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r'(?<=\\d) (?=\\d{3})', '', regex=True)\n",
    "\n",
    "# remove commas from salaries\n",
    "data.salary_in_description = data.salary_in_description.str.replace(',00', '', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace(r',..', '', regex=True) \n",
    "\n",
    "\n",
    "# remove points from salaries\n",
    "data.salary_in_description = data.salary_in_description.str.replace('.', '', regex=True)\n",
    "\n",
    "# replace '  à  ' by ' à '\n",
    "data.salary_in_description = data.salary_in_description.str.replace('  à  ', ' à ', regex=True)\n",
    "data.salary_in_description = data.salary_in_description.str.replace('  à ', ' à ', regex=True)\n",
    "\n",
    "# replace every character after k included by '000'\n",
    "data.salary_in_description = data.salary_in_description.str.replace('k.*', '000', regex=True)\n",
    "\n",
    "# remove remaining whitespace from the beginning of strings\n",
    "data.salary_in_description = data.salary_in_description.str.lstrip(' ')\n",
    "\n",
    "# add '000' to strings like '30 à 35' \n",
    "def add_zeros(match):\n",
    "    return match.group(1) + '000'\n",
    "data['salary_in_description'] = data['salary_in_description'].apply(lambda x: re.sub(r'\\b(\\d{2})\\b', add_zeros, x))\n",
    "\n",
    "# SECOND CLEAN using iterations : loop over remaining salaries for final clean \n",
    "# + assigning cleaned values to discrete_salary, lower_bound and upper_bound columns\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    # assign 'NC' to string not containing any digits (can't contain salary infos)\n",
    "    if not any(char.isdigit() for char in row['salary_in_description']):\n",
    "        data.at[index,'salary_in_description'] = 'NC'\n",
    "    \n",
    "    # assign 'NC' to string containing a whitespace between thousands and hundreds units (probably typing error - unexplotable infos)\n",
    "    elif re.search(r'(?<=\\d) (?=\\d{3})', row['salary_in_description']):\n",
    "        data.at[index,'salary_in_description'] = 'NC'\n",
    "    \n",
    "    # remove lenghty string (irrelevant information) and short strings (probably typing errors- unexplotable infos) == keep only properly formated salary info\n",
    "    elif len(row['salary_in_description']) > 20 or len(row['salary_in_description']) <= 1:\n",
    "            data.at[index,'salary_in_description'] = 'NC'                    \n",
    "    \n",
    "    # get rows containing ' à ' >>> extract salary range\n",
    "    elif ' à ' in row['salary_in_description'] and row['salary_in_description'] != 'NC':\n",
    "        #print(row.salary_in_description)\n",
    "        # get salary boundaries\n",
    "        data.at[index,'lower_bound'] = int(row['salary_in_description'].split(' à ')[0])\n",
    "        data.at[index,'upper_bound'] = int(row['salary_in_description'].split(' à ')[1])\n",
    "        \n",
    "        # get mean of lower and upper bound\n",
    "        data.at[index, 'discrete_salary'] = (data.loc[index, 'lower_bound'] + data.loc[index, 'upper_bound']) / 2\n",
    "        \n",
    "        # deduce salary period from wage value for strings who didn't contain any mention about period\n",
    "        if int(row['salary_in_description'].split('à')[0]) > 20000:\n",
    "            data.at[index, 'og_salary_period'] = 'an'\n",
    "        else:\n",
    "            data.at[index, 'og_salary_period'] = 'mois'\n",
    "            \n",
    "    # get rows already containing a discrete value (so w/o ' à ' in strings) + assign to discrete_salary column \n",
    "    elif ' à 'not in row['salary_in_description'] and row['salary_in_description'] != 'NC':\n",
    "        print(index, row.salary_in_description)\n",
    "\n",
    "        data.at[index, 'discrete_salary'] = int(row['salary_in_description'])\n",
    "        \n",
    "        # deduce salary period from wage value for strings who didn't contain any mention about period \n",
    "        if int(row['salary_in_description']) > 20000:\n",
    "            data.at[index, 'og_salary_period'] = 'an'\n",
    "        else:\n",
    "            data.at[index, 'og_salary_period'] = 'mois' \n",
    "            \n",
    "\n",
    "    \n",
    "           \n",
    "#convert new salaries period to year / month / day\n",
    "data = pp.convert_salary_period(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c1305-c0fd-4492-95b5-35dd85a3bdfc",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "759bf536-4e9c-4ea0-90a4-c525262393b3",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df s= feszd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18dbcc7-bd56-44af-bcaf-5374795aec44",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "ae79c671-a708-4b6d-8840-4f7f6c2f16c3",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e1f3b-7045-41d4-821c-22cf3fad07f5",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "16eef583-4705-4bb8-8a8b-f584c8dc7101",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.remote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9ac783-3f66-4328-aea7-b3d1ffcd107d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "b8762f95-c927-4eb3-9436-ce5c93ad1a8e",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask = data.title.str.contains('remote')\n",
    "\n",
    "data.title.loc[mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcaa5a-47a1-4d0f-aa01-06bd42c76673",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "85c56eeb-2352-433a-a382-1ec9d4d8bd9a",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 2000\n",
    "pd.options.display.max_rows = 561\n",
    "\n",
    "mask = data['chars_around_(TT)'] != 'NC '\n",
    "\n",
    "data.loc[:, ['chars_around_(TT)']].loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a0d2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "175fe7e4-059f-48a3-9f6d-90598ff33ccc",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Text normalization of job descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee22857-aeb9-4ccb-9c77-2bbf9aaa3cbe",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "ebfd1d28-011d-427f-998d-bfb4278076d6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "There are several reasons to text normalization, one is to reduce noise in data. Job descriptions contains a lot of irrelevant information such as punctuation, numbers, special characters, and stop words. By normalizing the data, it'll be easier for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d743542",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "82a96f08-911f-42a9-9661-d3414d6c7056",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get common stopwords from nltk library in both french and english\n",
    "stop_words = list(set(stopwords.words('french')).union(set(stopwords.words('english'))))\n",
    "\n",
    "punct_mark = [\"•\"]\n",
    "\n",
    "apostrophes_stop_words = [\"d'\", \"c'\", \"j'\", \"m'\", \"n'\", \"s'\", \"t'\", \"l'\" ,\"qu'\", \n",
    "                          \"jusqu'\", \"lorsqu'\", \"puisqu'\", \"quoiqu'\", \"qu'il\", \n",
    "                          \"qu'on\", \"qu'un\", \"qu'une\", \"sans qu'\", \"étant qu'\",\n",
    "                         \"qu’\", \"jusqu’\", \"lorsqu’\", \"puisqu’\", \"quoiqu’\", \n",
    "                          \"qu’il\", \"qu’on\", \"qu’un\", \"qu’une\", \"sans qu’\", \"étant qu’\",\n",
    "                         \"d’\", \"c’\", \"j’\", \"m’\", \"n’\", \"s’\", \"t’\", \"d’un\", \"d’une\", \"c’est\"]\n",
    "additional_fr_stop_words = [\n",
    "    \"au\", \"aux\", \"avec\", \"ce\", \"ces\", \"dans\", \"de\", \"des\", \"du\", \"elle\",\n",
    "    \"en\", \"et\", \"eux\", \"il\", \"je\", \"la\", \"le\", \"leur\", \"lui\", \"ma\",\n",
    "    \"mais\", \"me\", \"même\", \"mes\", \"moi\", \"mon\", \"ne\", \"nos\", \"notre\",\n",
    "    \"nous\", \"on\", \"ou\", \"par\", \"pas\", \"pour\", \"qu\", \"que\", \"qui\", \"sa\",\n",
    "    \"se\", \"ses\", \"son\", \"sur\", \"ta\", \"te\", \"tes\", \"toi\", \"ton\", \"tu\",\n",
    "    \"un\", \"une\", \"vos\", \"votre\", \"vous\", \"c’\", \"d’\", \"j’\", \"l’\", \"à\", \"m’\",\n",
    "    \"n’\", \"s’\", \"t’\", \"y’\", \"été\", \"étée\", \"étées\", \"étés\", \"étant\", \"suis\",\n",
    "    \"es\", \"est\", \"sommes\", \"êtes\", \"sont\", \"serai\", \"seras\", \"sera\",\n",
    "    \"serons\", \"serez\", \"seront\", \"serais\", \"serait\", \"serions\", \"seriez\",\n",
    "    \"seraient\", \"étais\", \"était\", \"étions\", \"étiez\", \"étaient\", \"fus\",\n",
    "    \"fut\", \"fûmes\", \"fûtes\", \"furent\", \"sois\", \"soit\", \"soyons\", \"soyez\",\n",
    "    \"soient\", \"fusse\", \"fusses\", \"fût\", \"fussions\", \"fussiez\", \"fussent\",\n",
    "    \"ayant\", \"eu\", \"eue\", \"eues\", \"eus\", \"ai\", \"as\", \"avons\", \"avez\",\n",
    "    \"ont\", \"aurai\", \"auras\", \"aura\", \"aurons\", \"aurez\", \"auront\", \"aurais\",\n",
    "    \"aurait\", \"aurions\", \"auriez\", \"auraient\", \"avais\", \"avait\", \"avions\",\n",
    "    \"aviez\", \"avaient\", \"eut\", \"eûmes\", \"eûtes\", \"eurent\", \"aie\", \"aies\",\n",
    "    \"ait\", \"ayons\"]\n",
    "\n",
    "all_stop_words = set(stop_words + apostrophes_stop_words + additional_fr_stop_words + punct_mark)\n",
    "all_stop_words = list(all_stop_words)\n",
    "\n",
    "\n",
    "# Copy the original description column to a new column\n",
    "data['description_normalized'] = data['description'].copy().str.lower()\n",
    "\n",
    "pattern = r\"(\\b\\w+)[`'’](\\w+)?\\b\"\n",
    "replacement = r\"\\2\"\n",
    "\n",
    "data['description_normalized'] = data['description_normalized'].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "\n",
    "# remove stop words in data.description_normalized\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: ' '.join([word for word in text.split() if word not in all_stop_words]))\n",
    "    \n",
    "# remove tabulation and punctuation\n",
    "data['description_normalized'] = data['description_normalized'].str.replace('[^\\w\\s]',' ', regex=True)\n",
    "    \n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: text.translate(translator))\n",
    "    \n",
    "# lemmatization\n",
    "\n",
    "# Load the French language model\n",
    "nlp = spacy.load(\"fr_core_news_lg\")\n",
    "\n",
    "# Define a custom tokenizer rule for \"data\"\n",
    "#special_cases = [{ORTH: \"data\", 'LEMMA': \"data\"}]\n",
    "#nlp.tokenizer.add_special_case(\"data\", special_cases)\n",
    "\n",
    "def lemmatize_description_normalized(text):\n",
    "    # Apply Spacy's tokenizer to the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Lemmatize each token in the document, taking into account context\n",
    "    lemmas = [token.lemma_ if token.pos_ != 'VERB' and token.text != 'data' else token.text for token in doc]\n",
    "    \n",
    "    # Join the lemmas back into a string\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "data['description_normalized'] = data['description_normalized'].apply(lambda text: lemmatize_description_normalized(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696e0ed-b7b5-473c-b74e-c99cf4d7675a",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "da5328f3-417d-4aac-959c-dd8c79327e39",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "Now, that our job descriptions are normalized, we can analyze them more precisely. In order to that, i'll use : N-grams tokenization and Word cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5836e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "539e7bac-cd6d-42b7-a663-7e1176ca0f45",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Performing N-grams tokenization on normalized job descripions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9481ba-6f40-4254-8504-abb894e98084",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "1824c996-3836-434e-baee-3804b3ceadf5",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "By using N-grams tokenization, we can create tokens that represent not just individual words, but also sequences of words that appear together frequently. This is important because the context in which words are used can greatly affect their meaning.\n",
    "\n",
    "These tokens might capture more complex and nuanced information about the job description and may provide more accurate insights into the skills, qualifications, and requirements of the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513098b8",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "420a604e-288a-4dfa-a7f5-18d7ff6b8938",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "trigrams = []\n",
    "bigrams = []\n",
    "monograms = []\n",
    "\n",
    "def get_most_common_ngrams(data, n, k):\n",
    "    \"\"\"\n",
    "    This function takes a list of strings, an integer n (for the n-gram size), \n",
    "    and an integer k (for the number of most common n-grams to return). \n",
    "    It returns a list of the k most common n-grams in the input data.\n",
    "    \"\"\"\n",
    "    ngrams_list = []  # Create an empty list to store the n-grams\n",
    "    \n",
    "    for text in data:\n",
    "        \n",
    "        # Tokenize the text into words\n",
    "        words = nltk.word_tokenize(text)\n",
    "        # Generate the n-grams and add them to the list\n",
    "        ngrams_list.extend(list(ngrams(words, n)))\n",
    "        \n",
    "    # Count the occurrences of each n-gram using FreqDist\n",
    "    freq_dist = FreqDist(ngrams_list)\n",
    "    \n",
    "    # Get the k most common n-grams and return them\n",
    "    return freq_dist.most_common(k)\n",
    "\n",
    "trigrams = get_most_common_ngrams(data.description_normalized, 3, 1000)\n",
    "bigrams = get_most_common_ngrams(data.description_normalized, 2, 1000)\n",
    "monograms = get_most_common_ngrams(data.description_normalized, 1, 1000)\n",
    "\n",
    "    \n",
    "# Print the most common trigrams and bigrams\n",
    "print(\"The 5 most common trigrams are:\\n\")\n",
    "for trigram, count in trigrams[:5]:\n",
    "    print(' '.join(trigram), count)\n",
    "\n",
    "print(\"\\nThe 5 most common bigrams are:\\n\")\n",
    "for bigram, count in bigrams[:5]:\n",
    "    print(' '.join(bigram), count)\n",
    "    \n",
    "print(\"\\nThe 5 most common monograms are:\\n\")\n",
    "for monogram, count in monograms[:5]:\n",
    "    print(' '.join(monogram), count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e3d02c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "1172d996-06c7-4bc9-861f-9d9e9d8c8e9c",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Visualizing aggregated job descriptions w/ WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f87f2-444b-4423-87d3-42086b8e557c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "0d0a67b0-c07c-4f17-af2a-1ce4ba1487e9",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "WordCloud provide a quick and visually appealing way to identify the most common terms and phrases used in job descriptions.\n",
    "\n",
    "Below i divided my dataframe by search query to get query-related wordclouds. Grouping data by job titles might have be more precise but as there are over 1300 unique job titles, the quick & easy wordcloud solution would have become less interesting. So i used search_query as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cbc73",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "7fb63ac1-5051-494c-9b07-46a94ca2cc4a",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# groupby data by search query and aggregate descriptions\n",
    "agg_descriptions_by_query = data.groupby('search_query')['description_normalized'].sum().reset_index()\n",
    "\n",
    "\n",
    "for job in agg_descriptions_by_query.search_query.values:\n",
    "\n",
    "    # get aggregated description for each query\n",
    "    agg_text = agg_descriptions_by_query.loc[agg_descriptions_by_query.search_query == job, 'description_normalized'].values[0]\n",
    "\n",
    "    # Create a word cloud object and generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color=\"white\").generate(agg_text)\n",
    "    print(\"\\n***\",job,\"***\\n\")\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f526509",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "d9f06708-4e76-4d66-9e0b-2db385d28300",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "additionnal_stop_words = [\"client\", \"solution\", \"service\", \"donnée\", \"plus\", \"busines\", \"entreprise\", \"développement\", \"produit, \"team\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905215b1",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "d91f67f2-17c2-4ad0-9b5b-e316dc404755",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcef97",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "0d45ce8b-f016-4e2c-be1b-ce434036c4c6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## replace keywords for easier keyword extraction\n",
    "\n",
    "data.description_prepro.replace(['(?i)(Google Tag Manager)|\\b(GTM)\\b', \"(?i)\\b(GA4)\\b|\\b(GA)\\b\", '(?i)\\b(Google Colab)\\b', '\\b(GCP)\\b|(google cloud plateform)\\b'], value=['Google Tag Manager', 'Google Analytics', 'Google Colaboratory', 'Google Cloud Plateform'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)\\b(AWS)\\b|Amazon Web Services'], value=['Amazon Web Services'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)SKlearn|Scikit'], value=['ScikitLearn'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)data viz'], value=['data visualisation'] ,regex=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b649a",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "5ffe5888-cf8a-45a5-ac66-514acf5393af",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_visualization_tools = ['d3.js', 'ggplot2', 'tableau', 'matplotlib', 'seaborn', 'lumira',\n",
    "                            'spotfire', 'grafana', 'google data studio', 'looker', 'mode', 'metabase', 'kibana']\n",
    "\n",
    "\n",
    "\n",
    "tools = [\n",
    "'sas', 'Spark', 'BigML', 'D3.js', 'MATLAB', 'Excel', 'ggplot2', 'Tableau', 'Jupyter', \n",
    "'Matplotlib', 'NLTK', 'TensorFlow', 'Weka', 'Google Analytics', 'KNIME', \n",
    "'Flink', 'MongoDB', 'Minitab', 'Rapidminer', 'DataRobot', 'NLTK', 'Hadoop', 'Power BI', \n",
    "'QlikView', 'MySQL', 'Neo4j', 'HANA', 'Spotfire', 'SPSS', 'STATA', 'RiverLogic', \n",
    "'Lumira', 'Pig', 'Keras', 'NumPy', 'PyTorch', 'Seaborn', 'Wolfram Mathematica', \n",
    "'WebSockets', 'Algorithms.io', 'ForecastThis', 'BigQuery', 'GitHub', \n",
    "'Pycharm', 'Visual Studio Code', 'Linux', 'Windows', 'macOS', 'Google Colaboratory', \n",
    "'Google Cloud Plateform', 'Watson Studio', 'Amazon Web Services', \n",
    "'EC2', 'Amazon Elastic Compute Cloud', 'Microsoft Azure', \n",
    "'Nvidia Jetson Nano', 'Arduino', 'beam', 'semantria', 'trackur', 'cassandra', 'octoparse', \n",
    "'Content Grabber', 'OpenRefine', 'Google Fusion Table', 'scipy', 'pandas', 'NPM', 'Redshift', \n",
    "'Snowflake', 'Alteryx', 'Domino Data Lab', 'Kafka', 'Hbase', 'Elasticsearch', 'Maven', \n",
    "'Ansible', 'Gitlab', 'Jenkins', 'Bash', 'IntelliJ', 'MySQL', 'PostreSQL', 'Sonar', \n",
    "'Jira', 'OpenCV', 'TimescaleDB', 'Grafana', 'Google Sheet', 'Pig', 'Talend', 'MSBI',\n",
    "'SAP BO', 'Abode Campaign', 'Google Data Studio', 'Dataform', 'Looker',\n",
    "'Mode', 'Metabase', 'Power Query', 'Power Pivot', 'DataIku', 'MLFlow', 'DVC', 'Kibana', 'SageMaker',\n",
    "'Minio', 'S3', 'MQTT'\n",
    "]\n",
    "\n",
    "keywords_programming = [\n",
    "'sql', 'python', 'r', 'c', 'c#', 'javascript', 'js',  'java', 'scala', 'sas', 'matlab', \n",
    "'c++', 'c/c++', 'perl', 'go', 'typescript', 'bash', 'html', 'css', 'php', 'powershell', 'rust', \n",
    "'kotlin', 'ruby',  'dart', 'assembly', 'swift', 'vba', 'lua', 'groovy', 'delphi', 'objective-c', \n",
    "'haskell', 'elixir', 'julia', 'clojure', 'solidity', 'lisp', 'f#', 'fortran', 'erlang', 'apl', \n",
    "'cobol', 'ocaml', 'crystal', 'javascript/typescript', 'golang', 'nosql', 'mongodb', 't-sql', 'no-sql',\n",
    "'visual_basic', 'pascal', 'mongo', 'pl/sql',  'sass', 'vb.net', 'mssql', \n",
    "]\n",
    "\n",
    "keywords_libraries = [\n",
    "'scikit-learn', 'jupyter', 'theano', 'openCV', 'spark', 'nltk', 'mlpack', 'chainer', 'fann', 'shogun', \n",
    "'dlib', 'mxnet', 'node.js', 'vue', 'vue.js', 'keras', 'ember.js', 'jse/jee',\n",
    "]\n",
    "\n",
    "keywords_analyst_tools = [\n",
    "'excel', 'tableau',  'word', 'powerpoint', 'looker', 'powerbi', 'outlook', 'azure', 'jira', 'twilio',  'snowflake', \n",
    "'shell', 'linux', 'sas', 'sharepoint', 'mysql', 'visio', 'git', 'mssql', 'powerpoints', 'postgresql', 'spreadsheets',\n",
    "'seaborn', 'pandas', 'gdpr', 'spreadsheet', 'alteryx', 'github', 'postgres', 'ssis', 'numpy', 'power_bi', 'spss', 'ssrs', \n",
    "'microstrategy',  'cognos', 'dax', 'matplotlib', 'dplyr', 'tidyr', 'ggplot2', 'plotly', 'esquisse', 'rshiny', 'mlr',\n",
    "'docker', 'linux', 'jira',  'hadoop', 'airflow', 'redis', 'graphql', 'sap', 'tensorflow', 'node', 'asp.net', 'unix',\n",
    "'jquery', 'pyspark', 'pytorch', 'gitlab', 'selenium', 'splunk', 'bitbucket', 'qlik', 'terminal', 'atlassian', 'unix/linux',\n",
    "'linux/unix', 'ubuntu', 'nuix', 'datarobot',\n",
    "]\n",
    "\n",
    "keywords_cloud_tools = [\n",
    "'aws', 'azure', 'gcp', 'snowflake', 'redshift', 'bigquery', 'aurora',\n",
    "]\n",
    "\n",
    "keywords_general_tools = [\n",
    "'microsoft', 'slack', 'apache', 'ibm', 'html5', 'datadog', 'bloomberg',  'ajax', 'persicope', 'oracle', \n",
    "]\n",
    "\n",
    "keywords_general = [\n",
    "'coding', 'server', 'database', 'cloud', 'warehousing', 'scrum', 'devops', 'programming', 'saas', 'ci/cd', 'cicd', \n",
    "'ml', 'data_lake', 'frontend','front-end', 'back-end', 'backend', 'json', 'xml', 'ios', 'kanban', 'nlp',\n",
    "'iot', 'codebase', 'agile/scrum', 'agile', 'ai/ml', 'ai', 'paas', 'machine_learning', 'macros', 'iaas',\n",
    "'fullstack', 'dataops', 'scrum/agile', 'ssas', 'mlops', 'debug', 'etl', 'a/b', 'slack', 'erp', 'oop', \n",
    "'object-oriented', 'etl/elt', 'elt', 'dashboarding', 'big-data', 'twilio', 'ui/ux', 'ux/ui', 'vlookup', \n",
    "'crossover',  'data_lake', 'data_lakes', 'bi', 'pack office'\n",
    "]\n",
    "\n",
    "ml_tools = [\n",
    "'Tensorflow', 'Keras', 'PyTorch', 'ScikitLearn', 'sklearn', 'scikit'\n",
    "]\n",
    "\n",
    "big_data_tools = [\n",
    "'Hadoop', 'HDFS', 'YARN', 'Hive', 'Map', 'Reduce', 'Tez', 'Spark'\n",
    "]\n",
    "\n",
    "general_skills = [\n",
    "'informatique décisionnelle', 'Extraction', 'Nettoyage', 'transformation', 'ingestion', \n",
    "'data visualisation', 'modélisation', 'reporting', 'veille technologique', 'data mining',\n",
    "'kpi', 'computer vision'\n",
    "]\n",
    "\n",
    "soft_skills = [\n",
    "\"Esprit d'analyse\", \"Sens du service\", \"Rigueur\", \"communication\", 'positif', 'créatif', \n",
    "'pragmatique', 'souple', 'agile', \"Autonome\", 'Polyvalent', 'travailler en équipe', \"esprit d'équipe\"\n",
    "'esprit de synthèse', 'aisance relationnelle', 'force de proposition', \"capacité d'analyse\",\n",
    "\"anglais\", \"espagnol\", \"francais\"\n",
    "]\n",
    "\n",
    "coding_languages = [\n",
    "'SQL', 'Python', 'R', 'Julia', 'Scala', 'C++', 'Java', 'Javascript', 'Go'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Add up all keyword lists\n",
    "all_kw = tools + keywords_programming + keywords_libraries + keywords_analyst_tools + keywords_cloud_tools + keywords_general_tools + keywords_general + ml_tools + big_data_tools + general_skills + soft_skills + coding_languages\n",
    "\n",
    "# Lower case & remove duplicates\n",
    "all_kw = list(set([word.lower() for word in all_kw]))\n",
    "\n",
    "# keyword extraction\n",
    "data['extracted_kw'] = data['description'].apply(lambda x: [])\n",
    "\n",
    "for text in enumerate(data.description):\n",
    "\n",
    "    kw_list = [word for word in all_kw if word in text[1]]\n",
    "    data.loc[text[0], \"extracted_kw\"].extend(kw_list)\n",
    "\n",
    "# number of keywords extracted\n",
    "data['extracted_kw_len'] = data['extracted_kw'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9f79ba-8d64-4bd1-9d0e-4b011b5b7390",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "05ad6b2c-5882-4068-beda-39c4fcad0d5d",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_useless_col(data):\n",
    "    \n",
    "    data.drop(['Unnamed: 0', 'extensions','level_0', 'index', 'Unnamed: 0.1', 'job_highlights', 'related_links', 'thumbnail',\n",
    "       'extensions', 'job_id', 'Unnamed: 0', 'chars_around_salaire', 'salary_in_description'], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = drop_useless_col(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8890d-2ea6-4b07-8d07-04c78d53820c",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "010e526e-9ea5-4c89-8493-e362838b0949",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## Encoding"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "gg_jobs_env",
   "language": "python",
   "name": "gg_jobs_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
