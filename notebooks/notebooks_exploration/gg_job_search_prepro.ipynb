{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef84455",
   "metadata": {},
   "source": [
    "# Scraping bullshit jobs ... a long way to hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9487d252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# library for plotting data over maps\n",
    "import shapefile as shp\n",
    "\n",
    "# libraries used for tokenization\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
    "\n",
    "# Libraries used to remove similar job description based on cosine similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# libraries used for text normalization\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd10655",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ced546c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gg_job_search_all_RAW.csv')\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e1272",
   "metadata": {},
   "source": [
    "## General overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "0bcbba87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6045 entries, 0 to 6044\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     6045 non-null   int64 \n",
      " 1   title          6045 non-null   object\n",
      " 2   company_name   6045 non-null   object\n",
      " 3   location       6041 non-null   object\n",
      " 4   via            6045 non-null   object\n",
      " 5   description    5985 non-null   object\n",
      " 6   related_links  5985 non-null   object\n",
      " 7   thumbnail      4574 non-null   object\n",
      " 8   extensions     6045 non-null   object\n",
      " 9   job_id         5818 non-null   object\n",
      " 10  posted_at      6045 non-null   object\n",
      " 11  schedule_type  5981 non-null   object\n",
      " 12  date_time      6045 non-null   object\n",
      " 13  search_query   6045 non-null   object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 661.3+ KB\n",
      "None \n",
      "\n",
      "Unnamed: 0         10\n",
      "title            1202\n",
      "company_name     1038\n",
      "location          312\n",
      "via                86\n",
      "description      1931\n",
      "related_links    2584\n",
      "thumbnail         682\n",
      "extensions        476\n",
      "job_id           2286\n",
      "posted_at          26\n",
      "schedule_type       4\n",
      "date_time         411\n",
      "search_query        3\n",
      "dtype: int64 \n",
      "\n",
      "data scientist    2084\n",
      "data analyst      2042\n",
      "data engineer     1919\n",
      "Name: search_query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.info(), '\\n')\n",
    "print(data.nunique(), '\\n')\n",
    "print(data.search_query.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01234",
   "metadata": {},
   "source": [
    "## Null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "b7e83723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location            4\n",
       "description        60\n",
       "related_links      60\n",
       "thumbnail        1471\n",
       "job_id            227\n",
       "schedule_type      64\n",
       "dtype: int64"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47929c2",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "583af771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates : 2090\n",
      "Number of duplicates (based on identical job_id) : 3758 \n",
      " ======================================================================\n",
      "Search query results :\n",
      " data scientist    1078\n",
      "data analyst       720\n",
      "data engineer      489\n",
      "Name: search_query, dtype: int64 \n",
      " ======================================================================\n",
      "Final number of rows :  2287\n",
      "Final number of columns :  15\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates :',data[data.duplicated()].shape[0])\n",
    "print('Number of duplicates (based on identical job_id) :', data[data.duplicated(['job_id'])].shape[0],'\\n', '='*70) \n",
    "\n",
    "# removing duplicates based on job_id\n",
    "data = data.drop_duplicates(['job_id'])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e020",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Purely based job id, we can skim off a lots of duplicates. Nonetheless i still found a few of identical / near-identical job descriptions, maybe because [sometimes recruiters or companies post the same advert for a job which results in duplicate data.](https://medium.com/analytics-vidhya/data-science-job-search-using-nlp-and-lda-in-python-12ecbfac79f9)\n",
    "\n",
    "To get rid off these ones, i could use cosine similatity between job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "e082a5f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates (based on cosine similarity) :  783\n",
      "Final number of rows :  1504\n",
      "Final number of columns :  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85948/683578045.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n"
     ]
    }
   ],
   "source": [
    "### removing duplicates based on cosine similarity between \n",
    "## job descriptions (code from Thomas Caffrey, see link above)\n",
    "\n",
    "# Defining our collection of job description texts to tokenize\n",
    "data.description = data.description.fillna('')\n",
    "corpus = data['description']\n",
    "\n",
    "# instantiate CountVectorizer object\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Fit_transform to vectorize each job description (map terms to feature indices)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarities and put it in dataframe\n",
    "cos_df = pd.DataFrame(cosine_similarity(X_train_counts))\n",
    "\n",
    "\n",
    "## reshape dataframe for easier comparison\n",
    "\n",
    "# get arrays of rows indices and col indices from col_df.shape\n",
    "i, j = np.indices(cos_df.shape).reshape(2,-1)\n",
    "\n",
    "# reshape values to get a 1D array \n",
    "cos_values = cos_df.values.reshape(-1)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'i': i, 'j': j, 'sim':cos_values})\n",
    "\n",
    "# get cosine similarity values only above 0.98 \n",
    "cos_rem = cos_sim_df[(cos_sim_df['sim'] > 0.98) & (i!=j)]\n",
    "\n",
    "# Method to remove duplicates but keep first instance:\n",
    "# Trying to drop duplicates on i and j columns won't work as the row numbers of duplicates are either in i or j not both.\n",
    "# Setting another column that combines the i & j values ensures that duplicates can be dropped.\n",
    "\n",
    "cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n",
    "drop_rows = np.unique(cos_rem.drop_duplicates('i*j')['i'].values)\n",
    "\n",
    "# keep only non-duplicated job postings\n",
    "data = data[~data.index.isin(drop_rows)] \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Number of duplicates (based on cosine similarity) : ' ,drop_rows.shape[0])\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599a6b",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4d501f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ['il y a 13 heures', 'À plein temps']\n",
       "1       ['il y a 13 heures', 'À plein temps']\n",
       "2       ['il y a 13 heures', 'À plein temps']\n",
       "3       ['il y a 21 heures', 'À plein temps']\n",
       "4       ['il y a 16 heures', 'À plein temps']\n",
       "                        ...                  \n",
       "1499    ['il y a 24 heures', 'À plein temps']\n",
       "1500            ['il y a 15 heures', 'Stage']\n",
       "1501     ['il y a 3 heures', 'À plein temps']\n",
       "1502    ['il y a 22 heures', 'À plein temps']\n",
       "1503            ['il y a 15 heures', 'Stage']\n",
       "Name: extensions, Length: 1504, dtype: object"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d1851",
   "metadata": {},
   "source": [
    "## Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "aaef9dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(data.title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242f91",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Based on this sample of job titles, we could create : \n",
    "    \n",
    "    * Contract_type (full-time, part-time ...) \n",
    "    * Contract_status (CDI, CDD, work-study, internship ...)\n",
    "    * Duration of Contract (Duration/Undetermined)\n",
    "    * Experience ( Senior, Junior ...)\n",
    "    * Data Specialization (Supply chain, Marketing, Clinical ...)\n",
    "    * Multiple titles (Analyst/Scientist, Scientist/ML Engineer, Manager/Analyst ...)\n",
    "    * Specific expertise asked for (Python, Power BI ...)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1679e1",
   "metadata": {},
   "source": [
    "## Explore company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3400f598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique companies : 942\n"
     ]
    }
   ],
   "source": [
    "print('number of unique companies :', data.company_name.nunique())\n",
    "#data.company_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1583ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 50000\n",
    "#data.loc[data['company_name'] == 'Unspecified', ['description']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05598b",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "When ***Unspecified***, companies name can be found in description column.\n",
    "\n",
    "Possible new columns :\n",
    "\n",
    "    * Group/Holding (Y/N/NC)\n",
    "    * Interim company (Y/N/NC)\n",
    "\n",
    "Based on the number of job posting per company we could potentially infer about : **size of company ? / Amount of data to work on** / \n",
    "\n",
    "Adding a time variable and much more data, the number of similar / identical job postings for the same company could maybe give insights on the **company's turnover rate / company's growth / magnitude of need-urgency to hire** ...  \n",
    "\n",
    "It seems like extracting additional informations without more context will be difficult. Having access to each company's structure information we could create :\n",
    "\n",
    "    * Size of company\n",
    "    * Industry\n",
    "    * Public / Private\n",
    "    \n",
    "We'll see if can extract more related informations in the following columns. Otherwise, we could try to scrap **Glassdoor databases** (or similar) to get those informations.remains to be seen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0ce67",
   "metadata": {},
   "source": [
    "## Explore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "f01afb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique locations :  236\n"
     ]
    }
   ],
   "source": [
    "print('number of unique locations : ', data.location.nunique())\n",
    "#data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "29d39825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "#data.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cabad",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "We could create a map of th repartition of job posting based on location provided.\n",
    "\n",
    "Some companies don't provide precise location *(ex : location = FRANCE)* and the information is not available in description column either. Further investigations will be needed for these companies, perhaps in conjunction with other databases *(GLASSDOOR / SIRENE databases for instance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2d818",
   "metadata": {},
   "source": [
    "## Explore via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "79a48ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique job plateforms :  76\n"
     ]
    }
   ],
   "source": [
    "print('number of unique job plateforms : ', data.via.nunique())\n",
    "# data.via.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b57edd",
   "metadata": {},
   "source": [
    "## Explore description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "acda0f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1504.000000\n",
      "mean      2949.141622\n",
      "std       1639.399405\n",
      "min          0.000000\n",
      "25%       1793.500000\n",
      "50%       2678.000000\n",
      "75%       3838.500000\n",
      "max      14170.000000\n",
      "Name: description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.description.str.len().describe())\n",
    "\n",
    "plt.hist(data.description.str.len(), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf1f6",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "\n",
    "We could create :\n",
    "\n",
    "    * Lenght of description (dunno what informations it could provide yet)\n",
    "    * Toold required (Excel, Google Tag Manager ...)\n",
    "    * Coding languages required (R, Python, SQL ...)\n",
    "    * Skills required (reporting, data visualization)\n",
    "    * Required experience\n",
    "    * Duration of contract\n",
    "    * Avantages (ticket resto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5b4965fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_employment_type(row):\n",
    "    employment_type_dict = {\n",
    "        'CDI': ['cdi',],\n",
    "        'internship': ['stage', 'internship', 'stagiaire'],\n",
    "        'apprenticeship': ['alternant', 'apprenti', 'professionnalisation', 'alternance'],\n",
    "        'CDD': ['cdd'],\n",
    "        'freelance': ['freelance', 'prestataire', 'freelancer'],\n",
    "        'interim': ['interim'],\n",
    "        'consultant': ['consultant'],\n",
    "    }\n",
    "    for employment_type, keywords in employment_type_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return employment_type\n",
    "    return np.NaN\n",
    "\n",
    "def create_seniority_level(row):\n",
    "    seniority_level_dict = {\n",
    "        'senior': ['senior', 'advanced', 'avance', 'sr', 'experimente'],\n",
    "        'mid-level': ['confirme'],\n",
    "        'junior': ['junior', 'debutant', 'jr', 'entre-level'],\n",
    "    }\n",
    "    for seniority_level, keywords in seniority_level_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U): #or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return seniority_level\n",
    "    return np.NaN\n",
    "\n",
    "def create_executive_title(row):\n",
    "    executive_title_dict = {\n",
    "        'lead': ['lead'],\n",
    "        'Director': ['director', 'directeur'],\n",
    "        'Manager': ['manager', 'project manager', 'chef de projet'],\n",
    "        'Assistant': ['assistant'],\n",
    "        'Chief': ['chief'],\n",
    "        'Head': ['head'],\n",
    "        'Supervisor': ['supervisor'],\n",
    "    }\n",
    "    for executive_title, keywords in executive_title_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U):\n",
    "                return executive_title\n",
    "    return np.NaN\n",
    "\n",
    "def create_job_specialization(row):\n",
    "    job_specialization_dict = {\n",
    "        'career': ['career'],\n",
    "        'web': ['web'],\n",
    "        'media': ['media'],\n",
    "        'online': ['online'],\n",
    "        'marketing': ['marketing', 'market'],\n",
    "        'crm': ['crm'],\n",
    "        'assurance': ['indemnisation'],\n",
    "        'immobilier': ['immobilier'],\n",
    "        'product': ['product'],\n",
    "        'people': ['people', 'hr', 'human', 'workforce'],\n",
    "        'informatique': ['informatique'],\n",
    "        'supply_chain': ['supply'],\n",
    "        'logistique': ['logistique'],\n",
    "        'medical': ['medical', 'clinique', 'sante'],\n",
    "        'finance': ['finance'],\n",
    "        'recherche': ['recherche'],\n",
    "        'tv': ['tv'],\n",
    "        'game': ['game'],\n",
    "        'geo': ['geo'],\n",
    "    }\n",
    "    for job_specialization, keywords in job_specialization_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword,unidecode(row['title']), re.I|re.U):\n",
    "                return job_specialization\n",
    "    return np.NaN\n",
    "\n",
    "def create_remote(row):\n",
    "    remote_dict = {\n",
    "        'Y': ['remote', 'teletravail hybride', 'teletravail complet', 'jour de teletravail',\n",
    "                   'jours de teletravail', 'teletravail partiel', 'distanciel', 'teletravail']        \n",
    "    }\n",
    "    for remote, keywords in remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "def create_full_partial_remote(row):\n",
    "    full_partial_remote_dict = {\n",
    "        'full': ['full remote', 'teletravail complet'],\n",
    "        'partial_remote': ['teletravail de','teletravail hybride', 'jours de teletravail', 'teletravail partiel', 'jour de teletravail',]\n",
    "    }\n",
    "    for remote, keywords in full_partial_remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "# creation of holding feature Y/N\n",
    "\n",
    "conditions = [\n",
    "    \n",
    "    (data['company_name'].str.contains('Groupe(?i)|Holdings*(?i)')),\n",
    "    (data['description'].str.contains('ESN(?i)')).astype(bool)\n",
    "    \n",
    "]\n",
    "\n",
    "choices = ['Holding', 'ESN']\n",
    "\n",
    "data['holding'] = np.select(conditions, choices, default='NC')\n",
    "\n",
    "data['holding'].value_counts()\n",
    "\n",
    "\n",
    "#data = data.applymap(lambda x: unidecode(x) if isinstance(x, str) else x) # used for accent-insensitive search, got replaced directly in functions (see above)\n",
    "# lower every string of dataframe for easier search\n",
    "data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "data['seniority_level'] = data.apply(create_seniority_level, axis=1) \n",
    "data['employment_type'] = data.apply(create_employment_type, axis=1) \n",
    "data['job_specialization'] = data.apply(create_job_specialization, axis=1) \n",
    "data['remote'] = data.apply(create_remote, axis=1) \n",
    "data['full_partial_remote'] = data.apply(create_full_partial_remote, axis=1)\n",
    "data['via'] = data.via.apply(prepro_via)\n",
    "#data['salary'] = data['extensions'].apply(prepro_extension)\n",
    "#data['salary'] = data['salary'].apply(prepro_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3bba53d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepro_salary(data):\n",
    "    \n",
    "    # define salary column from extensions columns\n",
    "    data['salary'] = pd.Series(data.extensions.str.split(', ', expand=True)[1])\n",
    "    \n",
    "    # Get rid of non-salary values >>> np.NaN\n",
    "    data['salary'].replace([\"'à plein temps']\", \"'à temps partiel']\", \"'stage']\", \"'prestataire']\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # clean salary strings\n",
    "    for salary_str in data.salary_str:\n",
    "        if 'par an' in salary_str:\n",
    "            \n",
    "            \n",
    "    # strip par mois / par an / par jour\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = prepro_salary(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8a90acae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100\\xa0€ à 125\\xa0€ par an'                          4\n",
       "'35\\xa0k\\xa0€ à 45\\xa0k\\xa0€ par an'                  3\n",
       "'2\\u202f034,10\\xa0€ à 2\\u202f627,16\\xa0€ par mois'    2\n",
       "'125\\xa0€ à 150\\xa0€ par an'                          2\n",
       "'39\\u202f690\\xa0$us à 73\\u202f710\\xa0$us par an'      2\n",
       "'55\\xa0k\\xa0€ à 60\\xa0k\\xa0€ par an'                  2\n",
       "'34\\xa0k\\xa0€ à 50\\xa0k\\xa0€ par an'                  2\n",
       "'45\\xa0k\\xa0€ à 55\\xa0k\\xa0€ par an'                  2\n",
       "'55\\xa0k\\xa0€ à 65\\xa0k\\xa0€ par an'                  2\n",
       "'50\\xa0k\\xa0€ à 60\\xa0k\\xa0€ par an'                  2\n",
       "'40\\xa0k\\xa0€ à 60\\xa0k\\xa0€ par an'                  2\n",
       "'38\\xa0k\\xa0€ à 42\\xa0k\\xa0€ par an'                  2\n",
       "'50\\xa0k\\xa0€ à 65\\xa0k\\xa0€ par an'                  2\n",
       "'30\\xa0k\\xa0€ à 35\\xa0k\\xa0€ par an'                  1\n",
       "'35\\xa0k\\xa0€ à 42\\xa0k\\xa0€ par an']                 1\n",
       "'1,2\\xa0k\\xa0€ par mois'                              1\n",
       "'2\\xa0k\\xa0€ à 2,5\\xa0k\\xa0€ par mois'                1\n",
       "'22\\u202f480\\xa0€ à 44\\u202f832\\xa0€ par an'          1\n",
       "'20\\xa0k\\xa0€ à 22\\xa0k\\xa0€ par an'                  1\n",
       "'52\\xa0k\\xa0€ à 60\\xa0k\\xa0€ par an'                  1\n",
       "'60\\xa0k\\xa0€ à 65\\xa0k\\xa0€ par an'                  1\n",
       "'92\\u202f350\\xa0$us à 130\\u202f000\\xa0$us par an'     1\n",
       "'95\\xa0k\\xa0$us à 171\\xa0k\\xa0$us par an'             1\n",
       "'80\\xa0k\\xa0€ à 90\\xa0k\\xa0€ par an'                  1\n",
       "'62\\u202f370\\xa0$us à 115\\u202f830\\xa0$us par an'     1\n",
       "'77,3\\xa0k\\xa0$us à 134\\xa0k\\xa0$us par an'           1\n",
       "'41\\xa0k\\xa0€ à 45\\xa0k\\xa0€ par an'                  1\n",
       "'37\\xa0k\\xa0€ à 45\\xa0k\\xa0€ par an'                  1\n",
       "'40\\xa0k\\xa0€ à 42\\xa0k\\xa0€ par an'                  1\n",
       "'500\\xa0€ par jour'                                   1\n",
       "'40\\xa0k\\xa0€ à 48\\xa0k\\xa0€ par an'                  1\n",
       "'45\\xa0€ à 70\\xa0€ par an'                            1\n",
       "'26\\xa0k\\xa0€ à 32\\xa0k\\xa0€ par an'                  1\n",
       "'45\\xa0k\\xa0€ à 65\\xa0k\\xa0€ par an'                  1\n",
       "'20\\u202f512\\xa0€ par an'                             1\n",
       "'40\\xa0k\\xa0€ à 45\\xa0k\\xa0€ par an'                  1\n",
       "'1,8\\xa0k\\xa0€ à 2,5\\xa0k\\xa0€ par mois'              1\n",
       "'2\\u202f104\\xa0€ par mois']                           1\n",
       "'50\\xa0k\\xa0€ à 55\\xa0k\\xa0€ par an'                  1\n",
       "'40\\xa0k\\xa0€ à 65\\xa0k\\xa0€ par an'                  1\n",
       "'42\\u202f028\\xa0$us à 60\\u202f000\\xa0$us par an'      1\n",
       "'65\\xa0k\\xa0€ à 85\\xa0k\\xa0€ par an'                  1\n",
       "'45\\xa0k\\xa0€ par an'                                 1\n",
       "'1,5\\xa0k\\xa0€ à 2,5\\xa0k\\xa0€ par mois'              1\n",
       "'35\\xa0k\\xa0€ à 55\\xa0k\\xa0€ par an'                  1\n",
       "'400\\xa0€ à 600\\xa0€ par jour'                        1\n",
       "'33,6\\xa0k\\xa0€ à 50\\xa0k\\xa0€ par an'                1\n",
       "'45\\xa0k\\xa0€ à 50\\xa0k\\xa0€ par an'                  1\n",
       "'42\\xa0k\\xa0€ à 48\\xa0k\\xa0€ par an'                  1\n",
       "'67\\xa0k\\xa0€ par an'                                 1\n",
       "'48\\xa0k\\xa0€ à 65\\xa0k\\xa0€ par an'                  1\n",
       "'70\\xa0k\\xa0€ à 90\\xa0k\\xa0€ par an'                  1\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 53\n",
    "data.salary.value_counts().head(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "4c78b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'400\\xa0€ à 600\\xa0€ par jour'\n",
      "'500\\xa0€ par jour'\n"
     ]
    }
   ],
   "source": [
    "#def convert_to_int(salary_str)\n",
    "\n",
    "for salary_str in data.salary:\n",
    "    if type(salary_str) == str:\n",
    "        if 'par an' in salary_str:\n",
    "            salary_str = salary_str.replace(r\"\\\\xa0k\\\\xa0\", '000')\n",
    "            salary_str = salary_str.replace(r\"\\\\xa0k\", '000')\n",
    "        elif ('par mois' in salary_str) & (',' in salary_str):\n",
    "            salary_str = salary_str.replace(r\"\\\\xa0k\\\\xa0\", '00')\n",
    "            salary_str = salary_str.replace(r\"\\\\xa0k\", '00')\n",
    "        elif 'par jour' in salary_str:\n",
    "            salary_str = salary_str.replace(r\"\\\\xa0k\", '')\n",
    "    \n",
    "            print(salary_str)        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e282aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 5000\n",
    "pd.options.display.max_rows = 500\n",
    "\n",
    "data.salary = data.salary.astype(str)\n",
    "\n",
    "data['salary'] = data.salary.replace(r\"\\\\xa0k\\\\xa0\", 'k', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"\\\\u...f...\\\\xa0\", 'k', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"\\\\u...f...,\", ',', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"\\\\xa0\", 'k', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"]\", '', regex=True)\n",
    "data['salary'] = data.salary.replace(r\"'\", '', regex=True)\n",
    "\n",
    "#data['salary'] = data.salary.replace(r\"[,0-9]k\", \"00\")\n",
    "#data['salary'] = data.salary.replace(\"k€\", \"000\")\n",
    "import requests\n",
    "\n",
    "\n",
    "# Function to get exchange rate from USD to EUR\n",
    "def get_exchange_rate():\n",
    "    response = requests.get('https://openexchangerates.org/api/latest.json?app_id=4820391575d04bdd8d07b7e15fb0a463')\n",
    "    data = response.json()\n",
    "    return data['rates']['EUR'] / data['rates']['USD']\n",
    "\n",
    "# Create DataFrame with columns for lower and upper bounds and currency\n",
    "df = pd.DataFrame(columns=['lower_bound', 'upper_bound', 'currency'])\n",
    "df['lower_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['upper_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['currency'] = pd.Series([np.nan] * len(df))\n",
    "\n",
    "# Loop through series and extract salary information\n",
    "for row in series:\n",
    "    # Extract lower and upper bounds and currency\n",
    "    match = re.search(r'(\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) à (\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) (?:par mois|par an|par jour)?', row)\n",
    "    if match:\n",
    "        lower_bound = float(match.group(1).replace(',', '.')) * 1000 if match.group(2) == 'k' else float(match.group(1))\n",
    "        upper_bound = float(match.group(3).replace(',', '.')) * 1000 if match.group(4) == 'k' else float(match.group(3))\n",
    "        if match.group(2) == '$US':\n",
    "            exchange_rate = get_exchange_rate()\n",
    "            lower_bound *= exchange_rate\n",
    "            upper_bound *= exchange_rate\n",
    "        if 'par mois' in row:\n",
    "            lower_bound *= 12\n",
    "            upper_bound *= 12\n",
    "        currency = 'EUR' if match.group(2) in ['k', '€'] else 'USD'\n",
    "        # Assign values to DataFrame\n",
    "        df.loc[df.index.max() + 1] = [lower_bound, upper_bound, currency]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for indx in data.salary:\n",
    "    \n",
    "    # store salary_min and salary_max in column for further analysis\n",
    "    data.loc[i, 'lower_upper_salary_bound'].append((salary_min, salary_max))\n",
    "    # store mean salary in column for further analysis\n",
    "    data.loc[i, 'mean_salary_per_year'] = average  \n",
    "        \n",
    "        \n",
    "    # Remove any non-numeric characters\n",
    "    salary_str = re.sub('[^0-9]+', '', salary_str)\n",
    "    \n",
    "    # Split the salary range\n",
    "    salary_range = salary_str.split('à')\n",
    "    \n",
    "    # Convert the salaries to a common unit\n",
    "    # For example, if salaries are in euros per year:\n",
    "    salary_range = [int(s) for s in salary_range]\n",
    "    \n",
    "    # Calculate the average salary\n",
    "    avg_salary = sum(salary_range) / 2\n",
    "    \n",
    "    return avg_salary\n",
    "\n",
    "\n",
    "for \n",
    "\n",
    "\n",
    "salary_ranges = [\"35k€ to 45k€ per year\", \"40k€ to 50k€ per year\", \"\", \"50k€ to 60k€ per year\"]\n",
    "valid_salaries = []\n",
    "mean_valid_salaries = []\n",
    "\n",
    "for i, string in enumerate(data.salary)\n",
    "        if salary_range:\n",
    "            salary_range = salary_range.replace(\"k€\", \"000\")\n",
    "            \n",
    "            salary_range = salary_range.split('per year')[0]\n",
    "            \n",
    "            salary_min, salary_max = map(int, salary_range.split(\" to \"))\n",
    "            \n",
    "            valid_salaries.append((salary_min, salary_max))\n",
    "            \n",
    "            average = sum([salary_min, salary_max])/ len([salary_min, salary_max])\n",
    "            \n",
    "            data.loc[i, 'mean_salary_per_year'] = average\n",
    "\n",
    "data.mean_salary_per_year.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the exchange rate\n",
    "usd_to_eur = 0.82\n",
    "usd_to_eur_1000 = 900\n",
    "\n",
    "# Define a function to convert salary to euros\n",
    "def convert_to_eur(salary_str):\n",
    "    if \"k$US\" in salary_str:\n",
    "        salary = int(float(salary_str.replace(\"k$US\", \"\").replace(\",\", \"\")) * 1000 / usd_to_eur_1000)\n",
    "    elif \"$US\" in salary_str:\n",
    "        salary = int(float(salary_str.replace(\"$US\", \"\").replace(\",\", \"\")) / usd_to_eur)\n",
    "    elif \"k€\" in salary_str:\n",
    "        salary = int(float(salary_str.replace(\"k€\", \"\").replace(\",\", \"\")) * 1000)\n",
    "    else:\n",
    "        salary = int(float(salary_str.replace(\"€\", \"\").replace(\",\", \"\")))\n",
    "    return salary\n",
    "\n",
    "# Define a function to parse the salary string\n",
    "def parse_salary(salary_str):\n",
    "    if \"par jour\" in salary_str:\n",
    "        match = re.search(r\"(\\d+)k€ à (\\d+)k€ par jour\", salary_str)\n",
    "        if match:\n",
    "            min_salary = int(match.group(1)) * 1000\n",
    "            max_salary = int(match.group(2)) * 1000\n",
    "    elif \"par mois\" in salary_str:\n",
    "        match = re.search(r\"(\\d+(?:,\\d+)?)k€ à (\\d+(?:,\\d+)?)k€ par mois\", salary_str)\n",
    "        if match:\n",
    "            min_salary = int(float(match.group(1).replace(\",\", \"\")) * 1000)\n",
    "            max_salary = int(float(match.group(2).replace(\",\", \"\")) * 1000)\n",
    "    elif \"par an\" in salary_str:\n",
    "        match = re.search(r\"(\\d+(?:,\\d+)?)k€ à (\\d+(?:,\\d+)?)k€ par an\", salary_str)\n",
    "        if match:\n",
    "            min_salary = int(float(match.group(1).replace(\",\", \"\")) * 1000)\n",
    "            max_salary = int(float(match.group(2).replace(\",\", \"\")) * 1000)\n",
    "        else:\n",
    "            match = re.search(r\"(\\d+(?:,\\d+)?)k€ par an\", salary_str)\n",
    "            if match:\n",
    "                min_salary = int(float(match.group(1).replace(\",\", \"\")) * 1000)\n",
    "                max_salary = min_salary\n",
    "            else:\n",
    "                match = re.search(r\"(\\d+(?:,\\d+)?)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define conversion factors for different salary units\n",
    "CONVERSION_FACTORS = {\"par an\": 1, \"par mois\": 12, \"par jour\": 250}\n",
    "\n",
    "# Define a function to process the salary string\n",
    "def process_salary(salary_str):\n",
    "    # Check if the salary is missing or not applicable\n",
    "    if (salary_str == \"NC\") or (salary_str == \"None\"):\n",
    "        return pd.Series({\"average_salary\": None, \"salary_unit\": None})\n",
    "    \n",
    "    # Extract the salary range and units\n",
    "    salary_str = salary_str.strip()\n",
    "    salary_range, salary_unit = salary_str.split(\" par \")\n",
    "    \n",
    "    # Extract the minimum and maximum salaries\n",
    "    if \"à\" in salary_range:\n",
    "        salary_min, salary_max = salary_range.split(\" à \")\n",
    "    else:\n",
    "        salary_min = salary_max = salary_range\n",
    "    \n",
    "    # Clean up the salary values\n",
    "    salary_min = salary_min.replace(\",\", \".\").replace(\"k€\", \"000\").replace(\"k$\", \"000\")\n",
    "    salary_max = salary_max.replace(\",\", \".\").replace(\"k€\", \"000\").replace(\"k$\", \"000\")\n",
    "    \n",
    "    # Convert the salary values to numbers\n",
    "    salary_min = float(salary_min)\n",
    "    salary_max = float(salary_max)\n",
    "    \n",
    "    # Calculate the average salary\n",
    "    average_salary = (salary_min + salary_max) / 2\n",
    "    \n",
    "    # Convert the salary to a consistent unit\n",
    "    if salary_unit in CONVERSION_FACTORS:\n",
    "        conversion_factor = CONVERSION_FACTORS[salary_unit]\n",
    "        average_salary *= conversion_factor\n",
    "    \n",
    "    return pd.Series({\"average_salary\": average_salary, \"salary_unit\": salary_unit})\n",
    "\n",
    "# Define the series of salaries\n",
    "salaries = pd.Series({\n",
    "    7: \"40k€ à 48k€ par an\",\n",
    "    13: \"48k€ à 65k€ par an\",\n",
    "    51: \"20k€ par an\",\n",
    "    127: \"40k€ à 45k€ par an\",\n",
    "    128: \"1,8k€ à 2,5k€ par mois\",\n",
    "    130: \"2k€ par mois\",\n",
    "    151: \"50k€ à 65k€ par an\",\n",
    "    157: \"50k€ à 55k€ par an\",\n",
    "    163: \"40k€ à 65k€ par an\",\n",
    "    267: \"42k$US à 60k$US par an\",\n",
    "    283: \"125k€ à 150k€ par an\",\n",
    "    290: \"50k€ à 60k€ par an\",\n",
    "    292: \"50k€ à 60k€ par an\",\n",
    "    302: \"65k€ à 85k€ par an\",\n",
    "    399: \"45k€ à 55k€ par an\",\n",
    "    400: \"34k€ à 50k€ par an\",\n",
    "    428: \"34k€ à 50k€ par an\",\n",
    "    435: \"2,10k€ à 2,16k€ par mois\",\n",
    "    470:\n",
    "\n",
    "    \n",
    "\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get exchange rate from USD to EUR\n",
    "def get_exchange_rate():\n",
    "    response = requests.get('https://openexchangerates.org/api/latest.json?app_id=<your_app_id>')\n",
    "    data = response.json()\n",
    "    return data['rates']['EUR'] / data['rates']['USD']\n",
    "\n",
    "# Create DataFrame with columns for lower and upper bounds and currency\n",
    "df = pd.DataFrame(columns=['lower_bound', 'upper_bound', 'currency'])\n",
    "df['lower_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['upper_bound'] = pd.Series([np.nan] * len(df))\n",
    "df['currency'] = pd.Series([np.nan] * len(df))\n",
    "\n",
    "# Loop through series and extract salary information\n",
    "for row in series:\n",
    "    # Extract lower and upper bounds and currency\n",
    "    match = re.search(r'(\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) à (\\d+(?:,\\d+)?(?:\\.\\d+)?)(k|\\$US|€) (?:par mois|par an)?', row)\n",
    "    if match:\n",
    "        lower_bound = float(match.group(1).replace(',', '.')) * 1000 if match.group(2) == 'k' else float(match.group(1))\n",
    "        upper_bound = float(match.group(3).replace(',', '.')) * 1000 if match.group(4) == 'k' else float(match.group(3))\n",
    "        if match.group(2) == '$US':\n",
    "            exchange_rate = get_exchange_rate()\n",
    "            lower_bound *= exchange_rate\n",
    "            upper_bound *= exchange_rate\n",
    "        if 'par mois' in row:\n",
    "            lower_bound *= 12\n",
    "            upper_bound *= 12\n",
    "        currency = 'EUR' if match.group(2) in ['k', '€'] else 'USD'\n",
    "        # Assign values to DataFrame\n",
    "        df.loc[df.index.max() + 1] = [lower_bound, upper_bound, currency]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_col(data):\n",
    "    \n",
    "    data.drop(['extensions'], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d96642d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [301]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fds \u001b[38;5;241m=\u001b[39m \u001b[43mfz\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fz' is not defined"
     ]
    }
   ],
   "source": [
    "fds = fz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a0d2",
   "metadata": {},
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep original and work on data;description copy\n",
    "data.loc[:, 'description_prepro'] = data.description.copy()\n",
    "\n",
    "# case conversion\n",
    "data.description_prepro = data.description_prepro.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717cb90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "# get common stopwords from nltk library in both french and english\n",
    "stop_words = list(stopwords.words('french')) + list(stopwords.words('english'))\n",
    "\n",
    "punct_mark = [\"•\"]\n",
    "\n",
    "apostrophes_stop_words = [\"d'\", \"c'\", \"j'\", \"m'\", \"n'\", \"s'\", \"t'\", \"qu'\", \n",
    "                          \"jusqu'\", \"lorsqu'\", \"puisqu'\", \"quoiqu'\", \"qu'il\", \n",
    "                          \"qu'on\", \"qu'un\", \"qu'une\", \"sans qu'\", \"étant qu'\",\n",
    "                         \"qu’\", \"jusqu’\", \"lorsqu’\", \"puisqu’\", \"quoiqu’\", \n",
    "                          \"qu’il\", \"qu’on\", \"qu’un\", \"qu’une\", \"sans qu’\", \"étant qu’\",\n",
    "                         \"d’\", \"c’\", \"j’\", \"m’\", \"n’\", \"s’\", \"t’\", \"d’un\", \"d’une\", \"c’est\"]\n",
    "additional_fr_stop_words = [\n",
    "    \"au\", \"aux\", \"avec\", \"ce\", \"ces\", \"dans\", \"de\", \"des\", \"du\", \"elle\",\n",
    "    \"en\", \"et\", \"eux\", \"il\", \"je\", \"la\", \"le\", \"leur\", \"lui\", \"ma\",\n",
    "    \"mais\", \"me\", \"même\", \"mes\", \"moi\", \"mon\", \"ne\", \"nos\", \"notre\",\n",
    "    \"nous\", \"on\", \"ou\", \"par\", \"pas\", \"pour\", \"qu\", \"que\", \"qui\", \"sa\",\n",
    "    \"se\", \"ses\", \"son\", \"sur\", \"ta\", \"te\", \"tes\", \"toi\", \"ton\", \"tu\",\n",
    "    \"un\", \"une\", \"vos\", \"votre\", \"vous\", \"c’\", \"d’\", \"j’\", \"l’\", \"à\", \"m’\",\n",
    "    \"n’\", \"s’\", \"t’\", \"y’\", \"été\", \"étée\", \"étées\", \"étés\", \"étant\", \"suis\",\n",
    "    \"es\", \"est\", \"sommes\", \"êtes\", \"sont\", \"serai\", \"seras\", \"sera\",\n",
    "    \"serons\", \"serez\", \"seront\", \"serais\", \"serait\", \"serions\", \"seriez\",\n",
    "    \"seraient\", \"étais\", \"était\", \"étions\", \"étiez\", \"étaient\", \"fus\",\n",
    "    \"fut\", \"fûmes\", \"fûtes\", \"furent\", \"sois\", \"soit\", \"soyons\", \"soyez\",\n",
    "    \"soient\", \"fusse\", \"fusses\", \"fût\", \"fussions\", \"fussiez\", \"fussent\",\n",
    "    \"ayant\", \"eu\", \"eue\", \"eues\", \"eus\", \"ai\", \"as\", \"avons\", \"avez\",\n",
    "    \"ont\", \"aurai\", \"auras\", \"aura\", \"aurons\", \"aurez\", \"auront\", \"aurais\",\n",
    "    \"aurait\", \"aurions\", \"auriez\", \"auraient\", \"avais\", \"avait\", \"avions\",\n",
    "    \"aviez\", \"avaient\", \"eut\", \"eûmes\", \"eûtes\", \"eurent\", \"aie\", \"aies\",\n",
    "    \"ait\", \"ayons\"]\n",
    "\n",
    "all_stop_words = set(stop_words + apostrophes_stop_words + additional_fr_stop_words + punct_mark)\n",
    "all_stop_words = list(all_stop_words)\n",
    "\n",
    "# remove stop words in data.description_prepro\n",
    "for text in enumerate(data.description_prepro):\n",
    "    \n",
    "    filtered_string = filter(lambda word: word not in all_stop_words, text[1].split()) \n",
    "    filtered_string = \" \".join(list(filtered_string))\n",
    "    data.loc[text[0], 'description_prepro'] = filtered_string\n",
    "    \n",
    "# Punctuation removal\n",
    "\n",
    "def punctuation_translator(text):\n",
    "\n",
    "    # Make a translation table to remove punctuation characters (third argument (ASCII characters mapped to none)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Use the translate method to remove punctuation characters\n",
    "    no_punctuation_text = text.translate(translator)\n",
    "    \n",
    "    return no_punctuation_text\n",
    "    \n",
    "data.description_prepro = data.description_prepro.apply(punctuation_translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description_prepro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5836e",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfc291",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4f603",
   "metadata": {},
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "trigrams_df = pd.DataFrame(columns=[\"word1\", \"word2\", 'word3'])\n",
    "\n",
    "for text in data.description_prepro:\n",
    "    \n",
    "    # split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # create the bigrams\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "\n",
    "    # convert the bigrams to a dataframe\n",
    "    new_trigrams_df = pd.DataFrame(trigrams, columns=[\"word1\", \"word2\", 'word3'])\n",
    "    trigrams_df = pd.concat([trigrams_df, new_trigrams_df])\n",
    "    \n",
    "most_common_trigrams = trigrams_df[\"word1\"] + \" \" + trigrams_df[\"word2\"]+ \" \" + trigrams_df[\"word3\"]\n",
    "most_common_trigrams= most_common_trigrams.value_counts().head(1000)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "print(\"The 5 most common trigrams are:\\n\")\n",
    "print(most_common_trigrams)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727c467",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d37b5",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "bigram_df = pd.DataFrame(columns=[\"word1\", \"word2\"])\n",
    "\n",
    "for text in data.description_prepro:\n",
    "    \n",
    "    # split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # create the bigrams\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "\n",
    "    # convert the bigrams to a dataframe\n",
    "    new_bigrams_df = pd.DataFrame(bigrams, columns=[\"word1\", \"word2\"])\n",
    "    bigram_df = pd.concat([bigram_df, new_bigrams_df])\n",
    "    \n",
    "most_common_bigrams = bigram_df[\"word1\"] + \" \" + bigram_df[\"word2\"]\n",
    "most_common_bigrams = most_common_bigrams.value_counts().head(1000)\n",
    "    \n",
    "print(\"The 5 most common bigrams are:\\n\")\n",
    "print(most_common_bigrams)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b2ff9",
   "metadata": {},
   "source": [
    "## Get most common words w/ CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0d292",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Initialize the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the fit_transform method to extract features from the text data\n",
    "features = vectorizer.fit_transform(data.description_prepro)\n",
    "\n",
    "# The result is a sparse matrix, which can be easily converted to a dense array using .toarray()\n",
    "features_array = features.toarray()\n",
    "\n",
    "# get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# put together term-doc matrix and feature names in pd.DataFrame\n",
    "word_description_occ = pd.DataFrame(data=features_array, columns=feature_names)\n",
    "\n",
    "# To get the most common words, we can sum up the values in each row\n",
    "word_counts = word_description_occ.sum(axis=0)\n",
    "\n",
    "# And then sort the word_counts in descending order\n",
    "most_common_words = word_counts.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "print(\"Most common words: \\n\")\n",
    "most_common_words.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905215b1",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62715032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## replace keywords for easier keyword extraction\n",
    "\n",
    "data.description_prepro.replace(['(?i)(Google Tag Manager)|\\b(GTM)\\b', \"(?i)\\b(GA4)\\b|\\b(GA)\\b\", '(?i)\\b(Google Colab)\\b', '\\b(GCP)\\b|(google cloud plateform)\\b'], value=['Google Tag Manager', 'Google Analytics', 'Google Colaboratory', 'Google Cloud Plateform'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)\\b(AWS)\\b|Amazon Web Services'], value=['Amazon Web Services'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)SKlearn|Scikit'], value=['ScikitLearn'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)data viz'], value=['data visualisation'] ,regex=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "'sas', 'Spark', 'BigML', 'D3.js', 'MATLAB', 'Excel', 'ggplot2', 'Tableau', 'Jupyter', \n",
    "'Matplotlib', 'NLTK', 'TensorFlow', 'Weka', 'Google Analytics', 'KNIME', \n",
    "'Flink', 'MongoDB', 'Minitab', 'Rapidminer', 'DataRobot', 'NLTK', 'Hadoop', 'Power BI', \n",
    "'QlikView', 'MySQL', 'Neo4j', 'HANA', 'Spotfire', 'SPSS', 'STATA', 'RiverLogic', \n",
    "'Lumira', 'Pig', 'Keras', 'NumPy', 'PyTorch', 'Seaborn', 'Wolfram Mathematica', \n",
    "'WebSockets', 'Algorithms.io', 'ForecastThis', 'BigQuery', 'GitHub', \n",
    "'Pycharm', 'Visual Studio Code', 'Linux', 'Windows', 'macOS', 'Google Colaboratory', \n",
    "'Google Cloud Plateform', 'Watson Studio', 'Amazon Web Services', \n",
    "'EC2', 'Amazon Elastic Compute Cloud', 'Microsoft Azure', \n",
    "'Nvidia Jetson Nano', 'Arduino', 'Beam', 'Semantria', 'Trackur', 'Cassandra', 'OctoParse', \n",
    "'Content Grabber', 'OpenRefine', 'Google Fusion Table', 'scipy', 'pandas', 'NPM', 'Redshift', \n",
    "'Snowflake', 'Alteryx', 'Domino Data Lab', 'Kafka', 'Hbase', 'Elasticsearch', 'Maven', \n",
    "'Ansible', 'Gitlab', 'Jenkins', 'Bash', 'IntelliJ', 'MySQL', 'PostreSQL', 'Sonar', \n",
    "'Jira', 'OpenCV', 'TimescaleDB', 'Grafana', 'Google Sheet', 'Pig', 'Talend', 'MSBI',\n",
    "'SAP BO', 'Abode Campaign', 'Google Data Studio', 'Dataform', 'Looker',\n",
    "'Mode', 'Metabase', 'Power Query', 'Power Pivot', 'DataIku', 'MLFlow', 'DVC', 'Kibana', 'SageMaker',\n",
    "'Minio', 'S3', 'MQTT'\n",
    "]\n",
    "\n",
    "keywords_programming = [\n",
    "'sql', 'python', 'r', 'c', 'c#', 'javascript', 'js',  'java', 'scala', 'sas', 'matlab', \n",
    "'c++', 'c/c++', 'perl', 'go', 'typescript', 'bash', 'html', 'css', 'php', 'powershell', 'rust', \n",
    "'kotlin', 'ruby',  'dart', 'assembly', 'swift', 'vba', 'lua', 'groovy', 'delphi', 'objective-c', \n",
    "'haskell', 'elixir', 'julia', 'clojure', 'solidity', 'lisp', 'f#', 'fortran', 'erlang', 'apl', \n",
    "'cobol', 'ocaml', 'crystal', 'javascript/typescript', 'golang', 'nosql', 'mongodb', 't-sql', 'no-sql',\n",
    "'visual_basic', 'pascal', 'mongo', 'pl/sql',  'sass', 'vb.net', 'mssql', \n",
    "]\n",
    "\n",
    "keywords_libraries = [\n",
    "'scikit-learn', 'jupyter', 'theano', 'openCV', 'spark', 'nltk', 'mlpack', 'chainer', 'fann', 'shogun', \n",
    "'dlib', 'mxnet', 'node.js', 'vue', 'vue.js', 'keras', 'ember.js', 'jse/jee',\n",
    "]\n",
    "\n",
    "keywords_analyst_tools = [\n",
    "'excel', 'tableau',  'word', 'powerpoint', 'looker', 'powerbi', 'outlook', 'azure', 'jira', 'twilio',  'snowflake', \n",
    "'shell', 'linux', 'sas', 'sharepoint', 'mysql', 'visio', 'git', 'mssql', 'powerpoints', 'postgresql', 'spreadsheets',\n",
    "'seaborn', 'pandas', 'gdpr', 'spreadsheet', 'alteryx', 'github', 'postgres', 'ssis', 'numpy', 'power_bi', 'spss', 'ssrs', \n",
    "'microstrategy',  'cognos', 'dax', 'matplotlib', 'dplyr', 'tidyr', 'ggplot2', 'plotly', 'esquisse', 'rshiny', 'mlr',\n",
    "'docker', 'linux', 'jira',  'hadoop', 'airflow', 'redis', 'graphql', 'sap', 'tensorflow', 'node', 'asp.net', 'unix',\n",
    "'jquery', 'pyspark', 'pytorch', 'gitlab', 'selenium', 'splunk', 'bitbucket', 'qlik', 'terminal', 'atlassian', 'unix/linux',\n",
    "'linux/unix', 'ubuntu', 'nuix', 'datarobot',\n",
    "]\n",
    "\n",
    "keywords_cloud_tools = [\n",
    "'aws', 'azure', 'gcp', 'snowflake', 'redshift', 'bigquery', 'aurora',\n",
    "]\n",
    "\n",
    "keywords_general_tools = [\n",
    "'microsoft', 'slack', 'apache', 'ibm', 'html5', 'datadog', 'bloomberg',  'ajax', 'persicope', 'oracle', \n",
    "]\n",
    "\n",
    "keywords_general = [\n",
    "'coding', 'server', 'database', 'cloud', 'warehousing', 'scrum', 'devops', 'programming', 'saas', 'ci/cd', 'cicd', \n",
    "'ml', 'data_lake', 'frontend','front-end', 'back-end', 'backend', 'json', 'xml', 'ios', 'kanban', 'nlp',\n",
    "'iot', 'codebase', 'agile/scrum', 'agile', 'ai/ml', 'ai', 'paas', 'machine_learning', 'macros', 'iaas',\n",
    "'fullstack', 'dataops', 'scrum/agile', 'ssas', 'mlops', 'debug', 'etl', 'a/b', 'slack', 'erp', 'oop', \n",
    "'object-oriented', 'etl/elt', 'elt', 'dashboarding', 'big-data', 'twilio', 'ui/ux', 'ux/ui', 'vlookup', \n",
    "'crossover',  'data_lake', 'data_lakes', 'bi', 'pack office'\n",
    "]\n",
    "\n",
    "ml_tools = [\n",
    "'Tensorflow', 'Keras', 'PyTorch', 'ScikitLearn', 'sklearn', 'scikit'\n",
    "]\n",
    "\n",
    "big_data_tools = [\n",
    "'Hadoop', 'HDFS', 'YARN', 'Hive', 'Map', 'Reduce', 'Tez', 'Spark'\n",
    "]\n",
    "\n",
    "skills = [\n",
    "'informatique décisionnelle', 'Extraction', 'Nettoyage', 'transformation', 'ingestion', \n",
    "'data visualisation', 'modélisation', 'reporting', 'veille technologique', 'data mining',\n",
    "'kpi', 'computer vision'\n",
    "]\n",
    "\n",
    "soft_skills = [\n",
    "\"Esprit d'analyse\", \"Sens du service\", \"Rigueur\", \"communication\", 'positif', 'créatif', \n",
    "'pragmatique', 'souple', 'agile', \"Autonome\", 'Polyvalent', 'travailler en équipe', \"esprit d'équipe\"\n",
    "'esprit de synthèse', 'aisance relationnelle', 'force de proposition', \"capacité d'analyse\",\n",
    "\"anglais\", \"espagnol\", \"francais\"\n",
    "]\n",
    "\n",
    "coding_languages = [\n",
    "'SQL', 'Python', 'R', 'Julia', 'Scala', 'C++', 'Java', 'Javascript', 'Go'\n",
    "]\n",
    "\n",
    "# Add up all keyword lists\n",
    "all_kw = tools + keywords_programming + keywords_libraries + keywords_analyst_tools + keywords_cloud_tools + keywords_general_tools + keywords_general + ml_tools + big_data_tools + skills + soft_skills + coding_languages\n",
    "\n",
    "# Lower case & remove duplicates\n",
    "all_kw = list(set([word.lower() for word in all_kw]))\n",
    "\n",
    "# keyword extraction\n",
    "data['kw_extraction'] = data['description_prepro'].apply(lambda x: [])\n",
    "\n",
    "for text in enumerate(data.description_prepro):\n",
    "\n",
    "    kw_list = [word for word in all_kw if word in text[1]]\n",
    "    data.loc[text[0], \"kw_extraction\"].extend(kw_list)\n",
    "\n",
    "# number of keywords extracted\n",
    "data['len_list'] = data['kw_extraction'].map(lambda x: len(x))\n",
    "data['len_list'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893224",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d8ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining constants\n",
    "keywords = ['durée', 'stage', 'contrat', 'localisation|lieu|location', 'statut', 'salaire', 'formation', 'profil', \n",
    "            'mission', 'avantage', 'experience', 'soft', 'langue']\n",
    "n_chars_after = 20\n",
    "m_chars_before = 60\n",
    "#col_name = f'charsss_around_{kw}'\n",
    "\n",
    "# loop over keywords list\n",
    "for kw in keywords:\n",
    "    \n",
    "    # define our new columns\n",
    "    data[f'chars_around_{kw}'] = 'NC '\n",
    "\n",
    "    # loop over each string in data.description\n",
    "    for i, text in zip(data.index, data['description_prepro']):\n",
    "        \n",
    "        # get iterator over all keywords matches in each string\n",
    "        search_obj = re.finditer(kw, text, re.I)\n",
    "\n",
    "        if search_obj: # ... is true, then ...\n",
    "\n",
    "            # get each keyword match in string\n",
    "            for match in search_obj:\n",
    "\n",
    "                # Find the start index of the keyword\n",
    "                start = match.span()[0]\n",
    "\n",
    "                # Find the end index of the keyword\n",
    "                end = match.span()[1]\n",
    "\n",
    "                # Truncate line to get only 'n' characters before and after the keyword\n",
    "                line = text[start-m_chars_before:end+n_chars_after]\n",
    "                \n",
    "                # add up ever line containing keyword match (if several) in corresponding cells\n",
    "                data.loc[i, f'chars_around_{kw}'] += line\n",
    "                \n",
    "                # get rid of by default 'NC' string\n",
    "                data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}'].replace('NC ', '')\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # if no keyword match then keep by default value\n",
    "            data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1c642",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a2339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc98e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
