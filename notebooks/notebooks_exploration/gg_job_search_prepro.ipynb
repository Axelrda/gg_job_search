{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef84455",
   "metadata": {},
   "source": [
    "# Scraping bullshit jobs ... a long way to hell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9487d252",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# import my functions\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpreprocess\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# library for plotting data over maps\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshapefile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mshp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocess'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "# import my functions\n",
    "import preprocess\n",
    "\n",
    "# library for plotting data over maps\n",
    "import shapefile as shp\n",
    "\n",
    "# libraries used for tokenization\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, MWETokenizer\n",
    "\n",
    "# Libraries used to remove similar job description based on cosine similarity \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# libraries used for text normalization\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# get exchange rate EUR/US DOLLAR\n",
    "import requests\n",
    "\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd10655",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced546c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/axel/ds_projects/projects/gg_job_search/data/gg_job_search_all_RAW.csv')\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e1272",
   "metadata": {},
   "source": [
    "## General overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcbba87",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8138 entries, 0 to 8137\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      8138 non-null   int64  \n",
      " 1   title           8138 non-null   object \n",
      " 2   company_name    8138 non-null   object \n",
      " 3   location        8127 non-null   object \n",
      " 4   via             8138 non-null   object \n",
      " 5   description     8078 non-null   object \n",
      " 6   job_highlights  1805 non-null   object \n",
      " 7   related_links   8078 non-null   object \n",
      " 8   thumbnail       6372 non-null   object \n",
      " 9   extensions      8138 non-null   object \n",
      " 10  job_id          7862 non-null   object \n",
      " 11  posted_at       8138 non-null   object \n",
      " 12  schedule_type   8063 non-null   object \n",
      " 13  date_time       8138 non-null   object \n",
      " 14  search_query    8138 non-null   object \n",
      " 15  Unnamed: 0.1    6333 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 1017.4+ KB\n",
      "None \n",
      "\n",
      "Unnamed: 0        6333\n",
      "title             1458\n",
      "company_name      1210\n",
      "location           341\n",
      "via                 91\n",
      "description       2376\n",
      "job_highlights     423\n",
      "related_links     3237\n",
      "thumbnail          793\n",
      "extensions         676\n",
      "job_id            2813\n",
      "posted_at           26\n",
      "schedule_type        4\n",
      "date_time          571\n",
      "search_query         3\n",
      "Unnamed: 0.1        10\n",
      "dtype: int64 \n",
      "\n",
      "data engineer     3118\n",
      "data scientist    2531\n",
      "data analyst      2489\n",
      "Name: search_query, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.info(), '\\n')\n",
    "print(data.nunique(), '\\n')\n",
    "print(data.search_query.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a01234",
   "metadata": {},
   "source": [
    "## Null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e83723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location            11\n",
       "description         60\n",
       "job_highlights    6333\n",
       "related_links       60\n",
       "thumbnail         1766\n",
       "job_id             276\n",
       "schedule_type       75\n",
       "Unnamed: 0.1      1805\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47929c2",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583af771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates : 546\n",
      "Number of duplicates (based on identical job_id) : 5324 \n",
      " ======================================================================\n",
      "Search query results :\n",
      " data scientist    1479\n",
      "data analyst       720\n",
      "data engineer      615\n",
      "Name: search_query, dtype: int64 \n",
      " ======================================================================\n",
      "Final number of rows :  2814\n",
      "Final number of columns :  17\n"
     ]
    }
   ],
   "source": [
    "print('Number of duplicates :',data[data.duplicated()].shape[0])\n",
    "print('Number of duplicates (based on identical job_id) :', data[data.duplicated(['job_id'])].shape[0],'\\n', '='*70) \n",
    "\n",
    "# removing duplicates based on job_id\n",
    "data = data.drop_duplicates(['job_id'])\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Search query results :\\n', data.search_query.value_counts(), '\\n', '='*70)\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94e020",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Purely based job id, we can skim off a lots of duplicates. Nonetheless i still found a few of identical / near-identical job descriptions, maybe because [sometimes recruiters or companies post the same advert for a job which results in duplicate data.](https://medium.com/analytics-vidhya/data-science-job-search-using-nlp-and-lda-in-python-12ecbfac79f9)\n",
    "\n",
    "To get rid off these ones, i could use cosine similatity between job descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e082a5f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates (based on cosine similarity) :  1037\n",
      "Final number of rows :  1777\n",
      "Final number of columns :  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9295/1214297801.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n"
     ]
    }
   ],
   "source": [
    "### removing duplicates based on cosine similarity between \n",
    "## job descriptions (code from Thomas Caffrey, see link above)\n",
    "\n",
    "# Defining our collection of job description texts to tokenize\n",
    "data.description = data['description'].fillna('')\n",
    "corpus = data['description']\n",
    "\n",
    "# instantiate CountVectorizer object\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# Fit_transform to vectorize each job description (map terms to feature indices)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "\n",
    "# Compute cosine similarities and put it in dataframe\n",
    "cos_df = pd.DataFrame(cosine_similarity(X_train_counts))\n",
    "\n",
    "\n",
    "## reshape dataframe for easier comparison\n",
    "\n",
    "# get arrays of rows indices and col indices from col_df.shape\n",
    "i, j = np.indices(cos_df.shape).reshape(2,-1)\n",
    "\n",
    "# reshape values to get a 1D array \n",
    "cos_values = cos_df.values.reshape(-1)\n",
    "\n",
    "cos_sim_df = pd.DataFrame({'i': i, 'j': j, 'sim':cos_values})\n",
    "\n",
    "# get cosine similarity values only above 0.98 \n",
    "cos_rem = cos_sim_df[(cos_sim_df['sim'] > 0.98) & (i!=j)]\n",
    "\n",
    "# Method to remove duplicates but keep first instance:\n",
    "# Trying to drop duplicates on i and j columns won't work as the row numbers of duplicates are either in i or j not both.\n",
    "# Setting another column that combines the i & j values ensures that duplicates can be dropped.\n",
    "\n",
    "cos_rem['i*j'] = cos_rem['i'] * cos_rem['j']\n",
    "drop_rows = np.unique(cos_rem.drop_duplicates('i*j')['i'].values)\n",
    "\n",
    "# keep only non-duplicated job postings\n",
    "data = data[~data.index.isin(drop_rows)] \n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "print('Number of duplicates (based on cosine similarity) : ' ,drop_rows.shape[0])\n",
    "print('Final number of rows : ', data.shape[0])\n",
    "print('Final number of columns : ', data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56599a6b",
   "metadata": {},
   "source": [
    "## Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d501f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   ['il y a 17 heures', 'À plein temps']\n",
       "1       ['il y a 11 heures', '35\\xa0k\\xa0€ à 40\\xa0k\\x...\n",
       "2                   ['il y a 19 heures', 'À plein temps']\n",
       "3                   ['il y a 14 heures', 'À plein temps']\n",
       "4                   ['il y a 22 heures', 'À plein temps']\n",
       "                              ...                        \n",
       "1772                 ['il y a 8 heures', 'À plein temps']\n",
       "1773                 ['il y a 8 heures', 'À plein temps']\n",
       "1774                 ['il y a 8 heures', 'À plein temps']\n",
       "1775                 ['il y a 8 heures', 'À plein temps']\n",
       "1776                 ['il y a 8 heures', 'À plein temps']\n",
       "Name: extensions, Length: 1777, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d1851",
   "metadata": {},
   "source": [
    "## Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaef9dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(data.title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242f91",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "Based on this sample of job titles, we could create : \n",
    "    \n",
    "    * Contract_type (full-time, part-time ...) \n",
    "    * Contract_status (CDI, CDD, work-study, internship ...)\n",
    "    * Duration of Contract (Duration/Undetermined)\n",
    "    * Experience ( Senior, Junior ...)\n",
    "    * Data Specialization (Supply chain, Marketing, Clinical ...)\n",
    "    * Multiple titles (Analyst/Scientist, Scientist/ML Engineer, Manager/Analyst ...)\n",
    "    * Specific expertise asked for (Python, Power BI ...)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1679e1",
   "metadata": {},
   "source": [
    "## Explore company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3400f598",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique companies : 1078\n"
     ]
    }
   ],
   "source": [
    "print('number of unique companies :', data.company_name.nunique())\n",
    "#data.company_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1583ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 20\n",
    "#data.loc[data['company_name'] == 'Unspecified', ['description']].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05598b",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "When ***Unspecified***, companies name can be found in description column.\n",
    "\n",
    "Possible new columns :\n",
    "\n",
    "    * Group/Holding (Y/N/NC)\n",
    "    * Interim company (Y/N/NC)\n",
    "\n",
    "Based on the number of job posting per company we could potentially infer about : **size of company ? / Amount of data to work on** / \n",
    "\n",
    "Adding a time variable and much more data, the number of similar / identical job postings for the same company could maybe give insights on the **company's turnover rate / company's growth / magnitude of need-urgency to hire** ...  \n",
    "\n",
    "It seems like extracting additional informations without more context will be difficult. Having access to each company's structure information we could create :\n",
    "\n",
    "    * Size of company\n",
    "    * Industry\n",
    "    * Public / Private\n",
    "    \n",
    "We'll see if can extract more related informations in the following columns. Otherwise, we could try to scrap **Glassdoor databases** (or similar) to get those informations.remains to be seen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b0ce67",
   "metadata": {},
   "source": [
    "## Explore location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01afb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique locations :  252\n"
     ]
    }
   ],
   "source": [
    "print('number of unique locations : ', data.location.nunique())\n",
    "#data.location.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29d39825",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 50\n",
    "#data.location.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0cabad",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "We could create a map of th repartition of job posting based on location provided.\n",
    "\n",
    "Some companies don't provide precise location *(ex : location = FRANCE)* and the information is not available in description column either. Further investigations will be needed for these companies, perhaps in conjunction with other databases *(GLASSDOOR / SIRENE databases for instance)*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2d818",
   "metadata": {},
   "source": [
    "## Explore via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a48ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique job plateforms :  79\n"
     ]
    }
   ],
   "source": [
    "print('number of unique job plateforms : ', data.via.nunique())\n",
    "# data.via.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b57edd",
   "metadata": {},
   "source": [
    "## Explore description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acda0f5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1777.000000\n",
      "mean      2940.416995\n",
      "std       1637.007016\n",
      "min          0.000000\n",
      "25%       1781.000000\n",
      "50%       2678.000000\n",
      "75%       3844.000000\n",
      "max      14170.000000\n",
      "Name: description, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.description.str.len().describe())\n",
    "\n",
    "plt.hist(data.description.str.len(), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391bf1f6",
   "metadata": {},
   "source": [
    "#### INSIGHTS :\n",
    "\n",
    "\n",
    "We could create :\n",
    "\n",
    "    * Lenght of description (dunno what informations it could provide yet)\n",
    "    * Toold required (Excel, Google Tag Manager ...)\n",
    "    * Coding languages required (R, Python, SQL ...)\n",
    "    * Skills required (reporting, data visualization)\n",
    "    * Required experience\n",
    "    * Duration of contract\n",
    "    * Avantages (ticket resto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "763cf47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_employment_type(row):\n",
    "    employment_type_dict = {\n",
    "        'CDI': ['cdi',],\n",
    "        'internship': ['stage', 'internship', 'stagiaire'],\n",
    "        'apprenticeship': ['alternant', '(apprenti)', 'professionnalisation', 'alternance'],\n",
    "        'CDD': ['cdd'],\n",
    "        'freelance': ['freelance', 'prestataire', 'freelancer'],\n",
    "        'interim': ['interim'],\n",
    "        'consultant': ['consultant'],\n",
    "    }\n",
    "    for employment_type, keywords in employment_type_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return employment_type\n",
    "    return np.NaN\n",
    "\n",
    "def create_seniority_level(row):\n",
    "    seniority_level_dict = {\n",
    "        'senior': ['senior', 'advanced', 'avance', 'sr', 'experimente'],\n",
    "        'mid-level': ['confirme'],\n",
    "        'junior': ['junior', 'debutant', 'jr', 'entre-level'],\n",
    "    }\n",
    "    for seniority_level, keywords in seniority_level_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U): #or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return seniority_level\n",
    "    return np.NaN\n",
    "\n",
    "def create_executive_title(row):\n",
    "    executive_title_dict = {\n",
    "        'lead': ['lead'],\n",
    "        'Director': ['director', 'directeur'],\n",
    "        'Manager': ['manager', 'project manager', 'chef de projet'],\n",
    "        'Assistant': ['assistant'],\n",
    "        'Chief': ['chief'],\n",
    "        'Head': ['head'],\n",
    "        'Supervisor': ['supervisor'],\n",
    "    }\n",
    "    for executive_title, keywords in executive_title_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U):\n",
    "                return executive_title\n",
    "    return np.NaN\n",
    "\n",
    "def create_job_specialization(row):\n",
    "    job_specialization_dict = {\n",
    "        'career': ['career'],\n",
    "        'web': ['web'],\n",
    "        'media': ['media'],\n",
    "        'online': ['online'],\n",
    "        'marketing': ['marketing', 'market'],\n",
    "        'crm': ['crm'],\n",
    "        'assurance': ['indemnisation'],\n",
    "        'immobilier': ['immobilier'],\n",
    "        'product': ['product'],\n",
    "        'people': ['people', 'hr', 'human', 'workforce'],\n",
    "        'informatique': ['informatique'],\n",
    "        'supply_chain': ['supply'],\n",
    "        'logistique': ['logistique'],\n",
    "        'medical': ['medical', 'clinique', 'sante'],\n",
    "        'finance': ['finance'],\n",
    "        'recherche': ['recherche'],\n",
    "        'tv': ['tv'],\n",
    "        'game': ['game'],\n",
    "        'geo': ['geo'],\n",
    "    }\n",
    "    for job_specialization, keywords in job_specialization_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword,unidecode(row['title']), re.I|re.U):\n",
    "                return job_specialization\n",
    "    return np.NaN\n",
    "\n",
    "def create_remote(row):\n",
    "    remote_dict = {\n",
    "        'Y': ['remote', 'teletravail hybride', 'teletravail complet', 'jour de teletravail',\n",
    "                   'jours de teletravail', 'teletravail partiel', 'distanciel', 'teletravail', '(TT)']        \n",
    "    }\n",
    "    for remote, keywords in remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "def create_full_partial_remote(row):\n",
    "    full_partial_remote_dict = {\n",
    "        'full': ['full remote', 'teletravail complet'],\n",
    "        'partial_remote': ['teletravail de','teletravail hybride', 'jours de teletravail', 'teletravail partiel', 'jour de teletravail',]\n",
    "    }\n",
    "    for remote, keywords in full_partial_remote_dict.items():\n",
    "        for keyword in keywords:\n",
    "            if re.search(keyword, unidecode(row['title']), re.I|re.U) or re.search(keyword, unidecode(row['description']), re.IGNORECASE):\n",
    "                return remote\n",
    "    return np.NaN\n",
    "\n",
    "# creation of holding feature Y/N\n",
    "\n",
    "conditions = [\n",
    "    \n",
    "    (data['company_name'].str.contains('Groupe(?i)|Holdings*(?i)')),\n",
    "    (data['description'].str.contains('ESN(?i)')).astype(bool)\n",
    "    \n",
    "]\n",
    "\n",
    "choices = ['Holding', 'ESN']\n",
    "\n",
    "data['holding'] = np.select(conditions, choices, default=np.NaN)\n",
    "\n",
    "data['holding'].value_counts()\n",
    "\n",
    "\n",
    "#data = data.applymap(lambda x: unidecode(x) if isinstance(x, str) else x) # used for accent-insensitive search, got replaced directly in functions (see above)\n",
    "# lower every string of dataframe for easier search\n",
    "data = data.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "data['executive_title'] = data.apply(create_executive_title, axis=1)\n",
    "data['seniority_level'] = data.apply(create_seniority_level, axis=1) \n",
    "data['employment_type'] = data.apply(create_employment_type, axis=1) \n",
    "data['job_specialization'] = data.apply(create_job_specialization, axis=1) \n",
    "data['remote'] = data.apply(create_remote, axis=1) \n",
    "data['full_partial_remote'] = data.apply(create_full_partial_remote, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7790e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_salary_col(df):\n",
    "    \n",
    "    # Extract salary from extensions col\n",
    "    df['salary'] = pd.Series(df.extensions.str.split(', ', expand=True)[1])\n",
    "       \n",
    "    # Replace non-salary values to np.NaN\n",
    "    df['salary'].replace([\"'à plein temps']\", \"'à temps partiel']\", \"'stage']\", \"'prestataire']\"], np.NaN, inplace=True)\n",
    "    \n",
    "    # Convert null values to Nothing_found to make parsing easier\n",
    "    df.at[df.salary.isnull(), 'salary'] = \"nothing_found\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_og_salary_currency(df):\n",
    "    \n",
    "    df['og_salary_currency'] = df.salary.str.extract(r'(\\€|\\$us)', flags=re.IGNORECASE)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_og_salary_period(df):\n",
    "    \n",
    "    df['og_salary_period'] = df.salary.str.extract(r'(\\ban\\b|\\bmois\\b|\\bjour\\b)', flags=re.IGNORECASE)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def clean_salary(df):\n",
    "    \n",
    "    \n",
    "        ####### targeted clean for easier parsing #######\n",
    "\n",
    "        for index, salary_str in enumerate(df.salary):\n",
    "                \n",
    "                if 'nothing_found' not in salary_str:\n",
    "                    \n",
    "                    # split salary period from rest of string\n",
    "                    salary_str =  salary_str.split(' par')[0]\n",
    "                    \n",
    "                    # remove single quote\n",
    "                    salary_str = salary_str.split(\"'\")[1]\n",
    "\n",
    "                    # remove unicode chars\n",
    "                    salary_str = salary_str.replace(\"\\\\xa0\", \"\")\n",
    "                    salary_str = salary_str.replace(\"\\\\u202f\", \"\")\n",
    "\n",
    "                    #remove currency\n",
    "                    salary_str = salary_str.replace(\"€\", \"\")\n",
    "                    salary_str = salary_str.replace(\"$us\", \"\")\n",
    "\n",
    "                    df.at[index, 'salary'] = salary_str\n",
    "                    \n",
    "        return df\n",
    "\n",
    "def get_salary_range_and_mean(df):                    \n",
    "                 \n",
    "                                        \n",
    "        for index, row in df.iterrows():\n",
    "    \n",
    "            # extract boundaries of YEAR salaries given as a range + calculate mean salary\n",
    "            if (row['og_salary_period'] == 'an') and 'à' in row['salary']:\n",
    "\n",
    "                # remove k \n",
    "                row['salary'] = row['salary'].replace('k', '000')\n",
    "\n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.salary.split(' à ')[0].split(',')[0])\n",
    "                upper_bound = float(row.salary.split(' à ')[1].split(',')[0])\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if lower_bound < 1000:\n",
    "                    lower_bound = lower_bound * 1000\n",
    "\n",
    "                if upper_bound < 1000:\n",
    "                    upper_bound = upper_bound * 1000\n",
    "                \n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "            # extract discrete YEAR salaries        \n",
    "            elif (row['og_salary_period'] == 'an') and 'à' not in row['salary']:\n",
    "\n",
    "                # remove k\n",
    "                row['salary'] = row['salary'].replace('k', '000')\n",
    "                row['salary'] = row['salary'].replace(',', '.')\n",
    "\n",
    "                # convert value to float\n",
    "                row['salary'] = float(row['salary'])\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if row['salary'] < 1000:\n",
    "                    row['salary'] = row['salary'] * 1000\n",
    "\n",
    "                # assign result to discrete salary column\n",
    "                df.at[index, 'discrete_salary'] = row['salary']\n",
    "\n",
    "                \n",
    "\n",
    "            \n",
    "            # extract boundaries of MONTH salaries given as a range + calculate mean salary   \n",
    "            elif (row['og_salary_period'] == 'mois') and 'à' in row['salary']:\n",
    "\n",
    "                # remove k and replace commas\n",
    "                row['salary'] = row['salary'].replace('k', '00')\n",
    "                row['salary'] = row['salary'].replace(',', '.')\n",
    "\n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.salary.split(' à ')[0])\n",
    "                upper_bound = float(row.salary.split(' à ')[1])\n",
    "\n",
    "                # re-establish a consistent value regarding to common month salaries\n",
    "                if lower_bound < 10:\n",
    "                    lower_bound = lower_bound * 100\n",
    "\n",
    "                if lower_bound < 1000:\n",
    "                    lower_bound = lower_bound * 10\n",
    "\n",
    "                if upper_bound < 10:\n",
    "                    upper_bound = upper_bound * 100\n",
    "\n",
    "                if upper_bound < 1000:\n",
    "                    upper_bound = upper_bound * 10\n",
    "\n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            # extract discrete MONTH salaries        \n",
    "            elif (row['og_salary_period'] == 'mois') and 'à' not in row['salary']:\n",
    "\n",
    "                # remove k and replace commas\n",
    "                row['salary'] = row['salary'].replace('k', '00')\n",
    "                row['salary'] = row['salary'].replace(',', '.')\n",
    "\n",
    "                # convert value to float\n",
    "                row['salary'] = float(row['salary'])\n",
    "\n",
    "\n",
    "                # re-establish a consistent value regarding to common year salaries\n",
    "                if row['salary'] < 1000:\n",
    "                    row['salary'] = row['salary'] * 1000\n",
    "\n",
    "                # assign result to discrete salary column\n",
    "                df.at[index, 'discrete_salary'] = row['salary']                 \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # extract boundaries of DAY salaries given as a range + calculate mean salary   \n",
    "            elif (row['og_salary_period'] == 'jour') and 'à' in row['salary']:\n",
    "                \n",
    "                # get upper and lower bounds\n",
    "                lower_bound = float(row.salary.split(' à ')[0])\n",
    "                upper_bound = float(row.salary.split(' à ')[1])\n",
    "\n",
    "                # get upper / lower bound and discrete_salary columns\n",
    "                df.at[index, 'lower_bound'] = lower_bound\n",
    "                df.at[index, 'upper_bound'] = upper_bound\n",
    "                df.at[index, 'discrete_salary'] = np.mean([lower_bound, upper_bound])\n",
    "                    \n",
    "                    \n",
    "        return df\n",
    "    \n",
    "            \n",
    " # Define a global variable to cache the exchange rate value\n",
    "cached_exchange_rate = None\n",
    "\n",
    "# Define a function to get the exchange rate value\n",
    "def get_exchange_rate():\n",
    "    # Declare the variable as global to modify its value in the function\n",
    "    global cached_exchange_rate\n",
    "    \n",
    "    try:\n",
    "        # Send a network request to get the exchange rate\n",
    "        response = requests.get('https://openexchangerates.org/api/latest.json?app_id=4820391575d04bdd8d07b7e15fb0a463')\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response and calculate the exchange rate\n",
    "        data = response.json()\n",
    "        exchange_rate = data['rates']['EUR'] / data['rates']['USD']\n",
    "        \n",
    "        # Cache the exchange rate value\n",
    "        cached_exchange_rate = exchange_rate\n",
    "        \n",
    "    except (requests.exceptions.RequestException, json.decoder.JSONDecodeError) as e:\n",
    "        # Handle any exceptions that occur during the API request\n",
    "        # If the API request fails, use the cached exchange rate value if it exists\n",
    "        if cached_exchange_rate is not None:\n",
    "            exchange_rate = cached_exchange_rate\n",
    "        else:\n",
    "            # If there is no cached exchange rate value, raise the original exception\n",
    "            raise e\n",
    "    \n",
    "    return exchange_rate           \n",
    "            \n",
    "# This function converts salary values in US dollars to euro currency\n",
    "def convert_salary_currency(df):\n",
    "    \n",
    "    # Create a boolean mask to select rows where salary is in US dollars\n",
    "    mask = df['og_salary_currency'] == '$us'   \n",
    "    \n",
    "    # Multiply the 'discrete_salary' column by the exchange rate\n",
    "    df.loc[mask,'discrete_salary'] *= get_exchange_rate()\n",
    "    \n",
    "    # Return the modified dataframe\n",
    "    return df\n",
    "\n",
    "\n",
    "# This function converts salary values from their original period to yearly, monthly, and daily rates\n",
    "def convert_salary_period(df):\n",
    "    \n",
    "    # Define constants used in the conversion calculations\n",
    "    n_days_per_year = 250\n",
    "    n_days_per_month = 20\n",
    "        \n",
    "    # Create a boolean mask to select rows where salary is reported annually\n",
    "    mask = df['og_salary_period'] == 'an'\n",
    "\n",
    "    # Convert the 'discrete_salary' values in the selected rows to yearly, monthly, and daily rates\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary']\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] / 12\n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] / n_days_per_year\n",
    "\n",
    "    # Create a boolean mask to select rows where salary is reported monthly\n",
    "    mask = df['og_salary_period'] == 'mois'\n",
    "\n",
    "    # Convert the 'discrete_salary' values in the selected rows to yearly, monthly, and daily rates\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary'] * 12\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] \n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] / n_days_per_month\n",
    "\n",
    "    # Create a boolean mask to select rows where salary is reported daily\n",
    "    mask = df['og_salary_period'] == 'jour'\n",
    "\n",
    "    # Convert the 'discrete_salary' values in the selected rows to yearly, monthly, and daily rates\n",
    "    df.loc[mask, 'year_salary'] = df.loc[mask,'discrete_salary'] * n_days_per_year\n",
    "    df.loc[mask, 'month_salary'] = df.loc[mask,'discrete_salary'] * n_days_per_month\n",
    "    df.loc[mask, 'day_salary'] = df.loc[mask,'discrete_salary'] \n",
    "\n",
    "    # Return the modified dataframe\n",
    "    return df\n",
    "    \n",
    "def salary_prepro(df):\n",
    "    \n",
    "    df = get_salary_col(df)\n",
    "    df = get_og_salary_period(df)\n",
    "    df = get_og_salary_currency(df)\n",
    "    df = clean_salary(df)\n",
    "    df = get_salary_range_and_mean(df)\n",
    "    df = convert_salary_period(df)\n",
    "    df = convert_salary_currency(df)\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "####################################\n",
    "#pd.options.display.max_rows = 80\n",
    "pd.options.display.max_colwidth = 20\n",
    "####################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a75d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_useless_col(data):\n",
    "    \n",
    "    data.drop(['extensions'], axis=1, inplace=True)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0778d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000.000000     5\n",
       "45000.000000     5\n",
       "50000.000000     5\n",
       "57500.000000     5\n",
       "112500.000000    4\n",
       "55000.000000     3\n",
       "52500.000000     3\n",
       "90000.000000     3\n",
       "41000.000000     2\n",
       "53222.192100     2\n",
       "2330.630000      2\n",
       "42000.000000     2\n",
       "83634.873300     1\n",
       "43000.000000     1\n",
       "99028.946500     1\n",
       "104355.859025    1\n",
       "85000.000000     1\n",
       "124842.179000    1\n",
       "32500.000000     1\n",
       "37500.000000     1\n",
       "56000.000000     1\n",
       "21000.000000     1\n",
       "1200.000000      1\n",
       "45346.000000     1\n",
       "475000.000000    1\n",
       "500000.000000    1\n",
       "49141.500000     1\n",
       "475.000000       1\n",
       "82722.492864     1\n",
       "62500.000000     1\n",
       "29000.000000     1\n",
       "33656.000000     1\n",
       "137500.000000    1\n",
       "44000.000000     1\n",
       "56500.000000     1\n",
       "20512.000000     1\n",
       "42500.000000     1\n",
       "2150.000000      1\n",
       "2104.000000      1\n",
       "47884.954282     1\n",
       "75000.000000     1\n",
       "2250.000000      1\n",
       "2000.000000      1\n",
       "500.000000       1\n",
       "41500.000000     1\n",
       "47500.000000     1\n",
       "67000.000000     1\n",
       "138452.792500    1\n",
       "38500.000000     1\n",
       "525000.000000    1\n",
       "Name: discrete_salary, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.discrete_salary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d96642d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fds \u001b[38;5;241m=\u001b[39m \u001b[43mfz\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fz' is not defined"
     ]
    }
   ],
   "source": [
    "fds = fz "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd35a0d2",
   "metadata": {},
   "source": [
    "## Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep original and work on data;description copy\n",
    "data.loc[:, 'description_prepro'] = data.description.copy()\n",
    "\n",
    "# case conversion\n",
    "data.description_prepro = data.description_prepro.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717cb90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "# get common stopwords from nltk library in both french and english\n",
    "stop_words = list(stopwords.words('french')) + list(stopwords.words('english'))\n",
    "\n",
    "punct_mark = [\"•\"]\n",
    "\n",
    "apostrophes_stop_words = [\"d'\", \"c'\", \"j'\", \"m'\", \"n'\", \"s'\", \"t'\", \"qu'\", \n",
    "                          \"jusqu'\", \"lorsqu'\", \"puisqu'\", \"quoiqu'\", \"qu'il\", \n",
    "                          \"qu'on\", \"qu'un\", \"qu'une\", \"sans qu'\", \"étant qu'\",\n",
    "                         \"qu’\", \"jusqu’\", \"lorsqu’\", \"puisqu’\", \"quoiqu’\", \n",
    "                          \"qu’il\", \"qu’on\", \"qu’un\", \"qu’une\", \"sans qu’\", \"étant qu’\",\n",
    "                         \"d’\", \"c’\", \"j’\", \"m’\", \"n’\", \"s’\", \"t’\", \"d’un\", \"d’une\", \"c’est\"]\n",
    "additional_fr_stop_words = [\n",
    "    \"au\", \"aux\", \"avec\", \"ce\", \"ces\", \"dans\", \"de\", \"des\", \"du\", \"elle\",\n",
    "    \"en\", \"et\", \"eux\", \"il\", \"je\", \"la\", \"le\", \"leur\", \"lui\", \"ma\",\n",
    "    \"mais\", \"me\", \"même\", \"mes\", \"moi\", \"mon\", \"ne\", \"nos\", \"notre\",\n",
    "    \"nous\", \"on\", \"ou\", \"par\", \"pas\", \"pour\", \"qu\", \"que\", \"qui\", \"sa\",\n",
    "    \"se\", \"ses\", \"son\", \"sur\", \"ta\", \"te\", \"tes\", \"toi\", \"ton\", \"tu\",\n",
    "    \"un\", \"une\", \"vos\", \"votre\", \"vous\", \"c’\", \"d’\", \"j’\", \"l’\", \"à\", \"m’\",\n",
    "    \"n’\", \"s’\", \"t’\", \"y’\", \"été\", \"étée\", \"étées\", \"étés\", \"étant\", \"suis\",\n",
    "    \"es\", \"est\", \"sommes\", \"êtes\", \"sont\", \"serai\", \"seras\", \"sera\",\n",
    "    \"serons\", \"serez\", \"seront\", \"serais\", \"serait\", \"serions\", \"seriez\",\n",
    "    \"seraient\", \"étais\", \"était\", \"étions\", \"étiez\", \"étaient\", \"fus\",\n",
    "    \"fut\", \"fûmes\", \"fûtes\", \"furent\", \"sois\", \"soit\", \"soyons\", \"soyez\",\n",
    "    \"soient\", \"fusse\", \"fusses\", \"fût\", \"fussions\", \"fussiez\", \"fussent\",\n",
    "    \"ayant\", \"eu\", \"eue\", \"eues\", \"eus\", \"ai\", \"as\", \"avons\", \"avez\",\n",
    "    \"ont\", \"aurai\", \"auras\", \"aura\", \"aurons\", \"aurez\", \"auront\", \"aurais\",\n",
    "    \"aurait\", \"aurions\", \"auriez\", \"auraient\", \"avais\", \"avait\", \"avions\",\n",
    "    \"aviez\", \"avaient\", \"eut\", \"eûmes\", \"eûtes\", \"eurent\", \"aie\", \"aies\",\n",
    "    \"ait\", \"ayons\"]\n",
    "\n",
    "all_stop_words = set(stop_words + apostrophes_stop_words + additional_fr_stop_words + punct_mark)\n",
    "all_stop_words = list(all_stop_words)\n",
    "\n",
    "# remove stop words in data.description_prepro\n",
    "for text in enumerate(data.description_prepro):\n",
    "    \n",
    "    filtered_string = filter(lambda word: word not in all_stop_words, text[1].split()) \n",
    "    filtered_string = \" \".join(list(filtered_string))\n",
    "    data.loc[text[0], 'description_prepro'] = filtered_string\n",
    "    \n",
    "# Punctuation removal\n",
    "\n",
    "def punctuation_translator(text):\n",
    "\n",
    "    # Make a translation table to remove punctuation characters (third argument (ASCII characters mapped to none)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    # Use the translate method to remove punctuation characters\n",
    "    no_punctuation_text = text.translate(translator)\n",
    "    \n",
    "    return no_punctuation_text\n",
    "    \n",
    "data.description_prepro = data.description_prepro.apply(punctuation_translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['description_prepro']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e5836e",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfc291",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b4f603",
   "metadata": {},
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "trigrams_df = pd.DataFrame(columns=[\"word1\", \"word2\", 'word3'])\n",
    "\n",
    "for text in data.description_prepro:\n",
    "    \n",
    "    # split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # create the bigrams\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "\n",
    "    # convert the bigrams to a dataframe\n",
    "    new_trigrams_df = pd.DataFrame(trigrams, columns=[\"word1\", \"word2\", 'word3'])\n",
    "    trigrams_df = pd.concat([trigrams_df, new_trigrams_df])\n",
    "    \n",
    "most_common_trigrams = trigrams_df[\"word1\"] + \" \" + trigrams_df[\"word2\"]+ \" \" + trigrams_df[\"word3\"]\n",
    "most_common_trigrams= most_common_trigrams.value_counts().head(1000)\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "print(\"The 5 most common trigrams are:\\n\")\n",
    "print(most_common_trigrams)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a727c467",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d37b5",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "bigram_df = pd.DataFrame(columns=[\"word1\", \"word2\"])\n",
    "\n",
    "for text in data.description_prepro:\n",
    "    \n",
    "    # split the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # create the bigrams\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "\n",
    "    # convert the bigrams to a dataframe\n",
    "    new_bigrams_df = pd.DataFrame(bigrams, columns=[\"word1\", \"word2\"])\n",
    "    bigram_df = pd.concat([bigram_df, new_bigrams_df])\n",
    "    \n",
    "most_common_bigrams = bigram_df[\"word1\"] + \" \" + bigram_df[\"word2\"]\n",
    "most_common_bigrams = most_common_bigrams.value_counts().head(1000)\n",
    "    \n",
    "print(\"The 5 most common bigrams are:\\n\")\n",
    "print(most_common_bigrams)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b2ff9",
   "metadata": {},
   "source": [
    "## Get most common words w/ CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0d292",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Initialize the CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the fit_transform method to extract features from the text data\n",
    "features = vectorizer.fit_transform(data.description_prepro)\n",
    "\n",
    "# The result is a sparse matrix, which can be easily converted to a dense array using .toarray()\n",
    "features_array = features.toarray()\n",
    "\n",
    "# get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# put together term-doc matrix and feature names in pd.DataFrame\n",
    "word_description_occ = pd.DataFrame(data=features_array, columns=feature_names)\n",
    "\n",
    "# To get the most common words, we can sum up the values in each row\n",
    "word_counts = word_description_occ.sum(axis=0)\n",
    "\n",
    "# And then sort the word_counts in descending order\n",
    "most_common_words = word_counts.sort_values(ascending=False)\n",
    "\n",
    "\n",
    "print(\"Most common words: \\n\")\n",
    "most_common_words.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905215b1",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62715032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## replace keywords for easier keyword extraction\n",
    "\n",
    "data.description_prepro.replace(['(?i)(Google Tag Manager)|\\b(GTM)\\b', \"(?i)\\b(GA4)\\b|\\b(GA)\\b\", '(?i)\\b(Google Colab)\\b', '\\b(GCP)\\b|(google cloud plateform)\\b'], value=['Google Tag Manager', 'Google Analytics', 'Google Colaboratory', 'Google Cloud Plateform'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)\\b(AWS)\\b|Amazon Web Services'], value=['Amazon Web Services'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)SKlearn|Scikit'], value=['ScikitLearn'] ,regex=True, inplace=True)\n",
    "data.description_prepro.replace(['(?i)data viz'], value=['data visualisation'] ,regex=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b649a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "'sas', 'Spark', 'BigML', 'D3.js', 'MATLAB', 'Excel', 'ggplot2', 'Tableau', 'Jupyter', \n",
    "'Matplotlib', 'NLTK', 'TensorFlow', 'Weka', 'Google Analytics', 'KNIME', \n",
    "'Flink', 'MongoDB', 'Minitab', 'Rapidminer', 'DataRobot', 'NLTK', 'Hadoop', 'Power BI', \n",
    "'QlikView', 'MySQL', 'Neo4j', 'HANA', 'Spotfire', 'SPSS', 'STATA', 'RiverLogic', \n",
    "'Lumira', 'Pig', 'Keras', 'NumPy', 'PyTorch', 'Seaborn', 'Wolfram Mathematica', \n",
    "'WebSockets', 'Algorithms.io', 'ForecastThis', 'BigQuery', 'GitHub', \n",
    "'Pycharm', 'Visual Studio Code', 'Linux', 'Windows', 'macOS', 'Google Colaboratory', \n",
    "'Google Cloud Plateform', 'Watson Studio', 'Amazon Web Services', \n",
    "'EC2', 'Amazon Elastic Compute Cloud', 'Microsoft Azure', \n",
    "'Nvidia Jetson Nano', 'Arduino', 'Beam', 'Semantria', 'Trackur', 'Cassandra', 'OctoParse', \n",
    "'Content Grabber', 'OpenRefine', 'Google Fusion Table', 'scipy', 'pandas', 'NPM', 'Redshift', \n",
    "'Snowflake', 'Alteryx', 'Domino Data Lab', 'Kafka', 'Hbase', 'Elasticsearch', 'Maven', \n",
    "'Ansible', 'Gitlab', 'Jenkins', 'Bash', 'IntelliJ', 'MySQL', 'PostreSQL', 'Sonar', \n",
    "'Jira', 'OpenCV', 'TimescaleDB', 'Grafana', 'Google Sheet', 'Pig', 'Talend', 'MSBI',\n",
    "'SAP BO', 'Abode Campaign', 'Google Data Studio', 'Dataform', 'Looker',\n",
    "'Mode', 'Metabase', 'Power Query', 'Power Pivot', 'DataIku', 'MLFlow', 'DVC', 'Kibana', 'SageMaker',\n",
    "'Minio', 'S3', 'MQTT'\n",
    "]\n",
    "\n",
    "keywords_programming = [\n",
    "'sql', 'python', 'r', 'c', 'c#', 'javascript', 'js',  'java', 'scala', 'sas', 'matlab', \n",
    "'c++', 'c/c++', 'perl', 'go', 'typescript', 'bash', 'html', 'css', 'php', 'powershell', 'rust', \n",
    "'kotlin', 'ruby',  'dart', 'assembly', 'swift', 'vba', 'lua', 'groovy', 'delphi', 'objective-c', \n",
    "'haskell', 'elixir', 'julia', 'clojure', 'solidity', 'lisp', 'f#', 'fortran', 'erlang', 'apl', \n",
    "'cobol', 'ocaml', 'crystal', 'javascript/typescript', 'golang', 'nosql', 'mongodb', 't-sql', 'no-sql',\n",
    "'visual_basic', 'pascal', 'mongo', 'pl/sql',  'sass', 'vb.net', 'mssql', \n",
    "]\n",
    "\n",
    "keywords_libraries = [\n",
    "'scikit-learn', 'jupyter', 'theano', 'openCV', 'spark', 'nltk', 'mlpack', 'chainer', 'fann', 'shogun', \n",
    "'dlib', 'mxnet', 'node.js', 'vue', 'vue.js', 'keras', 'ember.js', 'jse/jee',\n",
    "]\n",
    "\n",
    "keywords_analyst_tools = [\n",
    "'excel', 'tableau',  'word', 'powerpoint', 'looker', 'powerbi', 'outlook', 'azure', 'jira', 'twilio',  'snowflake', \n",
    "'shell', 'linux', 'sas', 'sharepoint', 'mysql', 'visio', 'git', 'mssql', 'powerpoints', 'postgresql', 'spreadsheets',\n",
    "'seaborn', 'pandas', 'gdpr', 'spreadsheet', 'alteryx', 'github', 'postgres', 'ssis', 'numpy', 'power_bi', 'spss', 'ssrs', \n",
    "'microstrategy',  'cognos', 'dax', 'matplotlib', 'dplyr', 'tidyr', 'ggplot2', 'plotly', 'esquisse', 'rshiny', 'mlr',\n",
    "'docker', 'linux', 'jira',  'hadoop', 'airflow', 'redis', 'graphql', 'sap', 'tensorflow', 'node', 'asp.net', 'unix',\n",
    "'jquery', 'pyspark', 'pytorch', 'gitlab', 'selenium', 'splunk', 'bitbucket', 'qlik', 'terminal', 'atlassian', 'unix/linux',\n",
    "'linux/unix', 'ubuntu', 'nuix', 'datarobot',\n",
    "]\n",
    "\n",
    "keywords_cloud_tools = [\n",
    "'aws', 'azure', 'gcp', 'snowflake', 'redshift', 'bigquery', 'aurora',\n",
    "]\n",
    "\n",
    "keywords_general_tools = [\n",
    "'microsoft', 'slack', 'apache', 'ibm', 'html5', 'datadog', 'bloomberg',  'ajax', 'persicope', 'oracle', \n",
    "]\n",
    "\n",
    "keywords_general = [\n",
    "'coding', 'server', 'database', 'cloud', 'warehousing', 'scrum', 'devops', 'programming', 'saas', 'ci/cd', 'cicd', \n",
    "'ml', 'data_lake', 'frontend','front-end', 'back-end', 'backend', 'json', 'xml', 'ios', 'kanban', 'nlp',\n",
    "'iot', 'codebase', 'agile/scrum', 'agile', 'ai/ml', 'ai', 'paas', 'machine_learning', 'macros', 'iaas',\n",
    "'fullstack', 'dataops', 'scrum/agile', 'ssas', 'mlops', 'debug', 'etl', 'a/b', 'slack', 'erp', 'oop', \n",
    "'object-oriented', 'etl/elt', 'elt', 'dashboarding', 'big-data', 'twilio', 'ui/ux', 'ux/ui', 'vlookup', \n",
    "'crossover',  'data_lake', 'data_lakes', 'bi', 'pack office'\n",
    "]\n",
    "\n",
    "ml_tools = [\n",
    "'Tensorflow', 'Keras', 'PyTorch', 'ScikitLearn', 'sklearn', 'scikit'\n",
    "]\n",
    "\n",
    "big_data_tools = [\n",
    "'Hadoop', 'HDFS', 'YARN', 'Hive', 'Map', 'Reduce', 'Tez', 'Spark'\n",
    "]\n",
    "\n",
    "skills = [\n",
    "'informatique décisionnelle', 'Extraction', 'Nettoyage', 'transformation', 'ingestion', \n",
    "'data visualisation', 'modélisation', 'reporting', 'veille technologique', 'data mining',\n",
    "'kpi', 'computer vision'\n",
    "]\n",
    "\n",
    "soft_skills = [\n",
    "\"Esprit d'analyse\", \"Sens du service\", \"Rigueur\", \"communication\", 'positif', 'créatif', \n",
    "'pragmatique', 'souple', 'agile', \"Autonome\", 'Polyvalent', 'travailler en équipe', \"esprit d'équipe\"\n",
    "'esprit de synthèse', 'aisance relationnelle', 'force de proposition', \"capacité d'analyse\",\n",
    "\"anglais\", \"espagnol\", \"francais\"\n",
    "]\n",
    "\n",
    "coding_languages = [\n",
    "'SQL', 'Python', 'R', 'Julia', 'Scala', 'C++', 'Java', 'Javascript', 'Go'\n",
    "]\n",
    "\n",
    "# Add up all keyword lists\n",
    "all_kw = tools + keywords_programming + keywords_libraries + keywords_analyst_tools + keywords_cloud_tools + keywords_general_tools + keywords_general + ml_tools + big_data_tools + skills + soft_skills + coding_languages\n",
    "\n",
    "# Lower case & remove duplicates\n",
    "all_kw = list(set([word.lower() for word in all_kw]))\n",
    "\n",
    "# keyword extraction\n",
    "data['kw_extraction'] = data['description_prepro'].apply(lambda x: [])\n",
    "\n",
    "for text in enumerate(data.description_prepro):\n",
    "\n",
    "    kw_list = [word for word in all_kw if word in text[1]]\n",
    "    data.loc[text[0], \"kw_extraction\"].extend(kw_list)\n",
    "\n",
    "# number of keywords extracted\n",
    "data['len_list'] = data['kw_extraction'].map(lambda x: len(x))\n",
    "data['len_list'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f893224",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d8ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining constants\n",
    "keywords = ['durée', 'stage', 'contrat', 'localisation|lieu|location', 'statut', 'salaire', 'formation', 'profil', \n",
    "            'mission', 'avantage', 'experience', 'soft', 'langue']\n",
    "n_chars_after = 20\n",
    "m_chars_before = 60\n",
    "#col_name = f'charsss_around_{kw}'\n",
    "\n",
    "# loop over keywords list\n",
    "for kw in keywords:\n",
    "    \n",
    "    # define our new columns\n",
    "    data[f'chars_around_{kw}'] = 'NC '\n",
    "\n",
    "    # loop over each string in data.description\n",
    "    for i, text in zip(data.index, data['description_prepro']):\n",
    "        \n",
    "        # get iterator over all keywords matches in each string\n",
    "        search_obj = re.finditer(kw, text, re.I)\n",
    "\n",
    "        if search_obj: # ... is true, then ...\n",
    "\n",
    "            # get each keyword match in string\n",
    "            for match in search_obj:\n",
    "\n",
    "                # Find the start index of the keyword\n",
    "                start = match.span()[0]\n",
    "\n",
    "                # Find the end index of the keyword\n",
    "                end = match.span()[1]\n",
    "\n",
    "                # Truncate line to get only 'n' characters before and after the keyword\n",
    "                line = text[start-m_chars_before:end+n_chars_after]\n",
    "                \n",
    "                # add up ever line containing keyword match (if several) in corresponding cells\n",
    "                data.loc[i, f'chars_around_{kw}'] += line\n",
    "                \n",
    "                # get rid of by default 'NC' string\n",
    "                data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}'].replace('NC ', '')\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # if no keyword match then keep by default value\n",
    "            data.loc[i, f'chars_around_{kw}'] = data.loc[i, f'chars_around_{kw}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1c642",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a2339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc98e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
