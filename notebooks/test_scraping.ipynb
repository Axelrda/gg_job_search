{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT NECESSARY LIBRARIES\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Scraping google jobs w/ serpapi\n",
    "import serpapi\n",
    "\n",
    "# generate UULE code from adress\n",
    "import uule_grabber\n",
    "\n",
    "# connect to SQLite database\n",
    "import sqlite3\n",
    "\n",
    "import os\n",
    "\n",
    "# necessary for path to files\n",
    "#from config import DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_KEY = os.environ.get(\"API_KEY\")\n",
    "SEARCH_QUERIES = os.environ.get(\"SEARCH_QUERIES\").split(',')\n",
    "COUNTRY_CODE = os.environ.get(\"COUNTRY_CODE\")\n",
    "TARGET_TYPE = os.environ.get(\"TARGET_TYPE\")\n",
    "DB_PATH = '../db/jobs_database.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine learning engineer',\n",
       " 'data scientist',\n",
       " 'data analyst',\n",
       " 'data engineer']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEARCH_QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite and create a new database (or open it if it already exists)\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # get canonical name for location of interest ==> FRANCE\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \"Canonical Name\"\n",
    "        FROM google_geotargets\n",
    "        WHERE \"Target Type\" = ? AND \"Country Code\" = ?;\n",
    "    \"\"\", (TARGET_TYPE, COUNTRY_CODE))\n",
    "\n",
    "    canonical_name = cursor.fetchall()[0][0]\n",
    "\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w+CAIQICIGRnJhbmNl'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert canonical_name to uule code\n",
    "uule_code = uule_grabber.uule(canonical_name)\n",
    "uule_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all scraped jobs during session\n",
    "all_jobs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_jobs_serpapi(SEARCH_QUERIES, API_KEY, uule_code, date=\"today\"):\n",
    "\n",
    "    all_jobs = pd.DataFrame()\n",
    "\n",
    "    print(\"Starting jobs scraping ...\")\n",
    "\n",
    "    for query in SEARCH_QUERIES:\n",
    "\n",
    "        for num  in range():\n",
    "\n",
    "            start_page = num * 10\n",
    "\n",
    "        # define parameters\n",
    "            params = {\n",
    "                'api_key': API_KEY,\n",
    "                'device':'desktop',\n",
    "                'uule': uule_code,                         # encoded location\n",
    "                'q': query,                          # search query\n",
    "                'google_domain': 'google.fr',\n",
    "                'hl': 'fr',                                 # language of the search\n",
    "                'gl': 'fr',                                 # country of the search\n",
    "                'engine': 'google_jobs',                    # SerpApi search engine\n",
    "                'start': start_page,                             # pagination\n",
    "                'chips': f'date_posted:{date}'  #'date_range:2023-05-18'   #'date_posted:today'\n",
    "            }\n",
    "\n",
    "            # query serapi\n",
    "            search = serpapi.search(params=params)\n",
    "            # get results as dict\n",
    "            res = search.as_dict()\n",
    "\n",
    "            # check if last search page, exceptions handling\n",
    "            try:\n",
    "                if res['error'] == \"Google hasn't returned any results for this query.\":\n",
    "                        break\n",
    "            except KeyError:\n",
    "                    print(f\"Getting SerpAPI data for page: {start_page} - {start_page+10} of '{query}' results\")\n",
    "            else:\n",
    "                    continue\n",
    "\n",
    "            # discard search metadata, keep job results\n",
    "            jobs = res['jobs_results']\n",
    "\n",
    "            # convert to dataframe\n",
    "            jobs_df = pd.DataFrame(jobs)\n",
    "            # convert json columns to dataframe\n",
    "            normalized_extensions = pd.json_normalize(jobs_df['detected_extensions'])\n",
    "\n",
    "            ten_jobs_df = pd.concat([jobs_df, normalized_extensions],axis=1).drop('detected_extensions', axis=1)\n",
    "            ten_jobs_df['date_time'] = datetime.datetime.now()\n",
    "            ten_jobs_df['search_query'] = query\n",
    "\n",
    "            # concat dataframe of 10 pulled results with all_jobs\n",
    "            all_jobs = pd.concat([all_jobs, ten_jobs_df])\n",
    "\n",
    "    all_jobs = all_jobs.drop_duplicates(subset='description')\n",
    "\n",
    "    all_jobs = all_jobs.reindex(columns=['title', 'company_name', 'location', 'via', 'description',\n",
    "        'job_highlights', 'related_links', 'thumbnail', 'extensions', 'job_id',\n",
    "        'posted_at', 'schedule_type', 'date_time', 'search_query'])\n",
    "\n",
    "    all_jobs = all_jobs.reset_index(drop=True)\n",
    "\n",
    "    print(\"Scraping jobs finished ✅\")\n",
    "\n",
    "    print(f\"{all_jobs.shape[0]} jobs were scraped\")\n",
    "\n",
    "    return all_jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_sqlite(DB_PATH, all_jobs):\n",
    "    #### EXPORT TO SQLITE DATABASE ####\n",
    "    # convert value to str format (sql database doesn't accept list type)\n",
    "    for column in all_jobs.columns:\n",
    "        all_jobs[column] = all_jobs[column].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "    print(\"Now, exporting data to SQL database ...\")\n",
    "\n",
    "    # export data to database\n",
    "    with sqlite3.connect(DB_PATH) as conn:\n",
    "        all_jobs.to_sql('unprocessed_data', conn, if_exists='append', index=False)\n",
    "\n",
    "    print(\"Data exported ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg_job_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
