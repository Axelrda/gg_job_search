{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT NECESSARY LIBRARIES\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Scraping google jobs w/ serpapi\n",
    "import serpapi\n",
    "\n",
    "# generate UULE code from adress\n",
    "import uule_grabber\n",
    "\n",
    "# connect to SQLite database\n",
    "import sqlite3\n",
    "\n",
    "import os\n",
    "\n",
    "from jobsearch.params import SERPAPI_KEY, SERPAPI_SEARCH_QUERIES, DB_PATH, FRANCE_UULE_CODE, GOOGLE_GEOTARGET_COUNTRY_CODE, GOOGLE_GEOTARGET_TARGET_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to SQLite and create a new database (or open it if it already exists)\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # get canonical name for location of interest ==> FRANCE\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT \"Canonical Name\"\n",
    "        FROM google_geotargets\n",
    "        WHERE \"Target Type\" = ? AND \"Country Code\" = ?;\n",
    "    \"\"\", (GOOGLE_GEOTARGET_TARGET_TYPE, GOOGLE_GEOTARGET_COUNTRY_CODE))\n",
    "\n",
    "    canonical_name = cursor.fetchall()[0][0]\n",
    "\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w+CAIQICIGRnJhbmNl'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert canonical_name to uule code\n",
    "uule_code = uule_grabber.uule(canonical_name)\n",
    "uule_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with all scraped jobs during session\n",
    "all_jobs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting jobs scraping ...\n",
      "Getting SerpAPI data for page: 0 - 10 of 'machine learning engineer' results\n",
      "Getting SerpAPI data for page: 10 - 20 of 'machine learning engineer' results\n",
      "Getting SerpAPI data for page: 0 - 10 of 'data scientist' results\n",
      "Getting SerpAPI data for page: 10 - 20 of 'data scientist' results\n",
      "Getting SerpAPI data for page: 0 - 10 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 10 - 20 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 20 - 30 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 30 - 40 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 40 - 50 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 50 - 60 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 60 - 70 of 'data analyst' results\n",
      "Getting SerpAPI data for page: 0 - 10 of 'data engineer' results\n",
      "Getting SerpAPI data for page: 10 - 20 of 'data engineer' results\n",
      "Getting SerpAPI data for page: 20 - 30 of 'data engineer' results\n",
      "Getting SerpAPI data for page: 30 - 40 of 'data engineer' results\n",
      "Getting SerpAPI data for page: 40 - 50 of 'data engineer' results\n",
      "Scraping jobs finished ✅\n",
      "116 jobs were scraped\n"
     ]
    }
   ],
   "source": [
    "all_jobs = pd.DataFrame()\n",
    "\n",
    "print(\"Starting jobs scraping ...\")\n",
    "\n",
    "for query in SERPAPI_SEARCH_QUERIES:\n",
    "\n",
    "    for num  in range(50):\n",
    "\n",
    "        start_page = num * 10\n",
    "\n",
    "    # define parameters\n",
    "        params = {\n",
    "            'api_key': SERPAPI_KEY,\n",
    "            'device':'desktop',\n",
    "            'uule': uule_code,                         # encoded location\n",
    "            'q': query,                          # search query\n",
    "            'google_domain': 'google.fr',\n",
    "            'hl': 'fr',                                 # language of the search\n",
    "            'gl': 'fr',                                 # country of the search\n",
    "            'engine': 'google_jobs',                    # SerpApi search engine\n",
    "            'start': start_page,                             # pagination\n",
    "            'chips': f'date_posted:today'  #'date_range:2023-05-18'   #'date_posted:today'\n",
    "        }\n",
    "\n",
    "        # query serapi\n",
    "        search = serpapi.search(params=params)\n",
    "        # get results as dict\n",
    "        res = search.as_dict()\n",
    "\n",
    "        # check if last search page, exceptions handling\n",
    "        try:\n",
    "            if res['error'] == \"Google hasn't returned any results for this query.\":\n",
    "                    break\n",
    "        except KeyError:\n",
    "                print(f\"Getting SerpAPI data for page: {start_page} - {start_page+10} of '{query}' results\")\n",
    "        else:\n",
    "                continue\n",
    "\n",
    "        # discard search metadata, keep job results\n",
    "        jobs = res['jobs_results']\n",
    "\n",
    "        # convert to dataframe\n",
    "        jobs_df = pd.DataFrame(jobs)\n",
    "        # convert json columns to dataframe\n",
    "        normalized_extensions = pd.json_normalize(jobs_df['detected_extensions'])\n",
    "\n",
    "        ten_jobs_df = pd.concat([jobs_df, normalized_extensions],axis=1).drop('detected_extensions', axis=1)\n",
    "        ten_jobs_df['date_time'] = datetime.datetime.now()\n",
    "        ten_jobs_df['search_query'] = query\n",
    "\n",
    "        # concat dataframe of 10 pulled results with all_jobs\n",
    "        all_jobs = pd.concat([all_jobs, ten_jobs_df])\n",
    "\n",
    "all_jobs = all_jobs.drop_duplicates(subset='description')\n",
    "\n",
    "all_jobs = all_jobs.reindex(columns=['title', 'company_name', 'location', 'via', 'description',\n",
    "    'job_highlights', 'related_links', 'thumbnail', 'extensions', 'job_id',\n",
    "    'posted_at', 'schedule_type', 'date_time', 'search_query'])\n",
    "\n",
    "all_jobs = all_jobs.reset_index(drop=True)\n",
    "\n",
    "print(\"Scraping jobs finished ✅\")\n",
    "\n",
    "print(f\"{all_jobs.shape[0]} jobs were scraped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, exporting data to SQL database ...\n",
      "Data exported ✅\n"
     ]
    }
   ],
   "source": [
    "#### EXPORT TO SQLITE DATABASE ####\n",
    "# convert value to str format (sql database doesn't accept list type)\n",
    "for column in all_jobs.columns:\n",
    "    all_jobs[column] = all_jobs[column].apply(lambda x: str(x) if isinstance(x, list) else x)\n",
    "\n",
    "print(\"Now, exporting data to SQL database ...\")\n",
    "\n",
    "# export data to database\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    all_jobs.to_sql('unprocessed_data', conn, if_exists='append', index=False)\n",
    "\n",
    "print(\"Data exported ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg_job_search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
