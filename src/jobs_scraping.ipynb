{"cells":[{"cell_type":"markdown","metadata":{"id":"e29d6f6e"},"source":["google job search scraping tuto : https://serpapi.com/blog/scrape-google-jobs-organic-results-with-python/\n"],"id":"e29d6f6e"},{"cell_type":"code","execution_count":1,"metadata":{"id":"f67Lvx-jUBIY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684616905251,"user_tz":-120,"elapsed":18436,"user":{"displayName":"Camarade Axel","userId":"05508807808245626778"}},"outputId":"fd493c5f-7de5-4d00-a8c4-f66c39791255"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","package_path = '/content/packages'\n","os.symlink('/content/drive/MyDrive/Packages', package_path)\n","sys.path.insert(0, package_path)"],"id":"f67Lvx-jUBIY"},{"cell_type":"markdown","metadata":{"id":"f6ca0a82"},"source":["## IMPORT NECESSARY LIBRARIES"],"id":"f6ca0a82"},{"cell_type":"code","execution_count":2,"metadata":{"id":"c5f79509","executionInfo":{"status":"ok","timestamp":1684616926831,"user_tz":-120,"elapsed":21598,"user":{"displayName":"Camarade Axel","userId":"05508807808245626778"}}},"outputs":[],"source":["import pandas as pd\n","import csv \n","\n","# Libraries for using google jobs API from SERPAPI\n","from serpapi import GoogleSearch\n","\n","import json, os\n","import datetime\n","\n","# Library to generate UULE code on the fly\n","import uule_grabber"],"id":"c5f79509"},{"cell_type":"markdown","metadata":{"id":"b91fae41"},"source":["## GET GEOTARGETS INTO UULE CODE"],"id":"b91fae41"},{"cell_type":"code","execution_count":3,"metadata":{"id":"a2df9107","executionInfo":{"status":"ok","timestamp":1684616928986,"user_tz":-120,"elapsed":2167,"user":{"displayName":"Camarade Axel","userId":"05508807808245626778"}}},"outputs":[],"source":["# assign all google geo location to dataset\n","geotargets = pd.read_csv('/content/drive/MyDrive/github/gg_job_search/src/geotargets-2022-12-21.csv') \n","\n","# get canonical name for our cities of interest >>> PARIS for instance\n","canonical_name = geotargets.loc[(geotargets['Target Type'] == 'Country') &( geotargets['Country Code'] == 'FR'), ['Canonical Name']]\n","canonical_name = canonical_name.iloc[0,0]\n","\n","# get corresponding uule code to cities of interest\n","cities = canonical_name\n","uule_codes = uule_grabber.uule(cities)"],"id":"a2df9107"},{"cell_type":"markdown","metadata":{"id":"0f803d72"},"source":["## COLLECT DATA W\\ SERPAPI"],"id":"0f803d72"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ff60a7d","executionInfo":{"status":"ok","timestamp":1684616941778,"user_tz":-120,"elapsed":12802,"user":{"displayName":"Camarade Axel","userId":"05508807808245626778"}},"outputId":"f3da243a-117e-497a-84d4-a3275bc2c802"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of df: (0, 14)\n"]}],"source":["# Defining our search query + necessary parameters for GoogleSearch object\n","\n","search_queries = [\"machine learning engineer\", \"data scientist\", \"data analyst\", \"data engineer\"]\n","\n","# initialize jobs_all outside of the loop\n","jobs_all = pd.DataFrame()\n","\n","# initialize all_jobs_queries outside of function\n","all_jobs_queries = pd.DataFrame()\n","\n","#for date in datelist:\n","for query in search_queries:\n","\n","    # serpapi will iterate up to n number of iterations\n","    for num  in range(45):\n","\n","        start = num * 10\n","\n","\n","    # define parameters\n","        params = {\n","            'api_key': '4b799b64af09be918f6d66d6e908184cba836c46596e58bfa8bf1fb9280e7f09',                              \n","            'device':'desktop',\n","            'uule': uule_codes,                         # encoded location \n","            'q': query,                          # search query\n","            'google_domain': 'google.fr',              \n","            'hl': 'fr',                                 # language of the search\n","            'gl': 'fr',                                 # country of the search\n","            'engine': 'google_jobs',                    # SerpApi search engine\n","            'start': start,                             # pagination\n","            'chips': 'date_posted:today'  #'data_range:2023-02-03'   #'date_posted:today'                 \n","        }\n","\n","        # get results \n","        search = GoogleSearch(params)\n","        results = search.get_dict()  # JSON file to python dict \n","\n","        # check if last search page, exceptions handling   \n","        try:\n","            if results['error'] == \"Google hasn't returned any results for this query.\":\n","                    break\n","        except KeyError:\n","                print(f\"Getting SerpAPI data for page: {start} of '{query}' results\")\n","        else:\n","                continue\n","\n","        # create dataframe of 10 pulled results\n","        jobs = results['jobs_results']\n","        jobs = pd.DataFrame(jobs)\n","        jobs = pd.concat([pd.DataFrame(jobs), \n","                        pd.json_normalize(jobs['detected_extensions'])], #convert detected extension key in json files into pandas df\n","                        axis=1).drop('detected_extensions', axis=1) # drop json object\n","        jobs['date_time'] = datetime.datetime.now() # add extraction date column for job results\n","\n","        # concat dataframe of 10 pulled results with jobs_all\n","        if start == 0:\n","                jobs_all = jobs\n","        else:\n","                jobs_all = pd.concat([jobs_all, jobs])\n","\n","        # assign ongoing query to pulled results dataframe\n","        jobs_all['search_query'] = query\n","\n","        # concat dataframe of all pulled results with all_jobs_queries \n","        all_jobs_queries = pd.concat([all_jobs_queries, jobs_all])\n","\n","# get rid of duplicates before exportation\n","all_jobs_queries.drop_duplicates(subset='description', inplace=True)\n","\n","# reindex columns to match the order of existing data\n","all_jobs_queries = all_jobs_queries.reindex(columns=['title', 'company_name', 'location', 'via', 'description',\n","       'job_highlights', 'related_links', 'thumbnail', 'extensions', 'job_id',\n","       'posted_at', 'schedule_type', 'date_time', 'search_query'])\n","\n","all_jobs_queries.to_csv('/content/drive/MyDrive/github/gg_job_search/data/gg_job_search_all_RAW.csv', mode='a', header=False, index=False)\n","\n","print(\"Shape of df:\", all_jobs_queries.shape)"],"id":"3ff60a7d"}],"metadata":{"canvas":{"colorPalette":["inherit","inherit","inherit","inherit","inherit","inherit","inherit","inherit","inherit","inherit"],"parameters":[],"version":"1.0"},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}