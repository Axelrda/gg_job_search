{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e29d6f6e",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "3cda2e52-88a5-4270-88b5-9d8d78b02e10",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "source": [
    "google job search scraping tuto : https://serpapi.com/blog/scrape-google-jobs-organic-results-with-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca0a82",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "fc92a435-34dd-4ca3-b69b-d3f385f271f6",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## IMPORT NECESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f79509",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "cb31f276-9da4-408d-929a-d65bcd868943",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv \n",
    "\n",
    "# Libraries for using google jobs API from SERPAPI\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# library for using SERPSTACK\n",
    "#import requests\n",
    "\n",
    "#from google.cloud import bigquery\n",
    "\n",
    "import json, os\n",
    "import datetime\n",
    "\n",
    "# Library to generate UULE code on the fly\n",
    "import uule_grabber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91fae41",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "6c1965b7-f2b7-4d29-b60d-a7930dd33ba4",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## GET GEOTARGETS INTO UULE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2df9107",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "958bde57-244b-4657-b802-34555bb7208e",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [],
   "source": [
    "# assign all google geo location to dataset\n",
    "geotargets = pd.read_csv('geotargets-2022-12-21.csv') \n",
    "# get canonical name for our cities of interest >>> PARIS for instance\n",
    "\n",
    "canonical_name = geotargets.loc[(geotargets['Target Type'] == 'Country') &( geotargets['Country Code'] == 'FR'), ['Canonical Name']]\n",
    "\n",
    "#canonical_name = geotargets.loc[(geotargets.Name == 'Nantes') & (geotargets['Country Code'] == 'FR') & (geotargets['Target Type'] == 'City'), ['Canonical Name']]\n",
    "#canonical_name = canonical_name.iloc[0,0]\n",
    "canonical_name = canonical_name.iloc[0,0]\n",
    "\n",
    "# get corresponding uule code to cities of interest\n",
    "cities = canonical_name\n",
    "uule_codes = uule_grabber.uule(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f803d72",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "1f7981ab-cc23-4ac1-a488-b71a9fbab8f5",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "source": [
    "## COLLECT DATA W\\ SERPAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff60a7d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "3db213b6-9f7f-4483-b957-9ca7f48d5ab7",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 0 of 'machine learning engineer' results\n",
      "https://serpapi.com/search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66754/3824445398.py:46: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  jobs = pd.concat([pd.DataFrame(jobs),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting SerpAPI data for page: 10 of 'machine learning engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 20 of 'machine learning engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 30 of 'machine learning engineer' results\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 0 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 10 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 20 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 30 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 40 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 50 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 60 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 70 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 80 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 90 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 100 of 'data scientist' results\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 0 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 10 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 20 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 30 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 40 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 50 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 60 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 70 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 80 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 90 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 100 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 110 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 120 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 130 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 140 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 150 of 'data analyst' results\n",
      "https://serpapi.com/search\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 0 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 10 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 20 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 30 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 40 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 50 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 60 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 70 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 80 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 90 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 100 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 110 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 120 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 130 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 140 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 150 of 'data engineer' results\n",
      "https://serpapi.com/search\n",
      "Getting SerpAPI data for page: 160 of 'data engineer' results\n",
      "https://serpapi.com/search\n"
     ]
    }
   ],
   "source": [
    "# Defining our search query + necessary parameters for GoogleSearch object\n",
    "\n",
    "search_queries = [\"machine learning engineer\", \"data scientist\", \"data analyst\", \"data engineer\"]\n",
    "\n",
    "# initialize jobs_all outside of the loop\n",
    "jobs_all = pd.DataFrame()\n",
    "#for date in datelist:\n",
    "for query in search_queries:\n",
    "\n",
    "    # serpapi will iterate up to n number of iterations\n",
    "    for num  in range(45):\n",
    "\n",
    "        start = num * 10\n",
    "\n",
    "\n",
    "    # define parameters\n",
    "        params = {\n",
    "            'api_key': '4b799b64af09be918f6d66d6e908184cba836c46596e58bfa8bf1fb9280e7f09',                              \n",
    "            'device':'desktop',\n",
    "            'uule': uule_codes,                         # encoded location \n",
    "            'q': query,                          # search query\n",
    "            'google_domain': 'google.fr',              \n",
    "            'hl': 'fr',                                 # language of the search\n",
    "            'gl': 'fr',                                 # country of the search\n",
    "            'engine': 'google_jobs',                    # SerpApi search engine\n",
    "            'start': start,                             # pagination\n",
    "            'chips': 'date_posted:today'  #'data_range:2023-02-03'   #'date_posted:today'                 \n",
    "        }\n",
    "\n",
    "        # get results \n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()  # JSON file to python dict \n",
    "\n",
    "        # check if last search page, exceptions handling   \n",
    "        try:\n",
    "            if results['error'] == \"Google hasn't returned any results for this query.\":\n",
    "                    break\n",
    "        except KeyError:\n",
    "                print(f\"Getting SerpAPI data for page: {start} of '{query}' results\")\n",
    "        else:\n",
    "                continue\n",
    "\n",
    "        # create dataframe of 10 pulled results\n",
    "        jobs = results['jobs_results']\n",
    "        jobs = pd.DataFrame(jobs)\n",
    "        jobs = pd.concat([pd.DataFrame(jobs), \n",
    "                        pd.json_normalize(jobs['detected_extensions'])], #convert detected extension key in json files into pandas df\n",
    "                        axis=1).drop('detected_extensions', 1) # drop json object\n",
    "        jobs['date_time'] = datetime.datetime.now() # add extraction date column for job results\n",
    "\n",
    "        # concat dataframe\n",
    "        if start == 0:\n",
    "                jobs_all = jobs\n",
    "        else:\n",
    "                jobs_all = pd.concat([jobs_all, jobs])\n",
    "\n",
    "\n",
    "        jobs_all['search_query'] = query\n",
    "\n",
    "\n",
    "    # exporting results to csv file\n",
    "    jobs_all.to_csv('../data/gg_job_search_all_RAW.csv', mode='a', header=False, index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7069cf5",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "none",
     "id": "06945d91-6d7a-45db-acc4-aa3b00524a6e",
     "isComponent": false,
     "name": "",
     "parents": []
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add/rm <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   ../data/gg_job_search_all_RAW.csv\u001b[m\n",
      "\t\u001b[31mdeleted:    ../notebooks/notebooks_models/.ipynb_checkpoints/Untitled-checkpoint.ipynb\u001b[m\n",
      "\t\u001b[31mdeleted:    ../notebooks/notebooks_models/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\u001b[m\n",
      "\t\u001b[31mdeleted:    ../notebooks/notebooks_models/Untitled.ipynb\u001b[m\n",
      "\t\u001b[31mdeleted:    ../notebooks/notebooks_models/Untitled1.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m../.mrx-link-logs/mrx-link-27619.log\u001b[m\n",
      "\t\u001b[31m../notebooks/notebooks_models/.mrx-link-logs/Untitled1.ipynb.log\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "datetime.datetime.now\n",
    "\n",
    "!git add gg_job_search_scrape.ipynb\n",
    "!git commit -m \"daily scrape\"\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
